{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd /home/fakoor/Desktop/chitsaz/emotion_detection/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa5GTlUs2306",
        "outputId": "bd5b5e11-a69d-4b36-c910-9950053b9975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'stock_time_llm'...\n",
            "remote: Enumerating objects: 90, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 90 (delta 34), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (90/90), 1.07 MiB | 1.87 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "/home/fakoor/Desktop/chitsaz/emotion_detection/stock_time_llm\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/androidwoman/stock_time_llm.git\n",
        "%cd /home/fakoor/Desktop/chitsaz/emotion_detection/stock_time_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hH3daSu_B_S2",
        "outputId": "2d5f182a-48d6-4adb-f60d-5cf41819f671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==2.2.2 (from -r requirements.txt (line 1))\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting accelerate==0.28.0 (from -r requirements.txt (line 2))\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting einops==0.7.0 (from -r requirements.txt (line 3))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting matplotlib==3.7.0 (from -r requirements.txt (line 4))\n",
            "  Downloading matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 5))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting pandas==1.5.3 (from -r requirements.txt (line 6))\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scikit_learn==1.2.2 (from -r requirements.txt (line 7))\n",
            "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.12.0 (from -r requirements.txt (line 8))\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting tqdm==4.65.0 (from -r requirements.txt (line 9))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
            "Collecting peft==0.4.0 (from -r requirements.txt (line 10))\n",
            "  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting transformers==4.31.0 (from -r requirements.txt (line 11))\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "Collecting deepspeed==0.14.0 (from -r requirements.txt (line 12))\n",
            "  Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m963.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting sentencepiece==0.2.0 (from -r requirements.txt (line 13))\n",
            "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: filelock in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2->-r requirements.txt (line 1))\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: psutil in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (6.0.0)\n",
            "Requirement already satisfied: pyyaml in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (0.24.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (0.4.5)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Using cached fonttools-4.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pillow>=6.2.0 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Using cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from pandas==1.5.3->-r requirements.txt (line 6)) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from scikit_learn==1.2.2->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from scikit_learn==1.2.2->-r requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 11)) (2024.7.24)\n",
            "Requirement already satisfied: requests in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 11)) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0->-r requirements.txt (line 11))\n",
            "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting hjson (from deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ninja (from deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting py-cpuinfo (from deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Collecting pydantic (from deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Downloading pydantic-2.9.1-py3-none-any.whl.metadata (146 kB)\n",
            "Collecting pynvml (from deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r requirements.txt (line 1)) (12.6.68)\n",
            "Requirement already satisfied: six>=1.5 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.7.0->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from jinja2->torch==2.2.2->-r requirements.txt (line 1)) (2.1.5)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.23.3 (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Downloading pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from sympy->torch==2.2.2->-r requirements.txt (line 1)) (1.3.0)\n",
            "Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0mm\n",
            "\u001b[?25hDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "Downloading matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
            "\u001b[?25hDownloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
            "Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading pydantic-2.9.1-py3-none-any.whl (434 kB)\n",
            "Downloading pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400345 sha256=8d72c049b30fef8b1d7207e4cd71e54103d745425e8dd431e4fa5fca6fdc7e13\n",
            "  Stored in directory: /home/fakoor/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tokenizers, sentencepiece, py-cpuinfo, ninja, hjson, triton, tqdm, pyparsing, pynvml, pydantic-core, pillow, nvidia-nccl-cu12, nvidia-cudnn-cu12, numpy, kiwisolver, fonttools, einops, cycler, annotated-types, scipy, pydantic, pandas, contourpy, transformers, torch, scikit_learn, matplotlib, deepspeed, accelerate, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.1\n",
            "    Uninstalling numpy-2.1.1:\n",
            "      Successfully uninstalled numpy-2.1.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1\n",
            "    Uninstalling torch-2.4.1:\n",
            "      Successfully uninstalled torch-2.4.1\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.5.1\n",
            "    Uninstalling scikit-learn-1.5.1:\n",
            "      Successfully uninstalled scikit-learn-1.5.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.34.2\n",
            "    Uninstalling accelerate-0.34.2:\n",
            "      Successfully uninstalled accelerate-0.34.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.21.0 requires tqdm>=4.66.3, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.28.0 annotated-types-0.7.0 contourpy-1.3.0 cycler-0.12.1 deepspeed-0.14.0 einops-0.7.0 fonttools-4.53.1 hjson-3.1.0 kiwisolver-1.4.7 matplotlib-3.7.0 ninja-1.11.1.1 numpy-1.23.5 nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 pandas-1.5.3 peft-0.4.0 pillow-10.4.0 py-cpuinfo-9.0.0 pydantic-2.9.1 pydantic-core-2.23.3 pynvml-11.5.3 pyparsing-3.1.4 scikit_learn-1.2.2 scipy-1.12.0 sentencepiece-0.2.0 tokenizers-0.13.3 torch-2.2.2 tqdm-4.65.0 transformers-4.31.0 triton-2.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.2.43-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pandas>=1.3.0 in ./.venv/lib/python3.10/site-packages (from yfinance) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in ./.venv/lib/python3.10/site-packages (from yfinance) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.31 in ./.venv/lib/python3.10/site-packages (from yfinance) (2.32.3)\n",
            "Collecting multitasking>=0.0.7 (from yfinance)\n",
            "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting lxml>=4.9.1 (from yfinance)\n",
            "  Using cached lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in ./.venv/lib/python3.10/site-packages (from yfinance) (4.3.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in ./.venv/lib/python3.10/site-packages (from yfinance) (2024.1)\n",
            "Collecting frozendict>=2.3.4 (from yfinance)\n",
            "  Downloading frozendict-2.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Collecting peewee>=3.16.2 (from yfinance)\n",
            "  Downloading peewee-3.17.6.tar.gz (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting beautifulsoup4>=4.11.1 (from yfinance)\n",
            "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting html5lib>=1.1 (from yfinance)\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4>=4.11.1->yfinance)\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: six>=1.9 in ./.venv/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Collecting webencodings (from html5lib>=1.1->yfinance)\n",
            "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2024.8.30)\n",
            "Downloading yfinance-0.2.43-py2.py3-none-any.whl (84 kB)\n",
            "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "Downloading frozendict-2.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
            "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "Using cached lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
            "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: peewee\n",
            "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for peewee: filename=peewee-3.17.6-cp310-cp310-linux_x86_64.whl size=848960 sha256=c460fade5c2d4b83291daefca13df1230ad21bc6273af0d59214fedfafe2482a\n",
            "  Stored in directory: /home/fakoor/.cache/pip/wheels/4b/b9/b0/83d6e258e8f963f5ff111a2cd8c483ca59372a86e6a2535212\n",
            "Successfully built peewee\n",
            "Installing collected packages: webencodings, peewee, multitasking, soupsieve, lxml, html5lib, frozendict, beautifulsoup4, yfinance\n",
            "Successfully installed beautifulsoup4-4.12.3 frozendict-2.4.4 html5lib-1.1 lxml-5.3.0 multitasking-0.0.11 peewee-3.17.6 soupsieve-2.6 webencodings-0.5.1 yfinance-0.2.43\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l102kT0SCAb1",
        "outputId": "93ebfd75-c7e3-4fd5-cd90-cf667d694c8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Download stock data for a specific ticker, e.g., Apple (AAPL)\n",
        "data = yf.download('AAPL', start='2000-01-01', end='2024-01-01', interval='1d')\n",
        "data.reset_index(inplace=True)\n",
        "\n",
        "# Prepare the data by selecting necessary columns and renaming them\n",
        "data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "data.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
        "\n",
        "# Save the data to CSV if needed\n",
        "data.to_csv('data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yq6sopLnDGNW"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR=\"./checkpoints/AAPL\"\n",
        "\n",
        "# Ensure the checkpoint directory exists\n",
        "!mkdir -p $CHECKPOINT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiYf2-EFE2ES",
        "outputId": "d2ec644c-77f7-4e4f-8d18-4a7533452c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/             \u001b[01;34mlayers\u001b[0m/           run_main.py\n",
            "data.csv                 LEGAL.md          run_pretrain.py\n",
            "\u001b[01;34mdata_provider\u001b[0m/           LICENSE           \u001b[01;34mscripts\u001b[0m/\n",
            "\u001b[01;34mdata_provider_pretrain\u001b[0m/  \u001b[01;34mmodels\u001b[0m/           time_llm_apple_5_epoch_train.ipynb\n",
            "\u001b[01;34mdataset\u001b[0m/                 README.md         \u001b[01;34mutils\u001b[0m/\n",
            "ds_config_zero2.json     requirements.txt\n",
            "\u001b[01;34mfigures\u001b[0m/                 run_m4.py\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GzSJr5AFirw",
        "outputId": "a6a0380e-2e6c-45fb-9e38-9ad09890bc6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_596906/3370358579.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.rename(columns={'timestamp': 'date'}, inplace=True)\n",
            "/tmp/ipykernel_596906/3370358579.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.rename(columns={'close': 'y'}, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = data\n",
        "df.rename(columns={'timestamp': 'date'}, inplace=True)\n",
        "df.rename(columns={'close': 'y'}, inplace=True)\n",
        "df = df[['date', 'y']]\n",
        "df.to_csv('data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "SGhypmqiFqrO",
        "outputId": "17ac53dc-9b11-4bf3-9bdc-709e08670dbc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>0.999442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-01-04</td>\n",
              "      <td>0.915179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-01-05</td>\n",
              "      <td>0.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-01-06</td>\n",
              "      <td>0.848214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-01-07</td>\n",
              "      <td>0.888393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6032</th>\n",
              "      <td>2023-12-22</td>\n",
              "      <td>193.600006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6033</th>\n",
              "      <td>2023-12-26</td>\n",
              "      <td>193.050003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6034</th>\n",
              "      <td>2023-12-27</td>\n",
              "      <td>193.149994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6035</th>\n",
              "      <td>2023-12-28</td>\n",
              "      <td>193.580002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6036</th>\n",
              "      <td>2023-12-29</td>\n",
              "      <td>192.529999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6037 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           date           y\n",
              "0    2000-01-03    0.999442\n",
              "1    2000-01-04    0.915179\n",
              "2    2000-01-05    0.928571\n",
              "3    2000-01-06    0.848214\n",
              "4    2000-01-07    0.888393\n",
              "...         ...         ...\n",
              "6032 2023-12-22  193.600006\n",
              "6033 2023-12-26  193.050003\n",
              "6034 2023-12-27  193.149994\n",
              "6035 2023-12-28  193.580002\n",
              "6036 2023-12-29  192.529999\n",
              "\n",
              "[6037 rows x 2 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMi-Hn-def4t",
        "outputId": "650f0fc4-7f0b-4a04-9ad2-b31dfda94501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection/emotions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd /home/fakoor/Desktop/chitsaz/emotion_detection/emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!export RDMAV_FORK_SAFE=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['RDMAV_FORK_SAFE'] = '1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection/emotions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd /home/fakoor/Desktop/chitsaz/emotion_detection/emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acReWe6pDDE_",
        "outputId": "73ed6643-89ec-48ef-f44f-1a1e85ec6444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------\n",
            "0\n",
            "['2000-01-03T00:00:00.000000000' '2000-01-04T00:00:00.000000000'\n",
            " '2000-01-05T00:00:00.000000000' ... '2020-05-21T00:00:00.000000000'\n",
            " '2020-05-22T00:00:00.000000000' '2020-05-26T00:00:00.000000000']\n",
            "-------------------------------\n",
            "1\n",
            "['2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
            " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000'\n",
            " '2020-03-02T00:00:00.000000000' '2020-03-03T00:00:00.000000000'\n",
            " '2020-03-04T00:00:00.000000000' '2020-03-05T00:00:00.000000000'\n",
            " '2020-03-06T00:00:00.000000000' '2020-03-09T00:00:00.000000000'\n",
            " '2020-03-10T00:00:00.000000000' '2020-03-11T00:00:00.000000000'\n",
            " '2020-03-12T00:00:00.000000000' '2020-03-13T00:00:00.000000000'\n",
            " '2020-03-16T00:00:00.000000000' '2020-03-17T00:00:00.000000000'\n",
            " '2020-03-18T00:00:00.000000000' '2020-03-19T00:00:00.000000000'\n",
            " '2020-03-20T00:00:00.000000000' '2020-03-23T00:00:00.000000000'\n",
            " '2020-03-24T00:00:00.000000000' '2020-03-25T00:00:00.000000000'\n",
            " '2020-03-26T00:00:00.000000000' '2020-03-27T00:00:00.000000000'\n",
            " '2020-03-30T00:00:00.000000000' '2020-03-31T00:00:00.000000000'\n",
            " '2020-04-01T00:00:00.000000000' '2020-04-02T00:00:00.000000000'\n",
            " '2020-04-03T00:00:00.000000000' '2020-04-06T00:00:00.000000000'\n",
            " '2020-04-07T00:00:00.000000000' '2020-04-08T00:00:00.000000000'\n",
            " '2020-04-09T00:00:00.000000000' '2020-04-13T00:00:00.000000000'\n",
            " '2020-04-14T00:00:00.000000000' '2020-04-15T00:00:00.000000000'\n",
            " '2020-04-16T00:00:00.000000000' '2020-04-17T00:00:00.000000000'\n",
            " '2020-04-20T00:00:00.000000000' '2020-04-21T00:00:00.000000000'\n",
            " '2020-04-22T00:00:00.000000000' '2020-04-23T00:00:00.000000000'\n",
            " '2020-04-24T00:00:00.000000000' '2020-04-27T00:00:00.000000000'\n",
            " '2020-04-28T00:00:00.000000000' '2020-04-29T00:00:00.000000000'\n",
            " '2020-04-30T00:00:00.000000000' '2020-05-01T00:00:00.000000000'\n",
            " '2020-05-04T00:00:00.000000000' '2020-05-05T00:00:00.000000000'\n",
            " '2020-05-06T00:00:00.000000000' '2020-05-07T00:00:00.000000000'\n",
            " '2020-05-08T00:00:00.000000000' '2020-05-11T00:00:00.000000000'\n",
            " '2020-05-12T00:00:00.000000000' '2020-05-13T00:00:00.000000000'\n",
            " '2020-05-14T00:00:00.000000000' '2020-05-15T00:00:00.000000000'\n",
            " '2020-05-18T00:00:00.000000000' '2020-05-19T00:00:00.000000000'\n",
            " '2020-05-20T00:00:00.000000000' '2020-05-21T00:00:00.000000000'\n",
            " '2020-05-22T00:00:00.000000000' '2020-05-26T00:00:00.000000000'\n",
            " '2020-05-27T00:00:00.000000000' '2020-05-28T00:00:00.000000000'\n",
            " '2020-05-29T00:00:00.000000000' '2020-06-01T00:00:00.000000000'\n",
            " '2020-06-02T00:00:00.000000000' '2020-06-03T00:00:00.000000000'\n",
            " '2020-06-04T00:00:00.000000000' '2020-06-05T00:00:00.000000000'\n",
            " '2020-06-08T00:00:00.000000000' '2020-06-09T00:00:00.000000000'\n",
            " '2020-06-10T00:00:00.000000000' '2020-06-11T00:00:00.000000000'\n",
            " '2020-06-12T00:00:00.000000000' '2020-06-15T00:00:00.000000000'\n",
            " '2020-06-16T00:00:00.000000000' '2020-06-17T00:00:00.000000000'\n",
            " '2020-06-18T00:00:00.000000000' '2020-06-19T00:00:00.000000000'\n",
            " '2020-06-22T00:00:00.000000000' '2020-06-23T00:00:00.000000000'\n",
            " '2020-06-24T00:00:00.000000000' '2020-06-25T00:00:00.000000000'\n",
            " '2020-06-26T00:00:00.000000000' '2020-06-29T00:00:00.000000000'\n",
            " '2020-06-30T00:00:00.000000000' '2020-07-01T00:00:00.000000000'\n",
            " '2020-07-02T00:00:00.000000000' '2020-07-06T00:00:00.000000000'\n",
            " '2020-07-07T00:00:00.000000000' '2020-07-08T00:00:00.000000000'\n",
            " '2020-07-09T00:00:00.000000000' '2020-07-10T00:00:00.000000000'\n",
            " '2020-07-13T00:00:00.000000000' '2020-07-14T00:00:00.000000000'\n",
            " '2020-07-15T00:00:00.000000000' '2020-07-16T00:00:00.000000000'\n",
            " '2020-07-17T00:00:00.000000000' '2020-07-20T00:00:00.000000000'\n",
            " '2020-07-21T00:00:00.000000000' '2020-07-22T00:00:00.000000000'\n",
            " '2020-07-23T00:00:00.000000000' '2020-07-24T00:00:00.000000000'\n",
            " '2020-07-27T00:00:00.000000000' '2020-07-28T00:00:00.000000000'\n",
            " '2020-07-29T00:00:00.000000000' '2020-07-30T00:00:00.000000000'\n",
            " '2020-07-31T00:00:00.000000000' '2020-08-03T00:00:00.000000000'\n",
            " '2020-08-04T00:00:00.000000000' '2020-08-05T00:00:00.000000000'\n",
            " '2020-08-06T00:00:00.000000000' '2020-08-07T00:00:00.000000000'\n",
            " '2020-08-10T00:00:00.000000000' '2020-08-11T00:00:00.000000000'\n",
            " '2020-08-12T00:00:00.000000000' '2020-08-13T00:00:00.000000000'\n",
            " '2020-08-14T00:00:00.000000000' '2020-08-17T00:00:00.000000000'\n",
            " '2020-08-18T00:00:00.000000000' '2020-08-19T00:00:00.000000000'\n",
            " '2020-08-20T00:00:00.000000000' '2020-08-21T00:00:00.000000000'\n",
            " '2020-08-24T00:00:00.000000000' '2020-08-25T00:00:00.000000000'\n",
            " '2020-08-26T00:00:00.000000000' '2020-08-27T00:00:00.000000000'\n",
            " '2020-08-28T00:00:00.000000000' '2020-08-31T00:00:00.000000000'\n",
            " '2020-09-01T00:00:00.000000000' '2020-09-02T00:00:00.000000000'\n",
            " '2020-09-03T00:00:00.000000000' '2020-09-04T00:00:00.000000000'\n",
            " '2020-09-08T00:00:00.000000000' '2020-09-09T00:00:00.000000000'\n",
            " '2020-09-10T00:00:00.000000000' '2020-09-11T00:00:00.000000000'\n",
            " '2020-09-14T00:00:00.000000000' '2020-09-15T00:00:00.000000000'\n",
            " '2020-09-16T00:00:00.000000000' '2020-09-17T00:00:00.000000000'\n",
            " '2020-09-18T00:00:00.000000000' '2020-09-21T00:00:00.000000000'\n",
            " '2020-09-22T00:00:00.000000000' '2020-09-23T00:00:00.000000000'\n",
            " '2020-09-24T00:00:00.000000000' '2020-09-25T00:00:00.000000000'\n",
            " '2020-09-28T00:00:00.000000000' '2020-09-29T00:00:00.000000000'\n",
            " '2020-09-30T00:00:00.000000000' '2020-10-01T00:00:00.000000000'\n",
            " '2020-10-02T00:00:00.000000000' '2020-10-05T00:00:00.000000000'\n",
            " '2020-10-06T00:00:00.000000000' '2020-10-07T00:00:00.000000000'\n",
            " '2020-10-08T00:00:00.000000000' '2020-10-09T00:00:00.000000000'\n",
            " '2020-10-12T00:00:00.000000000' '2020-10-13T00:00:00.000000000'\n",
            " '2020-10-14T00:00:00.000000000' '2020-10-15T00:00:00.000000000'\n",
            " '2020-10-16T00:00:00.000000000' '2020-10-19T00:00:00.000000000'\n",
            " '2020-10-20T00:00:00.000000000' '2020-10-21T00:00:00.000000000'\n",
            " '2020-10-22T00:00:00.000000000' '2020-10-23T00:00:00.000000000'\n",
            " '2020-10-26T00:00:00.000000000' '2020-10-27T00:00:00.000000000'\n",
            " '2020-10-28T00:00:00.000000000' '2020-10-29T00:00:00.000000000'\n",
            " '2020-10-30T00:00:00.000000000' '2020-11-02T00:00:00.000000000'\n",
            " '2020-11-03T00:00:00.000000000' '2020-11-04T00:00:00.000000000'\n",
            " '2020-11-05T00:00:00.000000000' '2020-11-06T00:00:00.000000000'\n",
            " '2020-11-09T00:00:00.000000000' '2020-11-10T00:00:00.000000000'\n",
            " '2020-11-11T00:00:00.000000000' '2020-11-12T00:00:00.000000000'\n",
            " '2020-11-13T00:00:00.000000000' '2020-11-16T00:00:00.000000000'\n",
            " '2020-11-17T00:00:00.000000000' '2020-11-18T00:00:00.000000000'\n",
            " '2020-11-19T00:00:00.000000000' '2020-11-20T00:00:00.000000000'\n",
            " '2020-11-23T00:00:00.000000000' '2020-11-24T00:00:00.000000000'\n",
            " '2020-11-25T00:00:00.000000000' '2020-11-27T00:00:00.000000000'\n",
            " '2020-11-30T00:00:00.000000000' '2020-12-01T00:00:00.000000000'\n",
            " '2020-12-02T00:00:00.000000000' '2020-12-03T00:00:00.000000000'\n",
            " '2020-12-04T00:00:00.000000000' '2020-12-07T00:00:00.000000000'\n",
            " '2020-12-08T00:00:00.000000000' '2020-12-09T00:00:00.000000000'\n",
            " '2020-12-10T00:00:00.000000000' '2020-12-11T00:00:00.000000000'\n",
            " '2020-12-14T00:00:00.000000000' '2020-12-15T00:00:00.000000000'\n",
            " '2020-12-16T00:00:00.000000000' '2020-12-17T00:00:00.000000000'\n",
            " '2020-12-18T00:00:00.000000000' '2020-12-21T00:00:00.000000000'\n",
            " '2020-12-22T00:00:00.000000000' '2020-12-23T00:00:00.000000000'\n",
            " '2020-12-24T00:00:00.000000000' '2020-12-28T00:00:00.000000000'\n",
            " '2020-12-29T00:00:00.000000000' '2020-12-30T00:00:00.000000000'\n",
            " '2020-12-31T00:00:00.000000000' '2021-01-04T00:00:00.000000000'\n",
            " '2021-01-05T00:00:00.000000000' '2021-01-06T00:00:00.000000000'\n",
            " '2021-01-07T00:00:00.000000000' '2021-01-08T00:00:00.000000000'\n",
            " '2021-01-11T00:00:00.000000000' '2021-01-12T00:00:00.000000000'\n",
            " '2021-01-13T00:00:00.000000000' '2021-01-14T00:00:00.000000000'\n",
            " '2021-01-15T00:00:00.000000000' '2021-01-19T00:00:00.000000000'\n",
            " '2021-01-20T00:00:00.000000000' '2021-01-21T00:00:00.000000000'\n",
            " '2021-01-22T00:00:00.000000000' '2021-01-25T00:00:00.000000000'\n",
            " '2021-01-26T00:00:00.000000000' '2021-01-27T00:00:00.000000000'\n",
            " '2021-01-28T00:00:00.000000000' '2021-01-29T00:00:00.000000000'\n",
            " '2021-02-01T00:00:00.000000000' '2021-02-02T00:00:00.000000000'\n",
            " '2021-02-03T00:00:00.000000000' '2021-02-04T00:00:00.000000000'\n",
            " '2021-02-05T00:00:00.000000000' '2021-02-08T00:00:00.000000000'\n",
            " '2021-02-09T00:00:00.000000000' '2021-02-10T00:00:00.000000000'\n",
            " '2021-02-11T00:00:00.000000000' '2021-02-12T00:00:00.000000000'\n",
            " '2021-02-16T00:00:00.000000000' '2021-02-17T00:00:00.000000000'\n",
            " '2021-02-18T00:00:00.000000000' '2021-02-19T00:00:00.000000000'\n",
            " '2021-02-22T00:00:00.000000000' '2021-02-23T00:00:00.000000000'\n",
            " '2021-02-24T00:00:00.000000000' '2021-02-25T00:00:00.000000000'\n",
            " '2021-02-26T00:00:00.000000000' '2021-03-01T00:00:00.000000000'\n",
            " '2021-03-02T00:00:00.000000000' '2021-03-03T00:00:00.000000000'\n",
            " '2021-03-04T00:00:00.000000000' '2021-03-05T00:00:00.000000000'\n",
            " '2021-03-08T00:00:00.000000000' '2021-03-09T00:00:00.000000000'\n",
            " '2021-03-10T00:00:00.000000000' '2021-03-11T00:00:00.000000000'\n",
            " '2021-03-12T00:00:00.000000000' '2021-03-15T00:00:00.000000000'\n",
            " '2021-03-16T00:00:00.000000000' '2021-03-17T00:00:00.000000000'\n",
            " '2021-03-18T00:00:00.000000000' '2021-03-19T00:00:00.000000000'\n",
            " '2021-03-22T00:00:00.000000000' '2021-03-23T00:00:00.000000000'\n",
            " '2021-03-24T00:00:00.000000000' '2021-03-25T00:00:00.000000000'\n",
            " '2021-03-26T00:00:00.000000000' '2021-03-29T00:00:00.000000000'\n",
            " '2021-03-30T00:00:00.000000000' '2021-03-31T00:00:00.000000000'\n",
            " '2021-04-01T00:00:00.000000000' '2021-04-05T00:00:00.000000000'\n",
            " '2021-04-06T00:00:00.000000000' '2021-04-07T00:00:00.000000000'\n",
            " '2021-04-08T00:00:00.000000000' '2021-04-09T00:00:00.000000000'\n",
            " '2021-04-12T00:00:00.000000000' '2021-04-13T00:00:00.000000000'\n",
            " '2021-04-14T00:00:00.000000000' '2021-04-15T00:00:00.000000000'\n",
            " '2021-04-16T00:00:00.000000000' '2021-04-19T00:00:00.000000000'\n",
            " '2021-04-20T00:00:00.000000000' '2021-04-21T00:00:00.000000000'\n",
            " '2021-04-22T00:00:00.000000000' '2021-04-23T00:00:00.000000000'\n",
            " '2021-04-26T00:00:00.000000000' '2021-04-27T00:00:00.000000000'\n",
            " '2021-04-28T00:00:00.000000000' '2021-04-29T00:00:00.000000000'\n",
            " '2021-04-30T00:00:00.000000000' '2021-05-03T00:00:00.000000000'\n",
            " '2021-05-04T00:00:00.000000000' '2021-05-05T00:00:00.000000000'\n",
            " '2021-05-06T00:00:00.000000000' '2021-05-07T00:00:00.000000000'\n",
            " '2021-05-10T00:00:00.000000000' '2021-05-11T00:00:00.000000000'\n",
            " '2021-05-12T00:00:00.000000000' '2021-05-13T00:00:00.000000000'\n",
            " '2021-05-14T00:00:00.000000000' '2021-05-17T00:00:00.000000000'\n",
            " '2021-05-18T00:00:00.000000000' '2021-05-19T00:00:00.000000000'\n",
            " '2021-05-20T00:00:00.000000000' '2021-05-21T00:00:00.000000000'\n",
            " '2021-05-24T00:00:00.000000000' '2021-05-25T00:00:00.000000000'\n",
            " '2021-05-26T00:00:00.000000000' '2021-05-27T00:00:00.000000000'\n",
            " '2021-05-28T00:00:00.000000000' '2021-06-01T00:00:00.000000000'\n",
            " '2021-06-02T00:00:00.000000000' '2021-06-03T00:00:00.000000000'\n",
            " '2021-06-04T00:00:00.000000000' '2021-06-07T00:00:00.000000000'\n",
            " '2021-06-08T00:00:00.000000000' '2021-06-09T00:00:00.000000000'\n",
            " '2021-06-10T00:00:00.000000000' '2021-06-11T00:00:00.000000000'\n",
            " '2021-06-14T00:00:00.000000000' '2021-06-15T00:00:00.000000000'\n",
            " '2021-06-16T00:00:00.000000000' '2021-06-17T00:00:00.000000000'\n",
            " '2021-06-18T00:00:00.000000000' '2021-06-21T00:00:00.000000000'\n",
            " '2021-06-22T00:00:00.000000000' '2021-06-23T00:00:00.000000000'\n",
            " '2021-06-24T00:00:00.000000000' '2021-06-25T00:00:00.000000000'\n",
            " '2021-06-28T00:00:00.000000000' '2021-06-29T00:00:00.000000000'\n",
            " '2021-06-30T00:00:00.000000000' '2021-07-01T00:00:00.000000000'\n",
            " '2021-07-02T00:00:00.000000000' '2021-07-06T00:00:00.000000000'\n",
            " '2021-07-07T00:00:00.000000000' '2021-07-08T00:00:00.000000000'\n",
            " '2021-07-09T00:00:00.000000000' '2021-07-12T00:00:00.000000000'\n",
            " '2021-07-13T00:00:00.000000000' '2021-07-14T00:00:00.000000000'\n",
            " '2021-07-15T00:00:00.000000000' '2021-07-16T00:00:00.000000000'\n",
            " '2021-07-19T00:00:00.000000000' '2021-07-20T00:00:00.000000000'\n",
            " '2021-07-21T00:00:00.000000000' '2021-07-22T00:00:00.000000000'\n",
            " '2021-07-23T00:00:00.000000000' '2021-07-26T00:00:00.000000000'\n",
            " '2021-07-27T00:00:00.000000000' '2021-07-28T00:00:00.000000000'\n",
            " '2021-07-29T00:00:00.000000000' '2021-07-30T00:00:00.000000000'\n",
            " '2021-08-02T00:00:00.000000000' '2021-08-03T00:00:00.000000000'\n",
            " '2021-08-04T00:00:00.000000000' '2021-08-05T00:00:00.000000000'\n",
            " '2021-08-06T00:00:00.000000000' '2021-08-09T00:00:00.000000000'\n",
            " '2021-08-10T00:00:00.000000000' '2021-08-11T00:00:00.000000000'\n",
            " '2021-08-12T00:00:00.000000000' '2021-08-13T00:00:00.000000000'\n",
            " '2021-08-16T00:00:00.000000000' '2021-08-17T00:00:00.000000000'\n",
            " '2021-08-18T00:00:00.000000000' '2021-08-19T00:00:00.000000000'\n",
            " '2021-08-20T00:00:00.000000000' '2021-08-23T00:00:00.000000000'\n",
            " '2021-08-24T00:00:00.000000000' '2021-08-25T00:00:00.000000000'\n",
            " '2021-08-26T00:00:00.000000000' '2021-08-27T00:00:00.000000000'\n",
            " '2021-08-30T00:00:00.000000000' '2021-08-31T00:00:00.000000000'\n",
            " '2021-09-01T00:00:00.000000000' '2021-09-02T00:00:00.000000000'\n",
            " '2021-09-03T00:00:00.000000000' '2021-09-07T00:00:00.000000000'\n",
            " '2021-09-08T00:00:00.000000000' '2021-09-09T00:00:00.000000000'\n",
            " '2021-09-10T00:00:00.000000000' '2021-09-13T00:00:00.000000000'\n",
            " '2021-09-14T00:00:00.000000000' '2021-09-15T00:00:00.000000000'\n",
            " '2021-09-16T00:00:00.000000000' '2021-09-17T00:00:00.000000000'\n",
            " '2021-09-20T00:00:00.000000000' '2021-09-21T00:00:00.000000000'\n",
            " '2021-09-22T00:00:00.000000000' '2021-09-23T00:00:00.000000000'\n",
            " '2021-09-24T00:00:00.000000000' '2021-09-27T00:00:00.000000000'\n",
            " '2021-09-28T00:00:00.000000000' '2021-09-29T00:00:00.000000000'\n",
            " '2021-09-30T00:00:00.000000000' '2021-10-01T00:00:00.000000000'\n",
            " '2021-10-04T00:00:00.000000000' '2021-10-05T00:00:00.000000000'\n",
            " '2021-10-06T00:00:00.000000000' '2021-10-07T00:00:00.000000000'\n",
            " '2021-10-08T00:00:00.000000000' '2021-10-11T00:00:00.000000000'\n",
            " '2021-10-12T00:00:00.000000000' '2021-10-13T00:00:00.000000000'\n",
            " '2021-10-14T00:00:00.000000000' '2021-10-15T00:00:00.000000000'\n",
            " '2021-10-18T00:00:00.000000000' '2021-10-19T00:00:00.000000000'\n",
            " '2021-10-20T00:00:00.000000000' '2021-10-21T00:00:00.000000000'\n",
            " '2021-10-22T00:00:00.000000000' '2021-10-25T00:00:00.000000000'\n",
            " '2021-10-26T00:00:00.000000000' '2021-10-27T00:00:00.000000000'\n",
            " '2021-10-28T00:00:00.000000000' '2021-10-29T00:00:00.000000000'\n",
            " '2021-11-01T00:00:00.000000000' '2021-11-02T00:00:00.000000000'\n",
            " '2021-11-03T00:00:00.000000000' '2021-11-04T00:00:00.000000000'\n",
            " '2021-11-05T00:00:00.000000000' '2021-11-08T00:00:00.000000000'\n",
            " '2021-11-09T00:00:00.000000000' '2021-11-10T00:00:00.000000000'\n",
            " '2021-11-11T00:00:00.000000000' '2021-11-12T00:00:00.000000000'\n",
            " '2021-11-15T00:00:00.000000000' '2021-11-16T00:00:00.000000000'\n",
            " '2021-11-17T00:00:00.000000000' '2021-11-18T00:00:00.000000000'\n",
            " '2021-11-19T00:00:00.000000000' '2021-11-22T00:00:00.000000000'\n",
            " '2021-11-23T00:00:00.000000000' '2021-11-24T00:00:00.000000000'\n",
            " '2021-11-26T00:00:00.000000000' '2021-11-29T00:00:00.000000000'\n",
            " '2021-11-30T00:00:00.000000000' '2021-12-01T00:00:00.000000000'\n",
            " '2021-12-02T00:00:00.000000000' '2021-12-03T00:00:00.000000000'\n",
            " '2021-12-06T00:00:00.000000000' '2021-12-07T00:00:00.000000000'\n",
            " '2021-12-08T00:00:00.000000000' '2021-12-09T00:00:00.000000000'\n",
            " '2021-12-10T00:00:00.000000000' '2021-12-13T00:00:00.000000000'\n",
            " '2021-12-14T00:00:00.000000000' '2021-12-15T00:00:00.000000000'\n",
            " '2021-12-16T00:00:00.000000000' '2021-12-17T00:00:00.000000000'\n",
            " '2021-12-20T00:00:00.000000000' '2021-12-21T00:00:00.000000000'\n",
            " '2021-12-22T00:00:00.000000000' '2021-12-23T00:00:00.000000000'\n",
            " '2021-12-27T00:00:00.000000000' '2021-12-28T00:00:00.000000000'\n",
            " '2021-12-29T00:00:00.000000000' '2021-12-30T00:00:00.000000000'\n",
            " '2021-12-31T00:00:00.000000000' '2022-01-03T00:00:00.000000000'\n",
            " '2022-01-04T00:00:00.000000000' '2022-01-05T00:00:00.000000000'\n",
            " '2022-01-06T00:00:00.000000000' '2022-01-07T00:00:00.000000000'\n",
            " '2022-01-10T00:00:00.000000000' '2022-01-11T00:00:00.000000000'\n",
            " '2022-01-12T00:00:00.000000000' '2022-01-13T00:00:00.000000000'\n",
            " '2022-01-14T00:00:00.000000000' '2022-01-18T00:00:00.000000000'\n",
            " '2022-01-19T00:00:00.000000000' '2022-01-20T00:00:00.000000000'\n",
            " '2022-01-21T00:00:00.000000000' '2022-01-24T00:00:00.000000000'\n",
            " '2022-01-25T00:00:00.000000000' '2022-01-26T00:00:00.000000000'\n",
            " '2022-01-27T00:00:00.000000000' '2022-01-28T00:00:00.000000000'\n",
            " '2022-01-31T00:00:00.000000000' '2022-02-01T00:00:00.000000000'\n",
            " '2022-02-02T00:00:00.000000000' '2022-02-03T00:00:00.000000000'\n",
            " '2022-02-04T00:00:00.000000000' '2022-02-07T00:00:00.000000000'\n",
            " '2022-02-08T00:00:00.000000000' '2022-02-09T00:00:00.000000000'\n",
            " '2022-02-10T00:00:00.000000000' '2022-02-11T00:00:00.000000000'\n",
            " '2022-02-14T00:00:00.000000000' '2022-02-15T00:00:00.000000000'\n",
            " '2022-02-16T00:00:00.000000000' '2022-02-17T00:00:00.000000000'\n",
            " '2022-02-18T00:00:00.000000000' '2022-02-22T00:00:00.000000000'\n",
            " '2022-02-23T00:00:00.000000000' '2022-02-24T00:00:00.000000000'\n",
            " '2022-02-25T00:00:00.000000000' '2022-02-28T00:00:00.000000000'\n",
            " '2022-03-01T00:00:00.000000000' '2022-03-02T00:00:00.000000000'\n",
            " '2022-03-03T00:00:00.000000000' '2022-03-04T00:00:00.000000000'\n",
            " '2022-03-07T00:00:00.000000000' '2022-03-08T00:00:00.000000000'\n",
            " '2022-03-09T00:00:00.000000000' '2022-03-10T00:00:00.000000000'\n",
            " '2022-03-11T00:00:00.000000000' '2022-03-14T00:00:00.000000000'\n",
            " '2022-03-15T00:00:00.000000000' '2022-03-16T00:00:00.000000000'\n",
            " '2022-03-17T00:00:00.000000000' '2022-03-18T00:00:00.000000000'\n",
            " '2022-03-21T00:00:00.000000000' '2022-03-22T00:00:00.000000000'\n",
            " '2022-03-23T00:00:00.000000000' '2022-03-24T00:00:00.000000000'\n",
            " '2022-03-25T00:00:00.000000000' '2022-03-28T00:00:00.000000000'\n",
            " '2022-03-29T00:00:00.000000000' '2022-03-30T00:00:00.000000000'\n",
            " '2022-03-31T00:00:00.000000000' '2022-04-01T00:00:00.000000000'\n",
            " '2022-04-04T00:00:00.000000000' '2022-04-05T00:00:00.000000000'\n",
            " '2022-04-06T00:00:00.000000000' '2022-04-07T00:00:00.000000000'\n",
            " '2022-04-08T00:00:00.000000000' '2022-04-11T00:00:00.000000000'\n",
            " '2022-04-12T00:00:00.000000000' '2022-04-13T00:00:00.000000000'\n",
            " '2022-04-14T00:00:00.000000000' '2022-04-18T00:00:00.000000000'\n",
            " '2022-04-19T00:00:00.000000000' '2022-04-20T00:00:00.000000000'\n",
            " '2022-04-21T00:00:00.000000000' '2022-04-22T00:00:00.000000000'\n",
            " '2022-04-25T00:00:00.000000000' '2022-04-26T00:00:00.000000000'\n",
            " '2022-04-27T00:00:00.000000000' '2022-04-28T00:00:00.000000000'\n",
            " '2022-04-29T00:00:00.000000000' '2022-05-02T00:00:00.000000000'\n",
            " '2022-05-03T00:00:00.000000000' '2022-05-04T00:00:00.000000000'\n",
            " '2022-05-05T00:00:00.000000000' '2022-05-06T00:00:00.000000000'\n",
            " '2022-05-09T00:00:00.000000000' '2022-05-10T00:00:00.000000000'\n",
            " '2022-05-11T00:00:00.000000000' '2022-05-12T00:00:00.000000000'\n",
            " '2022-05-13T00:00:00.000000000' '2022-05-16T00:00:00.000000000'\n",
            " '2022-05-17T00:00:00.000000000' '2022-05-18T00:00:00.000000000'\n",
            " '2022-05-19T00:00:00.000000000' '2022-05-20T00:00:00.000000000'\n",
            " '2022-05-23T00:00:00.000000000' '2022-05-24T00:00:00.000000000'\n",
            " '2022-05-25T00:00:00.000000000' '2022-05-26T00:00:00.000000000'\n",
            " '2022-05-27T00:00:00.000000000' '2022-05-31T00:00:00.000000000'\n",
            " '2022-06-01T00:00:00.000000000' '2022-06-02T00:00:00.000000000'\n",
            " '2022-06-03T00:00:00.000000000' '2022-06-06T00:00:00.000000000'\n",
            " '2022-06-07T00:00:00.000000000' '2022-06-08T00:00:00.000000000'\n",
            " '2022-06-09T00:00:00.000000000' '2022-06-10T00:00:00.000000000'\n",
            " '2022-06-13T00:00:00.000000000' '2022-06-14T00:00:00.000000000'\n",
            " '2022-06-15T00:00:00.000000000' '2022-06-16T00:00:00.000000000'\n",
            " '2022-06-17T00:00:00.000000000' '2022-06-21T00:00:00.000000000'\n",
            " '2022-06-22T00:00:00.000000000' '2022-06-23T00:00:00.000000000'\n",
            " '2022-06-24T00:00:00.000000000' '2022-06-27T00:00:00.000000000'\n",
            " '2022-06-28T00:00:00.000000000' '2022-06-29T00:00:00.000000000'\n",
            " '2022-06-30T00:00:00.000000000' '2022-07-01T00:00:00.000000000'\n",
            " '2022-07-05T00:00:00.000000000' '2022-07-06T00:00:00.000000000'\n",
            " '2022-07-07T00:00:00.000000000' '2022-07-08T00:00:00.000000000'\n",
            " '2022-07-11T00:00:00.000000000' '2022-07-12T00:00:00.000000000'\n",
            " '2022-07-13T00:00:00.000000000' '2022-07-14T00:00:00.000000000'\n",
            " '2022-07-15T00:00:00.000000000' '2022-07-18T00:00:00.000000000'\n",
            " '2022-07-19T00:00:00.000000000' '2022-07-20T00:00:00.000000000'\n",
            " '2022-07-21T00:00:00.000000000' '2022-07-22T00:00:00.000000000'\n",
            " '2022-07-25T00:00:00.000000000' '2022-07-26T00:00:00.000000000'\n",
            " '2022-07-27T00:00:00.000000000' '2022-07-28T00:00:00.000000000'\n",
            " '2022-07-29T00:00:00.000000000' '2022-08-01T00:00:00.000000000'\n",
            " '2022-08-02T00:00:00.000000000' '2022-08-03T00:00:00.000000000'\n",
            " '2022-08-04T00:00:00.000000000' '2022-08-05T00:00:00.000000000'\n",
            " '2022-08-08T00:00:00.000000000' '2022-08-09T00:00:00.000000000'\n",
            " '2022-08-10T00:00:00.000000000' '2022-08-11T00:00:00.000000000'\n",
            " '2022-08-12T00:00:00.000000000' '2022-08-15T00:00:00.000000000'\n",
            " '2022-08-16T00:00:00.000000000' '2022-08-17T00:00:00.000000000'\n",
            " '2022-08-18T00:00:00.000000000' '2022-08-19T00:00:00.000000000'\n",
            " '2022-08-22T00:00:00.000000000' '2022-08-23T00:00:00.000000000'\n",
            " '2022-08-24T00:00:00.000000000' '2022-08-25T00:00:00.000000000'\n",
            " '2022-08-26T00:00:00.000000000' '2022-08-29T00:00:00.000000000'\n",
            " '2022-08-30T00:00:00.000000000' '2022-08-31T00:00:00.000000000'\n",
            " '2022-09-01T00:00:00.000000000' '2022-09-02T00:00:00.000000000'\n",
            " '2022-09-06T00:00:00.000000000' '2022-09-07T00:00:00.000000000'\n",
            " '2022-09-08T00:00:00.000000000' '2022-09-09T00:00:00.000000000'\n",
            " '2022-09-12T00:00:00.000000000' '2022-09-13T00:00:00.000000000'\n",
            " '2022-09-14T00:00:00.000000000' '2022-09-15T00:00:00.000000000'\n",
            " '2022-09-16T00:00:00.000000000' '2022-09-19T00:00:00.000000000'\n",
            " '2022-09-20T00:00:00.000000000' '2022-09-21T00:00:00.000000000'\n",
            " '2022-09-22T00:00:00.000000000' '2022-09-23T00:00:00.000000000'\n",
            " '2022-09-26T00:00:00.000000000' '2022-09-27T00:00:00.000000000'\n",
            " '2022-09-28T00:00:00.000000000' '2022-09-29T00:00:00.000000000'\n",
            " '2022-09-30T00:00:00.000000000' '2022-10-03T00:00:00.000000000'\n",
            " '2022-10-04T00:00:00.000000000' '2022-10-05T00:00:00.000000000'\n",
            " '2022-10-06T00:00:00.000000000' '2022-10-07T00:00:00.000000000'\n",
            " '2022-10-10T00:00:00.000000000' '2022-10-11T00:00:00.000000000'\n",
            " '2022-10-12T00:00:00.000000000' '2022-10-13T00:00:00.000000000'\n",
            " '2022-10-14T00:00:00.000000000' '2022-10-17T00:00:00.000000000'\n",
            " '2022-10-18T00:00:00.000000000' '2022-10-19T00:00:00.000000000'\n",
            " '2022-10-20T00:00:00.000000000' '2022-10-21T00:00:00.000000000'\n",
            " '2022-10-24T00:00:00.000000000' '2022-10-25T00:00:00.000000000'\n",
            " '2022-10-26T00:00:00.000000000' '2022-10-27T00:00:00.000000000'\n",
            " '2022-10-28T00:00:00.000000000' '2022-10-31T00:00:00.000000000'\n",
            " '2022-11-01T00:00:00.000000000' '2022-11-02T00:00:00.000000000'\n",
            " '2022-11-03T00:00:00.000000000' '2022-11-04T00:00:00.000000000'\n",
            " '2022-11-07T00:00:00.000000000' '2022-11-08T00:00:00.000000000'\n",
            " '2022-11-09T00:00:00.000000000' '2022-11-10T00:00:00.000000000'\n",
            " '2022-11-11T00:00:00.000000000' '2022-11-14T00:00:00.000000000'\n",
            " '2022-11-15T00:00:00.000000000' '2022-11-16T00:00:00.000000000'\n",
            " '2022-11-17T00:00:00.000000000' '2022-11-18T00:00:00.000000000'\n",
            " '2022-11-21T00:00:00.000000000' '2022-11-22T00:00:00.000000000'\n",
            " '2022-11-23T00:00:00.000000000' '2022-11-25T00:00:00.000000000'\n",
            " '2022-11-28T00:00:00.000000000' '2022-11-29T00:00:00.000000000'\n",
            " '2022-11-30T00:00:00.000000000' '2022-12-01T00:00:00.000000000'\n",
            " '2022-12-02T00:00:00.000000000' '2022-12-05T00:00:00.000000000'\n",
            " '2022-12-06T00:00:00.000000000' '2022-12-07T00:00:00.000000000'\n",
            " '2022-12-08T00:00:00.000000000' '2022-12-09T00:00:00.000000000'\n",
            " '2022-12-12T00:00:00.000000000' '2022-12-13T00:00:00.000000000'\n",
            " '2022-12-14T00:00:00.000000000' '2022-12-15T00:00:00.000000000'\n",
            " '2022-12-16T00:00:00.000000000' '2022-12-19T00:00:00.000000000'\n",
            " '2022-12-20T00:00:00.000000000' '2022-12-21T00:00:00.000000000'\n",
            " '2022-12-22T00:00:00.000000000' '2022-12-23T00:00:00.000000000'\n",
            " '2022-12-27T00:00:00.000000000' '2022-12-28T00:00:00.000000000'\n",
            " '2022-12-29T00:00:00.000000000' '2022-12-30T00:00:00.000000000'\n",
            " '2023-01-03T00:00:00.000000000' '2023-01-04T00:00:00.000000000'\n",
            " '2023-01-05T00:00:00.000000000' '2023-01-06T00:00:00.000000000'\n",
            " '2023-01-09T00:00:00.000000000' '2023-01-10T00:00:00.000000000'\n",
            " '2023-01-11T00:00:00.000000000' '2023-01-12T00:00:00.000000000'\n",
            " '2023-01-13T00:00:00.000000000' '2023-01-17T00:00:00.000000000'\n",
            " '2023-01-18T00:00:00.000000000' '2023-01-19T00:00:00.000000000'\n",
            " '2023-01-20T00:00:00.000000000' '2023-01-23T00:00:00.000000000'\n",
            " '2023-01-24T00:00:00.000000000' '2023-01-25T00:00:00.000000000'\n",
            " '2023-01-26T00:00:00.000000000' '2023-01-27T00:00:00.000000000'\n",
            " '2023-01-30T00:00:00.000000000' '2023-01-31T00:00:00.000000000'\n",
            " '2023-02-01T00:00:00.000000000' '2023-02-02T00:00:00.000000000'\n",
            " '2023-02-03T00:00:00.000000000' '2023-02-06T00:00:00.000000000'\n",
            " '2023-02-07T00:00:00.000000000' '2023-02-08T00:00:00.000000000'\n",
            " '2023-02-09T00:00:00.000000000' '2023-02-10T00:00:00.000000000'\n",
            " '2023-02-13T00:00:00.000000000' '2023-02-14T00:00:00.000000000'\n",
            " '2023-02-15T00:00:00.000000000' '2023-02-16T00:00:00.000000000'\n",
            " '2023-02-17T00:00:00.000000000' '2023-02-21T00:00:00.000000000'\n",
            " '2023-02-22T00:00:00.000000000' '2023-02-23T00:00:00.000000000'\n",
            " '2023-02-24T00:00:00.000000000' '2023-02-27T00:00:00.000000000'\n",
            " '2023-02-28T00:00:00.000000000' '2023-03-01T00:00:00.000000000'\n",
            " '2023-03-02T00:00:00.000000000' '2023-03-03T00:00:00.000000000'\n",
            " '2023-03-06T00:00:00.000000000' '2023-03-07T00:00:00.000000000'\n",
            " '2023-03-08T00:00:00.000000000' '2023-03-09T00:00:00.000000000'\n",
            " '2023-03-10T00:00:00.000000000' '2023-03-13T00:00:00.000000000'\n",
            " '2023-03-14T00:00:00.000000000' '2023-03-15T00:00:00.000000000'\n",
            " '2023-03-16T00:00:00.000000000' '2023-03-17T00:00:00.000000000'\n",
            " '2023-03-20T00:00:00.000000000' '2023-03-21T00:00:00.000000000'\n",
            " '2023-03-22T00:00:00.000000000' '2023-03-23T00:00:00.000000000'\n",
            " '2023-03-24T00:00:00.000000000' '2023-03-27T00:00:00.000000000'\n",
            " '2023-03-28T00:00:00.000000000' '2023-03-29T00:00:00.000000000'\n",
            " '2023-03-30T00:00:00.000000000' '2023-03-31T00:00:00.000000000'\n",
            " '2023-04-03T00:00:00.000000000' '2023-04-04T00:00:00.000000000']\n",
            "-------------------------------\n",
            "2\n",
            "['2023-01-03T00:00:00.000000000' '2023-01-04T00:00:00.000000000'\n",
            " '2023-01-05T00:00:00.000000000' '2023-01-06T00:00:00.000000000'\n",
            " '2023-01-09T00:00:00.000000000' '2023-01-10T00:00:00.000000000'\n",
            " '2023-01-11T00:00:00.000000000' '2023-01-12T00:00:00.000000000'\n",
            " '2023-01-13T00:00:00.000000000' '2023-01-17T00:00:00.000000000'\n",
            " '2023-01-18T00:00:00.000000000' '2023-01-19T00:00:00.000000000'\n",
            " '2023-01-20T00:00:00.000000000' '2023-01-23T00:00:00.000000000'\n",
            " '2023-01-24T00:00:00.000000000' '2023-01-25T00:00:00.000000000'\n",
            " '2023-01-26T00:00:00.000000000' '2023-01-27T00:00:00.000000000'\n",
            " '2023-01-30T00:00:00.000000000' '2023-01-31T00:00:00.000000000'\n",
            " '2023-02-01T00:00:00.000000000' '2023-02-02T00:00:00.000000000'\n",
            " '2023-02-03T00:00:00.000000000' '2023-02-06T00:00:00.000000000'\n",
            " '2023-02-07T00:00:00.000000000' '2023-02-08T00:00:00.000000000'\n",
            " '2023-02-09T00:00:00.000000000' '2023-02-10T00:00:00.000000000'\n",
            " '2023-02-13T00:00:00.000000000' '2023-02-14T00:00:00.000000000'\n",
            " '2023-02-15T00:00:00.000000000' '2023-02-16T00:00:00.000000000'\n",
            " '2023-02-17T00:00:00.000000000' '2023-02-21T00:00:00.000000000'\n",
            " '2023-02-22T00:00:00.000000000' '2023-02-23T00:00:00.000000000'\n",
            " '2023-02-24T00:00:00.000000000' '2023-02-27T00:00:00.000000000'\n",
            " '2023-02-28T00:00:00.000000000' '2023-03-01T00:00:00.000000000'\n",
            " '2023-03-02T00:00:00.000000000' '2023-03-03T00:00:00.000000000'\n",
            " '2023-03-06T00:00:00.000000000' '2023-03-07T00:00:00.000000000'\n",
            " '2023-03-08T00:00:00.000000000' '2023-03-09T00:00:00.000000000'\n",
            " '2023-03-10T00:00:00.000000000' '2023-03-13T00:00:00.000000000'\n",
            " '2023-03-14T00:00:00.000000000' '2023-03-15T00:00:00.000000000'\n",
            " '2023-03-16T00:00:00.000000000' '2023-03-17T00:00:00.000000000'\n",
            " '2023-03-20T00:00:00.000000000' '2023-03-21T00:00:00.000000000'\n",
            " '2023-03-22T00:00:00.000000000' '2023-03-23T00:00:00.000000000'\n",
            " '2023-03-24T00:00:00.000000000' '2023-03-27T00:00:00.000000000'\n",
            " '2023-03-28T00:00:00.000000000' '2023-03-29T00:00:00.000000000'\n",
            " '2023-03-30T00:00:00.000000000' '2023-03-31T00:00:00.000000000'\n",
            " '2023-04-03T00:00:00.000000000' '2023-04-04T00:00:00.000000000'\n",
            " '2023-04-05T00:00:00.000000000' '2023-04-06T00:00:00.000000000'\n",
            " '2023-04-10T00:00:00.000000000' '2023-04-11T00:00:00.000000000'\n",
            " '2023-04-12T00:00:00.000000000' '2023-04-13T00:00:00.000000000'\n",
            " '2023-04-14T00:00:00.000000000' '2023-04-17T00:00:00.000000000'\n",
            " '2023-04-18T00:00:00.000000000' '2023-04-19T00:00:00.000000000'\n",
            " '2023-04-20T00:00:00.000000000' '2023-04-21T00:00:00.000000000'\n",
            " '2023-04-24T00:00:00.000000000' '2023-04-25T00:00:00.000000000'\n",
            " '2023-04-26T00:00:00.000000000' '2023-04-27T00:00:00.000000000'\n",
            " '2023-04-28T00:00:00.000000000' '2023-05-01T00:00:00.000000000'\n",
            " '2023-05-02T00:00:00.000000000' '2023-05-03T00:00:00.000000000'\n",
            " '2023-05-04T00:00:00.000000000' '2023-05-05T00:00:00.000000000'\n",
            " '2023-05-08T00:00:00.000000000' '2023-05-09T00:00:00.000000000'\n",
            " '2023-05-10T00:00:00.000000000' '2023-05-11T00:00:00.000000000'\n",
            " '2023-05-12T00:00:00.000000000' '2023-05-15T00:00:00.000000000'\n",
            " '2023-05-16T00:00:00.000000000' '2023-05-17T00:00:00.000000000'\n",
            " '2023-05-18T00:00:00.000000000' '2023-05-19T00:00:00.000000000'\n",
            " '2023-05-22T00:00:00.000000000' '2023-05-23T00:00:00.000000000'\n",
            " '2023-05-24T00:00:00.000000000' '2023-05-25T00:00:00.000000000'\n",
            " '2023-05-26T00:00:00.000000000' '2023-05-30T00:00:00.000000000'\n",
            " '2023-05-31T00:00:00.000000000' '2023-06-01T00:00:00.000000000'\n",
            " '2023-06-02T00:00:00.000000000' '2023-06-05T00:00:00.000000000'\n",
            " '2023-06-06T00:00:00.000000000' '2023-06-07T00:00:00.000000000'\n",
            " '2023-06-08T00:00:00.000000000' '2023-06-09T00:00:00.000000000'\n",
            " '2023-06-12T00:00:00.000000000' '2023-06-13T00:00:00.000000000'\n",
            " '2023-06-14T00:00:00.000000000' '2023-06-15T00:00:00.000000000'\n",
            " '2023-06-16T00:00:00.000000000' '2023-06-20T00:00:00.000000000'\n",
            " '2023-06-21T00:00:00.000000000' '2023-06-22T00:00:00.000000000'\n",
            " '2023-06-23T00:00:00.000000000' '2023-06-26T00:00:00.000000000'\n",
            " '2023-06-27T00:00:00.000000000' '2023-06-28T00:00:00.000000000'\n",
            " '2023-06-29T00:00:00.000000000' '2023-06-30T00:00:00.000000000'\n",
            " '2023-07-03T00:00:00.000000000' '2023-07-05T00:00:00.000000000'\n",
            " '2023-07-06T00:00:00.000000000' '2023-07-07T00:00:00.000000000'\n",
            " '2023-07-10T00:00:00.000000000' '2023-07-11T00:00:00.000000000'\n",
            " '2023-07-12T00:00:00.000000000' '2023-07-13T00:00:00.000000000'\n",
            " '2023-07-14T00:00:00.000000000' '2023-07-17T00:00:00.000000000'\n",
            " '2023-07-18T00:00:00.000000000' '2023-07-19T00:00:00.000000000'\n",
            " '2023-07-20T00:00:00.000000000' '2023-07-21T00:00:00.000000000'\n",
            " '2023-07-24T00:00:00.000000000' '2023-07-25T00:00:00.000000000'\n",
            " '2023-07-26T00:00:00.000000000' '2023-07-27T00:00:00.000000000'\n",
            " '2023-07-28T00:00:00.000000000' '2023-07-31T00:00:00.000000000'\n",
            " '2023-08-01T00:00:00.000000000' '2023-08-02T00:00:00.000000000'\n",
            " '2023-08-03T00:00:00.000000000' '2023-08-04T00:00:00.000000000'\n",
            " '2023-08-07T00:00:00.000000000' '2023-08-08T00:00:00.000000000'\n",
            " '2023-08-09T00:00:00.000000000' '2023-08-10T00:00:00.000000000'\n",
            " '2023-08-11T00:00:00.000000000' '2023-08-14T00:00:00.000000000'\n",
            " '2023-08-15T00:00:00.000000000' '2023-08-16T00:00:00.000000000'\n",
            " '2023-08-17T00:00:00.000000000' '2023-08-18T00:00:00.000000000'\n",
            " '2023-08-21T00:00:00.000000000' '2023-08-22T00:00:00.000000000'\n",
            " '2023-08-23T00:00:00.000000000' '2023-08-24T00:00:00.000000000'\n",
            " '2023-08-25T00:00:00.000000000' '2023-08-28T00:00:00.000000000'\n",
            " '2023-08-29T00:00:00.000000000' '2023-08-30T00:00:00.000000000'\n",
            " '2023-08-31T00:00:00.000000000' '2023-09-01T00:00:00.000000000'\n",
            " '2023-09-05T00:00:00.000000000' '2023-09-06T00:00:00.000000000'\n",
            " '2023-09-07T00:00:00.000000000' '2023-09-08T00:00:00.000000000'\n",
            " '2023-09-11T00:00:00.000000000' '2023-09-12T00:00:00.000000000'\n",
            " '2023-09-13T00:00:00.000000000' '2023-09-14T00:00:00.000000000'\n",
            " '2023-09-15T00:00:00.000000000' '2023-09-18T00:00:00.000000000'\n",
            " '2023-09-19T00:00:00.000000000' '2023-09-20T00:00:00.000000000'\n",
            " '2023-09-21T00:00:00.000000000' '2023-09-22T00:00:00.000000000'\n",
            " '2023-09-25T00:00:00.000000000' '2023-09-26T00:00:00.000000000'\n",
            " '2023-09-27T00:00:00.000000000' '2023-09-28T00:00:00.000000000'\n",
            " '2023-09-29T00:00:00.000000000' '2023-10-02T00:00:00.000000000'\n",
            " '2023-10-03T00:00:00.000000000' '2023-10-04T00:00:00.000000000'\n",
            " '2023-10-05T00:00:00.000000000' '2023-10-06T00:00:00.000000000'\n",
            " '2023-10-09T00:00:00.000000000' '2023-10-10T00:00:00.000000000'\n",
            " '2023-10-11T00:00:00.000000000' '2023-10-12T00:00:00.000000000'\n",
            " '2023-10-13T00:00:00.000000000' '2023-10-16T00:00:00.000000000'\n",
            " '2023-10-17T00:00:00.000000000' '2023-10-18T00:00:00.000000000'\n",
            " '2023-10-19T00:00:00.000000000' '2023-10-20T00:00:00.000000000'\n",
            " '2023-10-23T00:00:00.000000000' '2023-10-24T00:00:00.000000000'\n",
            " '2023-10-25T00:00:00.000000000' '2023-10-26T00:00:00.000000000'\n",
            " '2023-10-27T00:00:00.000000000' '2023-10-30T00:00:00.000000000'\n",
            " '2023-10-31T00:00:00.000000000' '2023-11-01T00:00:00.000000000'\n",
            " '2023-11-02T00:00:00.000000000' '2023-11-03T00:00:00.000000000'\n",
            " '2023-11-06T00:00:00.000000000' '2023-11-07T00:00:00.000000000'\n",
            " '2023-11-08T00:00:00.000000000' '2023-11-09T00:00:00.000000000'\n",
            " '2023-11-10T00:00:00.000000000' '2023-11-13T00:00:00.000000000'\n",
            " '2023-11-14T00:00:00.000000000' '2023-11-15T00:00:00.000000000'\n",
            " '2023-11-16T00:00:00.000000000' '2023-11-17T00:00:00.000000000'\n",
            " '2023-11-20T00:00:00.000000000' '2023-11-21T00:00:00.000000000'\n",
            " '2023-11-22T00:00:00.000000000' '2023-11-24T00:00:00.000000000'\n",
            " '2023-11-27T00:00:00.000000000' '2023-11-28T00:00:00.000000000'\n",
            " '2023-11-29T00:00:00.000000000' '2023-11-30T00:00:00.000000000'\n",
            " '2023-12-01T00:00:00.000000000' '2023-12-04T00:00:00.000000000'\n",
            " '2023-12-05T00:00:00.000000000' '2023-12-06T00:00:00.000000000'\n",
            " '2023-12-07T00:00:00.000000000' '2023-12-08T00:00:00.000000000'\n",
            " '2023-12-11T00:00:00.000000000' '2023-12-12T00:00:00.000000000'\n",
            " '2023-12-13T00:00:00.000000000' '2023-12-14T00:00:00.000000000'\n",
            " '2023-12-15T00:00:00.000000000' '2023-12-18T00:00:00.000000000'\n",
            " '2023-12-19T00:00:00.000000000' '2023-12-20T00:00:00.000000000'\n",
            " '2023-12-21T00:00:00.000000000' '2023-12-22T00:00:00.000000000'\n",
            " '2023-12-26T00:00:00.000000000' '2023-12-27T00:00:00.000000000'\n",
            " '2023-12-28T00:00:00.000000000' '2023-12-29T00:00:00.000000000']\n",
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.60it/s]\n",
            "[2024-09-15 13:05:54,966] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-09-15 13:05:55,171] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
            "[2024-09-15 13:05:55,171] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-09-15 13:05:55,171] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
            "[2024-09-15 13:05:55,424] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=192.168.59.30, master_port=29500\n",
            "[2024-09-15 13:05:55,424] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2024-09-15 13:05:57,656] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2024-09-15 13:05:57,657] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
            "[2024-09-15 13:05:57,657] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2024-09-15 13:05:57,658] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
            "[2024-09-15 13:05:57,658] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
            "[2024-09-15 13:05:57,658] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
            "[2024-09-15 13:05:57,658] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
            "[2024-09-15 13:05:57,658] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
            "[2024-09-15 13:05:57,658] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
            "[2024-09-15 13:05:57,658] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
            "[2024-09-15 13:05:57,944] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
            "[2024-09-15 13:05:57,945] [INFO] [utils.py:801:see_memory_usage] MA 2.71 GB         Max_MA 2.78 GB         CA 2.78 GB         Max_CA 3 GB \n",
            "[2024-09-15 13:05:57,945] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 41.85 GB, percent = 66.7%\n",
            "[2024-09-15 13:05:58,114] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
            "[2024-09-15 13:05:58,114] [INFO] [utils.py:801:see_memory_usage] MA 2.71 GB         Max_MA 2.84 GB         CA 2.91 GB         Max_CA 3 GB \n",
            "[2024-09-15 13:05:58,114] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 41.85 GB, percent = 66.7%\n",
            "[2024-09-15 13:05:58,114] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
            "[2024-09-15 13:05:58,281] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2024-09-15 13:05:58,281] [INFO] [utils.py:801:see_memory_usage] MA 2.71 GB         Max_MA 2.71 GB         CA 2.91 GB         Max_CA 3 GB \n",
            "[2024-09-15 13:05:58,281] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 41.86 GB, percent = 66.7%\n",
            "[2024-09-15 13:05:58,282] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
            "[2024-09-15 13:05:58,282] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
            "[2024-09-15 13:05:58,282] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
            "[2024-09-15 13:05:58,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4.000000000000002e-06], mom=[(0.95, 0.999)]\n",
            "[2024-09-15 13:05:58,283] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
            "[2024-09-15 13:05:58,283] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2024-09-15 13:05:58,283] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2024-09-15 13:05:58,283] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
            "[2024-09-15 13:05:58,283] [INFO] [config.py:1000:print]   amp_params ................... False\n",
            "[2024-09-15 13:05:58,283] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2024-09-15 13:05:58,283] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
            "[2024-09-15 13:05:58,283] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
            "[2024-09-15 13:05:58,283] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2024-09-15 13:05:58,283] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x726e0a9b66e0>\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   dump_state ................... False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
            "[2024-09-15 13:05:58,284] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   pld_params ................... False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   train_batch_size ............. 5\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  5\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   world_size ................... 1\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
            "[2024-09-15 13:05:58,285] [INFO] [config.py:986:print_user_config]   json = {\n",
            "    \"bf16\": {\n",
            "        \"enabled\": true, \n",
            "        \"auto_cast\": true\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 2, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 2.000000e+08, \n",
            "        \"overlap_comm\": true, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 2.000000e+08, \n",
            "        \"contiguous_gradients\": true, \n",
            "        \"sub_group_size\": 1.000000e+09\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"train_batch_size\": 5, \n",
            "    \"train_micro_batch_size_per_gpu\": 5, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"wall_clock_breakdown\": false, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": false\n",
            "    }, \n",
            "    \"zero_allow_untested_optimizer\": true\n",
            "}\n",
            "98it [00:09, 11.53it/s]\titers: 100, epoch: 1 | loss: 0.0081990\n",
            "\tspeed: 0.1386s/iter; left time: 7003.8969s\n",
            "198it [00:18, 11.63it/s]\titers: 200, epoch: 1 | loss: 0.0009820\n",
            "\tspeed: 0.0858s/iter; left time: 4330.4109s\n",
            "298it [00:26, 11.45it/s]\titers: 300, epoch: 1 | loss: 0.0005337\n",
            "\tspeed: 0.0863s/iter; left time: 4346.0945s\n",
            "399it [00:41,  4.79it/s]\titers: 400, epoch: 1 | loss: 0.0010824\n",
            "\tspeed: 0.1443s/iter; left time: 7251.6405s\n",
            "499it [01:02,  4.89it/s]\titers: 500, epoch: 1 | loss: 0.0004631\n",
            "\tspeed: 0.2091s/iter; left time: 10485.7733s\n",
            "599it [01:23,  4.73it/s]\titers: 600, epoch: 1 | loss: 0.0057290\n",
            "\tspeed: 0.2108s/iter; left time: 10552.7454s\n",
            "699it [01:44,  4.69it/s]\titers: 700, epoch: 1 | loss: 0.0116139\n",
            "\tspeed: 0.2112s/iter; left time: 10551.5391s\n",
            "799it [02:05,  4.70it/s]\titers: 800, epoch: 1 | loss: 0.0007532\n",
            "\tspeed: 0.2119s/iter; left time: 10561.4777s\n",
            "899it [02:26,  4.63it/s]\titers: 900, epoch: 1 | loss: 0.0045552\n",
            "\tspeed: 0.2126s/iter; left time: 10577.4221s\n",
            "999it [02:48,  4.67it/s]\titers: 1000, epoch: 1 | loss: 0.0023133\n",
            "\tspeed: 0.2133s/iter; left time: 10591.5509s\n",
            "1013it [02:51,  5.91it/s]\n",
            "Epoch: 1 cost time: 171.35258030891418\n",
            "144it [00:15,  9.16it/s]\n",
            "37it [00:04,  8.55it/s]\n",
            "Epoch: 1 | Train Loss: 0.0131218 Vali Loss: 0.1410268 Test Loss: 0.0881815 MAE Loss: 0.2450790\n",
            "lr = 0.0000040000\n",
            "Updating learning rate to 4.000000000000002e-06\n",
            "99it [00:21,  4.66it/s]\titers: 100, epoch: 2 | loss: 0.0101518\n",
            "\tspeed: 0.5044s/iter; left time: 24987.1446s\n",
            "199it [00:43,  4.63it/s]\titers: 200, epoch: 2 | loss: 0.0004168\n",
            "\tspeed: 0.2146s/iter; left time: 10608.0845s\n",
            "299it [01:04,  4.63it/s]\titers: 300, epoch: 2 | loss: 0.0037060\n",
            "\tspeed: 0.2146s/iter; left time: 10586.6825s\n",
            "399it [01:25,  4.62it/s]\titers: 400, epoch: 2 | loss: 0.0005651\n",
            "\tspeed: 0.2149s/iter; left time: 10583.5436s\n",
            "499it [01:47,  4.64it/s]\titers: 500, epoch: 2 | loss: 0.0029890\n",
            "\tspeed: 0.2148s/iter; left time: 10554.2770s\n",
            "599it [02:08,  4.62it/s]\titers: 600, epoch: 2 | loss: 0.0030925\n",
            "\tspeed: 0.2149s/iter; left time: 10536.5578s\n",
            "699it [02:30,  4.63it/s]\titers: 700, epoch: 2 | loss: 0.0022891\n",
            "\tspeed: 0.2148s/iter; left time: 10512.2364s\n",
            "799it [02:51,  4.60it/s]\titers: 800, epoch: 2 | loss: 0.0002834\n",
            "\tspeed: 0.2150s/iter; left time: 10498.3491s\n",
            "899it [03:13,  4.63it/s]\titers: 900, epoch: 2 | loss: 0.0038534\n",
            "\tspeed: 0.2151s/iter; left time: 10485.3749s\n",
            "999it [03:34,  4.61it/s]\titers: 1000, epoch: 2 | loss: 0.0010779\n",
            "\tspeed: 0.2151s/iter; left time: 10464.4424s\n",
            "1013it [03:38,  4.65it/s]\n",
            "Epoch: 2 cost time: 218.00592279434204\n",
            "144it [00:15,  9.11it/s]\n",
            "37it [00:04,  8.55it/s]\n",
            "Epoch: 2 | Train Loss: 0.0041847 Vali Loss: 0.0621977 Test Loss: 0.0484171 MAE Loss: 0.1775727\n",
            "Updating learning rate to 2.000000000000001e-06\n",
            "99it [00:21,  4.61it/s]\titers: 100, epoch: 3 | loss: 0.0027664\n",
            "\tspeed: 0.5375s/iter; left time: 26084.2870s\n",
            "199it [00:43,  4.66it/s]\titers: 200, epoch: 3 | loss: 0.0008531\n",
            "\tspeed: 0.2151s/iter; left time: 10417.4966s\n",
            "299it [01:04,  4.62it/s]\titers: 300, epoch: 3 | loss: 0.0008912\n",
            "\tspeed: 0.2153s/iter; left time: 10404.6432s\n",
            "399it [01:26,  4.63it/s]\titers: 400, epoch: 3 | loss: 0.0017211\n",
            "\tspeed: 0.2147s/iter; left time: 10353.4780s\n",
            "499it [01:47,  4.67it/s]\titers: 500, epoch: 3 | loss: 0.0160096\n",
            "\tspeed: 0.2151s/iter; left time: 10353.2563s\n",
            "599it [02:09,  4.61it/s]\titers: 600, epoch: 3 | loss: 0.0015321\n",
            "\tspeed: 0.2151s/iter; left time: 10330.5939s\n",
            "699it [02:30,  4.77it/s]\titers: 700, epoch: 3 | loss: 0.0131318\n",
            "\tspeed: 0.2150s/iter; left time: 10306.0951s\n",
            "799it [02:52,  4.61it/s]\titers: 800, epoch: 3 | loss: 0.0004429\n",
            "\tspeed: 0.2158s/iter; left time: 10318.3239s\n",
            "899it [03:13,  4.70it/s]\titers: 900, epoch: 3 | loss: 0.0012613\n",
            "\tspeed: 0.2154s/iter; left time: 10278.2296s\n",
            "999it [03:35,  4.63it/s]\titers: 1000, epoch: 3 | loss: 0.0001310\n",
            "\tspeed: 0.2154s/iter; left time: 10258.2197s\n",
            "1013it [03:38,  4.64it/s]\n",
            "Epoch: 3 cost time: 218.43165159225464\n",
            "144it [00:15,  9.10it/s]\n",
            "37it [00:04,  8.43it/s]\n",
            "Epoch: 3 | Train Loss: 0.0028982 Vali Loss: 0.0439611 Test Loss: 0.0344616 MAE Loss: 0.1492429\n",
            "Updating learning rate to 1.0000000000000006e-06\n",
            "99it [00:21,  4.62it/s]\titers: 100, epoch: 4 | loss: 0.0001368\n",
            "\tspeed: 0.5416s/iter; left time: 25733.5187s\n",
            "199it [00:43,  4.60it/s]\titers: 200, epoch: 4 | loss: 0.0000155\n",
            "\tspeed: 0.2157s/iter; left time: 10224.4571s\n",
            "299it [01:04,  4.63it/s]\titers: 300, epoch: 4 | loss: 0.0039115\n",
            "\tspeed: 0.2154s/iter; left time: 10191.1294s\n",
            "399it [01:26,  4.61it/s]\titers: 400, epoch: 4 | loss: 0.0037277\n",
            "\tspeed: 0.2156s/iter; left time: 10181.1960s\n",
            "499it [01:47,  4.64it/s]\titers: 500, epoch: 4 | loss: 0.0004194\n",
            "\tspeed: 0.2155s/iter; left time: 10154.1661s\n",
            "599it [02:09,  4.64it/s]\titers: 600, epoch: 4 | loss: 0.0008799\n",
            "\tspeed: 0.2152s/iter; left time: 10119.0459s\n",
            "699it [02:31,  4.63it/s]\titers: 700, epoch: 4 | loss: 0.0001281\n",
            "\tspeed: 0.2155s/iter; left time: 10109.6696s\n",
            "799it [02:52,  4.63it/s]\titers: 800, epoch: 4 | loss: 0.0002401\n",
            "\tspeed: 0.2150s/iter; left time: 10064.9342s\n",
            "899it [03:14,  4.61it/s]\titers: 900, epoch: 4 | loss: 0.0000474\n",
            "\tspeed: 0.2157s/iter; left time: 10074.1263s\n",
            "999it [03:35,  4.67it/s]\titers: 1000, epoch: 4 | loss: 0.0007495\n",
            "\tspeed: 0.2152s/iter; left time: 10032.0450s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 4 cost time: 218.68298196792603\n",
            "144it [00:15,  9.11it/s]\n",
            "37it [00:04,  8.49it/s]\n",
            "Epoch: 4 | Train Loss: 0.0027129 Vali Loss: 0.0397493 Test Loss: 0.0314780 MAE Loss: 0.1423652\n",
            "Updating learning rate to 5.000000000000003e-07\n",
            "99it [00:21,  4.71it/s]\titers: 100, epoch: 5 | loss: 0.0002433\n",
            "\tspeed: 0.5383s/iter; left time: 25029.2249s\n",
            "199it [00:43,  4.63it/s]\titers: 200, epoch: 5 | loss: 0.0005167\n",
            "\tspeed: 0.2155s/iter; left time: 10000.7061s\n",
            "299it [01:04,  4.75it/s]\titers: 300, epoch: 5 | loss: 0.0016150\n",
            "\tspeed: 0.2152s/iter; left time: 9962.9282s\n",
            "399it [01:26,  4.59it/s]\titers: 400, epoch: 5 | loss: 0.0012024\n",
            "\tspeed: 0.2159s/iter; left time: 9976.3866s\n",
            "499it [01:47,  4.71it/s]\titers: 500, epoch: 5 | loss: 0.0006902\n",
            "\tspeed: 0.2154s/iter; left time: 9931.7451s\n",
            "599it [02:09,  4.60it/s]\titers: 600, epoch: 5 | loss: 0.0002963\n",
            "\tspeed: 0.2157s/iter; left time: 9920.3801s\n",
            "699it [02:30,  4.62it/s]\titers: 700, epoch: 5 | loss: 0.0011964\n",
            "\tspeed: 0.2156s/iter; left time: 9893.5655s\n",
            "799it [02:52,  4.61it/s]\titers: 800, epoch: 5 | loss: 0.0016814\n",
            "\tspeed: 0.2155s/iter; left time: 9870.2621s\n",
            "899it [03:14,  4.64it/s]\titers: 900, epoch: 5 | loss: 0.0002889\n",
            "\tspeed: 0.2157s/iter; left time: 9855.8921s\n",
            "999it [03:35,  4.62it/s]\titers: 1000, epoch: 5 | loss: 0.0007080\n",
            "\tspeed: 0.2156s/iter; left time: 9830.1630s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 5 cost time: 218.74349904060364\n",
            "144it [00:15,  9.11it/s]\n",
            "37it [00:04,  8.52it/s]\n",
            "Epoch: 5 | Train Loss: 0.0026868 Vali Loss: 0.0389509 Test Loss: 0.0307390 MAE Loss: 0.1408533\n",
            "Updating learning rate to 2.5000000000000015e-07\n",
            "99it [00:21,  4.62it/s]\titers: 100, epoch: 6 | loss: 0.0000128\n",
            "\tspeed: 0.5392s/iter; left time: 24525.9260s\n",
            "199it [00:43,  4.62it/s]\titers: 200, epoch: 6 | loss: 0.0051720\n",
            "\tspeed: 0.2157s/iter; left time: 9787.5872s\n",
            "299it [01:04,  4.65it/s]\titers: 300, epoch: 6 | loss: 0.0002961\n",
            "\tspeed: 0.2160s/iter; left time: 9781.7855s\n",
            "399it [01:26,  4.60it/s]\titers: 400, epoch: 6 | loss: 0.0014186\n",
            "\tspeed: 0.2159s/iter; left time: 9756.9685s\n",
            "499it [01:48,  4.63it/s]\titers: 500, epoch: 6 | loss: 0.0120769\n",
            "\tspeed: 0.2158s/iter; left time: 9729.5454s\n",
            "599it [02:09,  4.60it/s]\titers: 600, epoch: 6 | loss: 0.0001069\n",
            "\tspeed: 0.2159s/iter; left time: 9710.4649s\n",
            "699it [02:31,  4.71it/s]\titers: 700, epoch: 6 | loss: 0.0006061\n",
            "\tspeed: 0.2154s/iter; left time: 9666.9143s\n",
            "799it [02:52,  4.62it/s]\titers: 800, epoch: 6 | loss: 0.0004426\n",
            "\tspeed: 0.2159s/iter; left time: 9668.5530s\n",
            "899it [03:14,  4.70it/s]\titers: 900, epoch: 6 | loss: 0.0001933\n",
            "\tspeed: 0.2157s/iter; left time: 9640.8682s\n",
            "999it [03:35,  4.64it/s]\titers: 1000, epoch: 6 | loss: 0.0001602\n",
            "\tspeed: 0.2156s/iter; left time: 9614.6409s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 6 cost time: 218.98525285720825\n",
            "144it [00:15,  9.10it/s]\n",
            "37it [00:04,  8.49it/s]\n",
            "Epoch: 6 | Train Loss: 0.0027498 Vali Loss: 0.0403789 Test Loss: 0.0325097 MAE Loss: 0.1459832\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 1.2500000000000007e-07\n",
            "99it [00:21,  4.64it/s]\titers: 100, epoch: 7 | loss: 0.0003495\n",
            "\tspeed: 0.4493s/iter; left time: 19981.6081s\n",
            "199it [00:43,  4.61it/s]\titers: 200, epoch: 7 | loss: 0.0000366\n",
            "\tspeed: 0.2155s/iter; left time: 9560.8071s\n",
            "299it [01:04,  4.63it/s]\titers: 300, epoch: 7 | loss: 0.0001667\n",
            "\tspeed: 0.2158s/iter; left time: 9553.4764s\n",
            "399it [01:26,  4.63it/s]\titers: 400, epoch: 7 | loss: 0.0000497\n",
            "\tspeed: 0.2156s/iter; left time: 9521.7193s\n",
            "499it [01:47,  4.64it/s]\titers: 500, epoch: 7 | loss: 0.0002033\n",
            "\tspeed: 0.2159s/iter; left time: 9514.2734s\n",
            "599it [02:09,  4.60it/s]\titers: 600, epoch: 7 | loss: 0.0009632\n",
            "\tspeed: 0.2160s/iter; left time: 9497.9781s\n",
            "699it [02:31,  4.64it/s]\titers: 700, epoch: 7 | loss: 0.0010130\n",
            "\tspeed: 0.2155s/iter; left time: 9455.9994s\n",
            "799it [02:52,  4.61it/s]\titers: 800, epoch: 7 | loss: 0.0078307\n",
            "\tspeed: 0.2155s/iter; left time: 9434.0437s\n",
            "899it [03:14,  4.62it/s]\titers: 900, epoch: 7 | loss: 0.0000521\n",
            "\tspeed: 0.2157s/iter; left time: 9420.6312s\n",
            "999it [03:35,  4.68it/s]\titers: 1000, epoch: 7 | loss: 0.0001145\n",
            "\tspeed: 0.2152s/iter; left time: 9377.3904s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 7 cost time: 218.8138346672058\n",
            "144it [00:15,  9.09it/s]\n",
            "37it [00:04,  8.47it/s]\n",
            "Epoch: 7 | Train Loss: 0.0029241 Vali Loss: 0.0383597 Test Loss: 0.0306299 MAE Loss: 0.1399905\n",
            "Updating learning rate to 6.250000000000004e-08\n",
            "99it [00:21,  4.60it/s]\titers: 100, epoch: 8 | loss: 0.0015317\n",
            "\tspeed: 0.5330s/iter; left time: 23163.3348s\n",
            "199it [00:43,  4.72it/s]\titers: 200, epoch: 8 | loss: 0.0003345\n",
            "\tspeed: 0.2156s/iter; left time: 9348.7542s\n",
            "299it [01:04,  4.58it/s]\titers: 300, epoch: 8 | loss: 0.0004828\n",
            "\tspeed: 0.2158s/iter; left time: 9334.3028s\n",
            "399it [01:26,  4.63it/s]\titers: 400, epoch: 8 | loss: 0.0002673\n",
            "\tspeed: 0.2155s/iter; left time: 9300.7312s\n",
            "499it [01:48,  4.58it/s]\titers: 500, epoch: 8 | loss: 0.0006263\n",
            "\tspeed: 0.2163s/iter; left time: 9313.4525s\n",
            "599it [02:09,  4.63it/s]\titers: 600, epoch: 8 | loss: 0.0010227\n",
            "\tspeed: 0.2157s/iter; left time: 9264.4111s\n",
            "699it [02:31,  4.60it/s]\titers: 700, epoch: 8 | loss: 0.0105384\n",
            "\tspeed: 0.2156s/iter; left time: 9241.2507s\n",
            "799it [02:52,  4.64it/s]\titers: 800, epoch: 8 | loss: 0.0025948\n",
            "\tspeed: 0.2156s/iter; left time: 9218.8560s\n",
            "899it [03:14,  4.65it/s]\titers: 900, epoch: 8 | loss: 0.0056069\n",
            "\tspeed: 0.2156s/iter; left time: 9195.7525s\n",
            "999it [03:35,  4.60it/s]\titers: 1000, epoch: 8 | loss: 0.0020509\n",
            "\tspeed: 0.2157s/iter; left time: 9179.6646s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 8 cost time: 218.95608162879944\n",
            "144it [00:15,  9.09it/s]\n",
            "37it [00:04,  8.45it/s]\n",
            "Epoch: 8 | Train Loss: 0.0023998 Vali Loss: 0.0387703 Test Loss: 0.0312722 MAE Loss: 0.1423144\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 3.125000000000002e-08\n",
            "99it [00:21,  4.63it/s]\titers: 100, epoch: 9 | loss: 0.0012826\n",
            "\tspeed: 0.4506s/iter; left time: 19126.4714s\n",
            "199it [00:43,  4.64it/s]\titers: 200, epoch: 9 | loss: 0.0004208\n",
            "\tspeed: 0.2159s/iter; left time: 9141.4921s\n",
            "299it [01:04,  4.62it/s]\titers: 300, epoch: 9 | loss: 0.0006740\n",
            "\tspeed: 0.2157s/iter; left time: 9113.9101s\n",
            "399it [01:26,  4.63it/s]\titers: 400, epoch: 9 | loss: 0.0010042\n",
            "\tspeed: 0.2160s/iter; left time: 9102.6276s\n",
            "499it [01:48,  4.63it/s]\titers: 500, epoch: 9 | loss: 0.0052598\n",
            "\tspeed: 0.2157s/iter; left time: 9070.2190s\n",
            "599it [02:09,  4.63it/s]\titers: 600, epoch: 9 | loss: 0.0002317\n",
            "\tspeed: 0.2157s/iter; left time: 9048.2682s\n",
            "699it [02:31,  4.66it/s]\titers: 700, epoch: 9 | loss: 0.0000066\n",
            "\tspeed: 0.2156s/iter; left time: 9023.4739s\n",
            "799it [02:52,  4.62it/s]\titers: 800, epoch: 9 | loss: 0.0039158\n",
            "\tspeed: 0.2156s/iter; left time: 8999.7862s\n",
            "899it [03:14,  4.70it/s]\titers: 900, epoch: 9 | loss: 0.0015082\n",
            "\tspeed: 0.2157s/iter; left time: 8984.1541s\n",
            "999it [03:35,  4.63it/s]\titers: 1000, epoch: 9 | loss: 0.0009951\n",
            "\tspeed: 0.2161s/iter; left time: 8978.0942s\n",
            "1013it [03:39,  4.63it/s]\n",
            "Epoch: 9 cost time: 219.02279543876648\n",
            "144it [00:15,  9.10it/s]\n",
            "37it [00:04,  8.44it/s]\n",
            "Epoch: 9 | Train Loss: 0.0024149 Vali Loss: 0.0380692 Test Loss: 0.0305408 MAE Loss: 0.1399387\n",
            "Updating learning rate to 1.562500000000001e-08\n",
            "99it [00:21,  4.61it/s]\titers: 100, epoch: 10 | loss: 0.0000858\n",
            "\tspeed: 0.5388s/iter; left time: 22325.8662s\n",
            "199it [00:43,  4.62it/s]\titers: 200, epoch: 10 | loss: 0.0014414\n",
            "\tspeed: 0.2155s/iter; left time: 8908.2291s\n",
            "299it [01:04,  4.60it/s]\titers: 300, epoch: 10 | loss: 0.0002995\n",
            "\tspeed: 0.2157s/iter; left time: 8895.0178s\n",
            "399it [01:26,  4.64it/s]\titers: 400, epoch: 10 | loss: 0.0002192\n",
            "\tspeed: 0.2159s/iter; left time: 8879.2631s\n",
            "499it [01:48,  4.59it/s]\titers: 500, epoch: 10 | loss: 0.0017326\n",
            "\tspeed: 0.2157s/iter; left time: 8851.0732s\n",
            "599it [02:09,  4.63it/s]\titers: 600, epoch: 10 | loss: 0.0085594\n",
            "\tspeed: 0.2159s/iter; left time: 8836.3611s\n",
            "699it [02:31,  4.60it/s]\titers: 700, epoch: 10 | loss: 0.0002684\n",
            "\tspeed: 0.2161s/iter; left time: 8825.9894s\n",
            "799it [02:52,  4.61it/s]\titers: 800, epoch: 10 | loss: 0.0286764\n",
            "\tspeed: 0.2161s/iter; left time: 8800.6566s\n",
            "899it [03:14,  4.67it/s]\titers: 900, epoch: 10 | loss: 0.0001236\n",
            "\tspeed: 0.2158s/iter; left time: 8768.1409s\n",
            "999it [03:35,  4.60it/s]\titers: 1000, epoch: 10 | loss: 0.0002769\n",
            "\tspeed: 0.2157s/iter; left time: 8741.5593s\n",
            "1013it [03:39,  4.62it/s]\n",
            "Epoch: 10 cost time: 219.0467586517334\n",
            "144it [00:15,  9.11it/s]\n",
            "37it [00:04,  8.42it/s]\n",
            "Epoch: 10 | Train Loss: 0.0026204 Vali Loss: 0.0376273 Test Loss: 0.0298394 MAE Loss: 0.1383114\n",
            "Updating learning rate to 7.812500000000005e-09\n",
            "99it [00:21,  4.61it/s]\titers: 100, epoch: 11 | loss: 0.0020757\n",
            "\tspeed: 0.5377s/iter; left time: 21734.5686s\n",
            "199it [00:43,  4.76it/s]\titers: 200, epoch: 11 | loss: 0.0045434\n",
            "\tspeed: 0.2155s/iter; left time: 8690.9467s\n",
            "299it [01:04,  4.63it/s]\titers: 300, epoch: 11 | loss: 0.0000855\n",
            "\tspeed: 0.2157s/iter; left time: 8676.6319s\n",
            "399it [01:26,  4.71it/s]\titers: 400, epoch: 11 | loss: 0.0000226\n",
            "\tspeed: 0.2156s/iter; left time: 8651.1009s\n",
            "499it [01:47,  4.55it/s]\titers: 500, epoch: 11 | loss: 0.0000115\n",
            "\tspeed: 0.2154s/iter; left time: 8620.7725s\n",
            "599it [02:09,  4.55it/s]\titers: 600, epoch: 11 | loss: 0.0036158\n",
            "\tspeed: 0.2202s/iter; left time: 8788.7581s\n",
            "699it [02:31,  4.75it/s]\titers: 700, epoch: 11 | loss: 0.0002747\n",
            "\tspeed: 0.2179s/iter; left time: 8677.3392s\n",
            "799it [02:53,  4.62it/s]\titers: 800, epoch: 11 | loss: 0.0000723\n",
            "\tspeed: 0.2158s/iter; left time: 8570.2216s\n",
            "899it [03:14,  4.62it/s]\titers: 900, epoch: 11 | loss: 0.0002003\n",
            "\tspeed: 0.2156s/iter; left time: 8544.1054s\n",
            "999it [03:36,  4.63it/s]\titers: 1000, epoch: 11 | loss: 0.0009761\n",
            "\tspeed: 0.2159s/iter; left time: 8532.6988s\n",
            "1013it [03:39,  4.61it/s]\n",
            "Epoch: 11 cost time: 219.5303385257721\n",
            "144it [00:15,  9.09it/s]\n",
            "37it [00:04,  8.57it/s]\n",
            "Epoch: 11 | Train Loss: 0.0027843 Vali Loss: 0.0373629 Test Loss: 0.0299241 MAE Loss: 0.1379990\n",
            "Updating learning rate to 3.906250000000002e-09\n",
            "99it [00:21,  4.65it/s]\titers: 100, epoch: 12 | loss: 0.0008464\n",
            "\tspeed: 0.5369s/iter; left time: 21159.6593s\n",
            "199it [00:43,  4.62it/s]\titers: 200, epoch: 12 | loss: 0.0052274\n",
            "\tspeed: 0.2162s/iter; left time: 8498.2289s\n",
            "299it [01:04,  4.81it/s]\titers: 300, epoch: 12 | loss: 0.0001741\n",
            "\tspeed: 0.2158s/iter; left time: 8462.8402s\n",
            "399it [01:26,  4.61it/s]\titers: 400, epoch: 12 | loss: 0.0003226\n",
            "\tspeed: 0.2159s/iter; left time: 8442.1606s\n",
            "499it [01:48,  4.67it/s]\titers: 500, epoch: 12 | loss: 0.0007998\n",
            "\tspeed: 0.2160s/iter; left time: 8427.2718s\n",
            "599it [02:09,  4.60it/s]\titers: 600, epoch: 12 | loss: 0.0056698\n",
            "\tspeed: 0.2159s/iter; left time: 8401.5495s\n",
            "699it [02:31,  4.69it/s]\titers: 700, epoch: 12 | loss: 0.0005134\n",
            "\tspeed: 0.2156s/iter; left time: 8366.7945s\n",
            "799it [02:52,  4.59it/s]\titers: 800, epoch: 12 | loss: 0.0020902\n",
            "\tspeed: 0.2157s/iter; left time: 8350.5975s\n",
            "899it [03:14,  4.61it/s]\titers: 900, epoch: 12 | loss: 0.0012814\n",
            "\tspeed: 0.2158s/iter; left time: 8331.0891s\n",
            "999it [03:36,  4.61it/s]\titers: 1000, epoch: 12 | loss: 0.0008964\n",
            "\tspeed: 0.2159s/iter; left time: 8314.9905s\n",
            "1013it [03:39,  4.62it/s]\n",
            "Epoch: 12 cost time: 219.12704706192017\n",
            "144it [00:15,  9.07it/s]\n",
            "37it [00:04,  8.52it/s]\n",
            "Epoch: 12 | Train Loss: 0.0023225 Vali Loss: 0.0380351 Test Loss: 0.0300951 MAE Loss: 0.1392715\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 1.953125000000001e-09\n",
            "99it [00:21,  4.63it/s]\titers: 100, epoch: 13 | loss: 0.0002423\n",
            "\tspeed: 0.4500s/iter; left time: 17277.2793s\n",
            "199it [00:43,  4.62it/s]\titers: 200, epoch: 13 | loss: 0.0002758\n",
            "\tspeed: 0.2160s/iter; left time: 8272.7056s\n",
            "299it [01:04,  4.61it/s]\titers: 300, epoch: 13 | loss: 0.0000391\n",
            "\tspeed: 0.2153s/iter; left time: 8223.7481s\n",
            "399it [01:26,  4.60it/s]\titers: 400, epoch: 13 | loss: 0.0006831\n",
            "\tspeed: 0.2157s/iter; left time: 8216.5293s\n",
            "499it [01:47,  4.61it/s]\titers: 500, epoch: 13 | loss: 0.0093105\n",
            "\tspeed: 0.2158s/iter; left time: 8199.8286s\n",
            "599it [02:09,  4.58it/s]\titers: 600, epoch: 13 | loss: 0.0000937\n",
            "\tspeed: 0.2158s/iter; left time: 8177.6411s\n",
            "699it [02:31,  4.64it/s]\titers: 700, epoch: 13 | loss: 0.0000425\n",
            "\tspeed: 0.2152s/iter; left time: 8133.2157s\n",
            "799it [02:52,  4.64it/s]\titers: 800, epoch: 13 | loss: 0.0001796\n",
            "\tspeed: 0.2157s/iter; left time: 8131.1036s\n",
            "899it [03:14,  4.62it/s]\titers: 900, epoch: 13 | loss: 0.0002403\n",
            "\tspeed: 0.2158s/iter; left time: 8111.6523s\n",
            "999it [03:35,  4.69it/s]\titers: 1000, epoch: 13 | loss: 0.0000769\n",
            "\tspeed: 0.2153s/iter; left time: 8074.0958s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 13 cost time: 218.84639477729797\n",
            "144it [00:15,  9.12it/s]\n",
            "37it [00:04,  8.45it/s]\n",
            "Epoch: 13 | Train Loss: 0.0029148 Vali Loss: 0.0376636 Test Loss: 0.0299083 MAE Loss: 0.1381785\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Updating learning rate to 9.765625000000006e-10\n",
            "99it [00:21,  4.63it/s]\titers: 100, epoch: 14 | loss: 0.0000134\n",
            "\tspeed: 0.4492s/iter; left time: 16793.6545s\n",
            "199it [00:43,  4.70it/s]\titers: 200, epoch: 14 | loss: 0.0005017\n",
            "\tspeed: 0.2154s/iter; left time: 8029.7926s\n",
            "299it [01:04,  4.62it/s]\titers: 300, epoch: 14 | loss: 0.0014674\n",
            "\tspeed: 0.2157s/iter; left time: 8018.7353s\n",
            "399it [01:26,  4.69it/s]\titers: 400, epoch: 14 | loss: 0.0003475\n",
            "\tspeed: 0.2154s/iter; left time: 7986.2869s\n",
            "499it [01:47,  4.61it/s]\titers: 500, epoch: 14 | loss: 0.0018211\n",
            "\tspeed: 0.2158s/iter; left time: 7980.9354s\n",
            "599it [02:09,  4.64it/s]\titers: 600, epoch: 14 | loss: 0.0001475\n",
            "\tspeed: 0.2154s/iter; left time: 7945.3569s\n",
            "699it [02:30,  4.62it/s]\titers: 700, epoch: 14 | loss: 0.0008038\n",
            "\tspeed: 0.2156s/iter; left time: 7929.6964s\n",
            "799it [02:52,  4.64it/s]\titers: 800, epoch: 14 | loss: 0.0010099\n",
            "\tspeed: 0.2157s/iter; left time: 7910.8629s\n",
            "899it [03:14,  4.61it/s]\titers: 900, epoch: 14 | loss: 0.0029427\n",
            "\tspeed: 0.2157s/iter; left time: 7889.7324s\n",
            "999it [03:35,  4.64it/s]\titers: 1000, epoch: 14 | loss: 0.0004850\n",
            "\tspeed: 0.2155s/iter; left time: 7863.0831s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 14 cost time: 218.75415706634521\n",
            "144it [00:15,  9.11it/s]\n",
            "37it [00:04,  8.53it/s]\n",
            "Epoch: 14 | Train Loss: 0.0025385 Vali Loss: 0.0375393 Test Loss: 0.0299560 MAE Loss: 0.1382151\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Updating learning rate to 4.882812500000003e-10\n",
            "99it [00:21,  4.63it/s]\titers: 100, epoch: 15 | loss: 0.0003520\n",
            "\tspeed: 0.4490s/iter; left time: 16328.5874s\n",
            "199it [00:43,  4.63it/s]\titers: 200, epoch: 15 | loss: 0.0009100\n",
            "\tspeed: 0.2157s/iter; left time: 7821.5803s\n",
            "299it [01:04,  4.64it/s]\titers: 300, epoch: 15 | loss: 0.0064690\n",
            "\tspeed: 0.2150s/iter; left time: 7775.2256s\n",
            "399it [01:26,  4.62it/s]\titers: 400, epoch: 15 | loss: 0.0000290\n",
            "\tspeed: 0.2156s/iter; left time: 7776.7109s\n",
            "499it [01:47,  4.71it/s]\titers: 500, epoch: 15 | loss: 0.0001761\n",
            "\tspeed: 0.2152s/iter; left time: 7739.7617s\n",
            "599it [02:09,  4.59it/s]\titers: 600, epoch: 15 | loss: 0.0001086\n",
            "\tspeed: 0.2157s/iter; left time: 7738.0181s\n",
            "699it [02:30,  4.66it/s]\titers: 700, epoch: 15 | loss: 0.0007151\n",
            "\tspeed: 0.2156s/iter; left time: 7713.1504s\n",
            "799it [02:52,  4.61it/s]\titers: 800, epoch: 15 | loss: 0.0005961\n",
            "\tspeed: 0.2159s/iter; left time: 7700.5428s\n",
            "899it [03:14,  4.63it/s]\titers: 900, epoch: 15 | loss: 0.0455445\n",
            "\tspeed: 0.2158s/iter; left time: 7676.1770s\n",
            "999it [03:35,  4.61it/s]\titers: 1000, epoch: 15 | loss: 0.0000396\n",
            "\tspeed: 0.2154s/iter; left time: 7640.5668s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 15 cost time: 218.7329351902008\n",
            "144it [00:15,  9.11it/s]\n",
            "37it [00:04,  8.48it/s]\n",
            "Epoch: 15 | Train Loss: 0.0028348 Vali Loss: 0.0372521 Test Loss: 0.0300665 MAE Loss: 0.1386077\n",
            "Updating learning rate to 2.4414062500000014e-10\n",
            "99it [00:21,  4.62it/s]\titers: 100, epoch: 16 | loss: 0.0015536\n",
            "\tspeed: 0.5385s/iter; left time: 19038.6264s\n",
            "199it [00:43,  4.75it/s]\titers: 200, epoch: 16 | loss: 0.0012390\n",
            "\tspeed: 0.2152s/iter; left time: 7586.8078s\n",
            "299it [01:04,  4.64it/s]\titers: 300, epoch: 16 | loss: 0.0025955\n",
            "\tspeed: 0.2156s/iter; left time: 7578.6810s\n",
            "399it [01:26,  4.75it/s]\titers: 400, epoch: 16 | loss: 0.0002068\n",
            "\tspeed: 0.2154s/iter; left time: 7551.1452s\n",
            "499it [01:47,  4.60it/s]\titers: 500, epoch: 16 | loss: 0.0000290\n",
            "\tspeed: 0.2156s/iter; left time: 7538.0662s\n",
            "599it [02:09,  4.71it/s]\titers: 600, epoch: 16 | loss: 0.0147096\n",
            "\tspeed: 0.2156s/iter; left time: 7513.6545s\n",
            "699it [02:30,  4.60it/s]\titers: 700, epoch: 16 | loss: 0.0004226\n",
            "\tspeed: 0.2153s/iter; left time: 7483.0338s\n",
            "799it [02:52,  4.61it/s]\titers: 800, epoch: 16 | loss: 0.0001357\n",
            "\tspeed: 0.2151s/iter; left time: 7452.9931s\n",
            "899it [03:14,  4.59it/s]\titers: 900, epoch: 16 | loss: 0.0003984\n",
            "\tspeed: 0.2156s/iter; left time: 7451.3656s\n",
            "999it [03:35,  4.65it/s]\titers: 1000, epoch: 16 | loss: 0.0015437\n",
            "\tspeed: 0.2155s/iter; left time: 7424.6709s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 16 cost time: 218.6865758895874\n",
            "144it [00:15,  9.11it/s]\n",
            "37it [00:04,  8.52it/s]\n",
            "Epoch: 16 | Train Loss: 0.0026899 Vali Loss: 0.0380435 Test Loss: 0.0298915 MAE Loss: 0.1383874\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 1.2207031250000007e-10\n",
            "99it [00:21,  4.58it/s]\titers: 100, epoch: 17 | loss: 0.0007713\n",
            "\tspeed: 0.4494s/iter; left time: 15432.4180s\n",
            "199it [00:43,  4.63it/s]\titers: 200, epoch: 17 | loss: 0.0023457\n",
            "\tspeed: 0.2154s/iter; left time: 7376.0635s\n",
            "299it [01:04,  4.61it/s]\titers: 300, epoch: 17 | loss: 0.0007024\n",
            "\tspeed: 0.2157s/iter; left time: 7366.3101s\n",
            "399it [01:26,  4.64it/s]\titers: 400, epoch: 17 | loss: 0.0001349\n",
            "\tspeed: 0.2151s/iter; left time: 7323.7557s\n",
            "499it [01:47,  4.61it/s]\titers: 500, epoch: 17 | loss: 0.0004054\n",
            "\tspeed: 0.2156s/iter; left time: 7319.6918s\n",
            "599it [02:09,  4.62it/s]\titers: 600, epoch: 17 | loss: 0.0015218\n",
            "\tspeed: 0.2157s/iter; left time: 7299.4530s\n",
            "699it [02:30,  4.65it/s]\titers: 700, epoch: 17 | loss: 0.0003655\n",
            "\tspeed: 0.2152s/iter; left time: 7262.9444s\n",
            "799it [02:52,  4.64it/s]\titers: 800, epoch: 17 | loss: 0.0004229\n",
            "\tspeed: 0.2157s/iter; left time: 7255.4098s\n",
            "899it [03:14,  4.62it/s]\titers: 900, epoch: 17 | loss: 0.0000963\n",
            "\tspeed: 0.2152s/iter; left time: 7219.2109s\n",
            "999it [03:35,  4.64it/s]\titers: 1000, epoch: 17 | loss: 0.0004077\n",
            "\tspeed: 0.2153s/iter; left time: 7201.2888s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 17 cost time: 218.6625726222992\n",
            "144it [00:15,  9.11it/s]\n",
            "37it [00:04,  8.46it/s]\n",
            "Epoch: 17 | Train Loss: 0.0026585 Vali Loss: 0.0375025 Test Loss: 0.0299950 MAE Loss: 0.1386078\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Updating learning rate to 6.103515625000004e-11\n",
            "99it [00:21,  4.63it/s]\titers: 100, epoch: 18 | loss: 0.0002380\n",
            "\tspeed: 0.4491s/iter; left time: 14967.7428s\n",
            "199it [00:43,  4.64it/s]\titers: 200, epoch: 18 | loss: 0.0000186\n",
            "\tspeed: 0.2154s/iter; left time: 7158.8932s\n",
            "299it [01:04,  4.61it/s]\titers: 300, epoch: 18 | loss: 0.0189847\n",
            "\tspeed: 0.2154s/iter; left time: 7136.3175s\n",
            "399it [01:26,  4.64it/s]\titers: 400, epoch: 18 | loss: 0.0002325\n",
            "\tspeed: 0.2154s/iter; left time: 7115.3860s\n",
            "499it [01:47,  4.64it/s]\titers: 500, epoch: 18 | loss: 0.0001553\n",
            "\tspeed: 0.2156s/iter; left time: 7099.5385s\n",
            "599it [02:09,  4.62it/s]\titers: 600, epoch: 18 | loss: 0.0005949\n",
            "\tspeed: 0.2157s/iter; left time: 7082.9288s\n",
            "699it [02:30,  4.62it/s]\titers: 700, epoch: 18 | loss: 0.0001115\n",
            "\tspeed: 0.2156s/iter; left time: 7055.5581s\n",
            "799it [02:52,  4.63it/s]\titers: 800, epoch: 18 | loss: 0.0587322\n",
            "\tspeed: 0.2160s/iter; left time: 7049.1115s\n",
            "899it [03:14,  4.63it/s]\titers: 900, epoch: 18 | loss: 0.0002665\n",
            "\tspeed: 0.2158s/iter; left time: 7019.4561s\n",
            "999it [03:35,  4.65it/s]\titers: 1000, epoch: 18 | loss: 0.0014167\n",
            "\tspeed: 0.2154s/iter; left time: 6986.4190s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 18 cost time: 218.7951419353485\n",
            "144it [00:15,  9.11it/s]\n",
            "37it [00:04,  8.42it/s]\n",
            "Epoch: 18 | Train Loss: 0.0026867 Vali Loss: 0.0375979 Test Loss: 0.0300382 MAE Loss: 0.1388846\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Updating learning rate to 3.051757812500002e-11\n",
            "99it [00:21,  4.60it/s]\titers: 100, epoch: 19 | loss: 0.0033529\n",
            "\tspeed: 0.4501s/iter; left time: 14546.1014s\n",
            "199it [00:43,  4.64it/s]\titers: 200, epoch: 19 | loss: 0.0003573\n",
            "\tspeed: 0.2151s/iter; left time: 6930.3793s\n",
            "299it [01:04,  4.64it/s]\titers: 300, epoch: 19 | loss: 0.0001115\n",
            "\tspeed: 0.2153s/iter; left time: 6915.3256s\n",
            "399it [01:26,  4.61it/s]\titers: 400, epoch: 19 | loss: 0.0001217\n",
            "\tspeed: 0.2155s/iter; left time: 6900.0866s\n",
            "499it [01:47,  4.68it/s]\titers: 500, epoch: 19 | loss: 0.0007295\n",
            "\tspeed: 0.2155s/iter; left time: 6879.1017s\n",
            "599it [02:09,  4.62it/s]\titers: 600, epoch: 19 | loss: 0.0002436\n",
            "\tspeed: 0.2158s/iter; left time: 6866.2042s\n",
            "699it [02:30,  4.66it/s]\titers: 700, epoch: 19 | loss: 0.0000056\n",
            "\tspeed: 0.2154s/iter; left time: 6832.2956s\n",
            "799it [02:52,  4.62it/s]\titers: 800, epoch: 19 | loss: 0.0002666\n",
            "\tspeed: 0.2156s/iter; left time: 6816.2590s\n",
            "899it [03:14,  4.66it/s]\titers: 900, epoch: 19 | loss: 0.0002738\n",
            "\tspeed: 0.2155s/iter; left time: 6790.4484s\n",
            "999it [03:35,  4.60it/s]\titers: 1000, epoch: 19 | loss: 0.0021714\n",
            "\tspeed: 0.2156s/iter; left time: 6773.7608s\n",
            "1013it [03:38,  4.63it/s]\n",
            "Epoch: 19 cost time: 218.70651054382324\n",
            "144it [00:15,  9.12it/s]\n",
            "37it [00:04,  8.41it/s]\n",
            "Epoch: 19 | Train Loss: 0.0023064 Vali Loss: 0.0375197 Test Loss: 0.0300590 MAE Loss: 0.1387293\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Updating learning rate to 1.525878906250001e-11\n",
            "99it [00:21,  4.66it/s]\titers: 100, epoch: 20 | loss: 0.0003429\n",
            "\tspeed: 0.4498s/iter; left time: 14080.4887s\n",
            "199it [00:43,  4.62it/s]\titers: 200, epoch: 20 | loss: 0.0098841\n",
            "\tspeed: 0.2156s/iter; left time: 6728.2521s\n",
            "299it [01:04,  4.75it/s]\titers: 300, epoch: 20 | loss: 0.0010496\n",
            "\tspeed: 0.2148s/iter; left time: 6682.0975s\n",
            "399it [01:26,  4.64it/s]\titers: 400, epoch: 20 | loss: 0.0000471\n",
            "\tspeed: 0.2156s/iter; left time: 6685.9122s\n",
            "499it [01:47,  4.61it/s]\titers: 500, epoch: 20 | loss: 0.0000586\n",
            "\tspeed: 0.2159s/iter; left time: 6671.2572s\n",
            "599it [02:09,  4.72it/s]\titers: 600, epoch: 20 | loss: 0.0000700\n",
            "\tspeed: 0.2159s/iter; left time: 6651.6294s\n",
            "699it [02:31,  4.62it/s]\titers: 700, epoch: 20 | loss: 0.0002282\n",
            "\tspeed: 0.2154s/iter; left time: 6615.0295s\n",
            "799it [02:52,  4.63it/s]\titers: 800, epoch: 20 | loss: 0.0005989\n",
            "\tspeed: 0.2142s/iter; left time: 6554.2358s\n",
            "899it [03:13,  4.63it/s]\titers: 900, epoch: 20 | loss: 0.0001596\n",
            "\tspeed: 0.2146s/iter; left time: 6546.4499s\n",
            "999it [03:35,  4.61it/s]\titers: 1000, epoch: 20 | loss: 0.0003270\n",
            "\tspeed: 0.2142s/iter; left time: 6511.5680s\n",
            "1013it [03:38,  4.64it/s]\n",
            "Epoch: 20 cost time: 218.39334440231323\n",
            "144it [00:15,  9.27it/s]\n",
            "37it [00:04,  8.45it/s]\n",
            "Epoch: 20 | Train Loss: 0.0028663 Vali Loss: 0.0374753 Test Loss: 0.0299608 MAE Loss: 0.1384854\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n",
            "0it [00:00, ?it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[163.76 164.66 162.03 160.8  160.1 ]\n",
            "------------ 0   2022-04-05\n",
            "1   2022-04-06\n",
            "2   2022-04-10\n",
            "3   2022-04-11\n",
            "4   2022-04-12\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "1it [00:00,  2.05it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[165.56 165.21 165.23 166.47 167.63]\n",
            "------------ 0   2022-04-13\n",
            "1   2022-04-14\n",
            "2   2022-04-17\n",
            "3   2022-04-18\n",
            "4   2022-04-19\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "2it [00:00,  3.79it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[166.65    165.02    165.32999 163.76999 163.76   ]\n",
            "------------ 0   2022-04-20\n",
            "1   2022-04-21\n",
            "2   2022-04-24\n",
            "3   2022-04-25\n",
            "4   2022-04-26\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "3it [00:00,  5.17it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[168.40999 169.68    169.59    168.54    167.45   ]\n",
            "------------ 0   2022-04-27\n",
            "1   2022-04-28\n",
            "2   2022-05-01\n",
            "3   2022-05-02\n",
            "4   2022-05-03\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "4it [00:00,  6.08it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[165.79    173.57    173.49998 171.77    173.55998]\n",
            "------------ 0   2022-05-04\n",
            "1   2022-05-05\n",
            "2   2022-05-08\n",
            "3   2022-05-09\n",
            "4   2022-05-10\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "5it [00:00,  6.87it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[173.75    172.56999 172.07    172.07    172.68999]\n",
            "------------ 0   2022-05-11\n",
            "1   2022-05-12\n",
            "2   2022-05-15\n",
            "3   2022-05-16\n",
            "4   2022-05-17\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "6it [00:01,  7.31it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[175.05    175.15999 174.2     171.56    171.83998]\n",
            "------------ 0   2022-05-18\n",
            "1   2022-05-19\n",
            "2   2022-05-22\n",
            "3   2022-05-23\n",
            "4   2022-05-24\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "7it [00:01,  7.73it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[172.98999 175.42998 177.29999 177.25    180.09   ]\n",
            "------------ 0   2022-05-25\n",
            "1   2022-05-26\n",
            "2   2022-05-30\n",
            "3   2022-05-31\n",
            "4   2022-06-01\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "8it [00:01,  8.16it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[180.95    179.57999 179.21    177.82    180.57   ]\n",
            "------------ 0   2022-06-02\n",
            "1   2022-06-05\n",
            "2   2022-06-06\n",
            "3   2022-06-07\n",
            "4   2022-06-08\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "9it [00:01,  8.44it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[180.96    183.79    183.31    183.94998 186.01   ]\n",
            "------------ 0   2022-06-09\n",
            "1   2022-06-12\n",
            "2   2022-06-13\n",
            "3   2022-06-14\n",
            "4   2022-06-15\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "10it [00:01,  8.60it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[184.91998 185.00998 183.96    186.99998 186.68   ]\n",
            "------------ 0   2022-06-16\n",
            "1   2022-06-20\n",
            "2   2022-06-21\n",
            "3   2022-06-22\n",
            "4   2022-06-23\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "11it [00:01,  8.70it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[185.27    188.06    189.25    189.58998 193.97   ]\n",
            "------------ 0   2022-06-26\n",
            "1   2022-06-27\n",
            "2   2022-06-28\n",
            "3   2022-06-29\n",
            "4   2022-06-30\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "12it [00:01,  8.79it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[192.46    191.32999 191.81    190.68    188.61   ]\n",
            "------------ 0   2022-07-03\n",
            "1   2022-07-05\n",
            "2   2022-07-06\n",
            "3   2022-07-07\n",
            "4   2022-07-10\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "13it [00:01,  8.84it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[188.07999 189.76999 190.54    190.69    193.98999]\n",
            "------------ 0   2022-07-11\n",
            "1   2022-07-12\n",
            "2   2022-07-13\n",
            "3   2022-07-14\n",
            "4   2022-07-17\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "14it [00:01,  8.90it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[193.73    195.09999 193.13    191.93999 192.74998]\n",
            "------------ 0   2022-07-18\n",
            "1   2022-07-19\n",
            "2   2022-07-20\n",
            "3   2022-07-21\n",
            "4   2022-07-24\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "15it [00:02,  8.98it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[193.61998 194.49998 193.22    195.83    196.45   ]\n",
            "------------ 0   2022-07-25\n",
            "1   2022-07-26\n",
            "2   2022-07-27\n",
            "3   2022-07-28\n",
            "4   2022-07-31\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "16it [00:02,  9.03it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[195.60999 192.58    191.17    181.99    178.85   ]\n",
            "------------ 0   2022-08-01\n",
            "1   2022-08-02\n",
            "2   2022-08-03\n",
            "3   2022-08-04\n",
            "4   2022-08-07\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "17it [00:02,  9.06it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[179.79999 178.19    177.97    177.79    179.46   ]\n",
            "------------ 0   2022-08-08\n",
            "1   2022-08-09\n",
            "2   2022-08-10\n",
            "3   2022-08-11\n",
            "4   2022-08-14\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "18it [00:02,  8.98it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[177.45    176.56999 174.      174.48999 175.84   ]\n",
            "------------ 0   2022-08-15\n",
            "1   2022-08-16\n",
            "2   2022-08-17\n",
            "3   2022-08-18\n",
            "4   2022-08-21\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "19it [00:02,  9.00it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[177.22998 181.11998 176.38    178.61    180.18999]\n",
            "------------ 0   2022-08-22\n",
            "1   2022-08-23\n",
            "2   2022-08-24\n",
            "3   2022-08-25\n",
            "4   2022-08-28\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "20it [00:02,  8.99it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[184.12    187.65    187.86998 189.46    189.7    ]\n",
            "------------ 0   2022-08-29\n",
            "1   2022-08-30\n",
            "2   2022-08-31\n",
            "3   2022-09-01\n",
            "4   2022-09-05\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "21it [00:02,  9.04it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[182.91    177.56    178.18    179.36    176.29999]\n",
            "------------ 0   2022-09-06\n",
            "1   2022-09-07\n",
            "2   2022-09-08\n",
            "3   2022-09-11\n",
            "4   2022-09-12\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "22it [00:02,  9.01it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[174.21    175.74    175.01    177.97    179.06999]\n",
            "------------ 0   2022-09-13\n",
            "1   2022-09-14\n",
            "2   2022-09-15\n",
            "3   2022-09-18\n",
            "4   2022-09-19\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "23it [00:02,  9.04it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[175.48999 173.93    174.78998 176.08    171.95999]\n",
            "------------ 0   2022-09-20\n",
            "1   2022-09-21\n",
            "2   2022-09-22\n",
            "3   2022-09-25\n",
            "4   2022-09-26\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[170.43    170.69    171.20999 173.75    172.4    ]\n",
            "------------ 0   2022-09-27\n",
            "1   2022-09-28\n",
            "2   2022-09-29\n",
            "3   2022-10-02\n",
            "4   2022-10-03\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "25it [00:03,  9.24it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[173.66 174.91 177.49 178.99 178.39]\n",
            "------------ 0   2022-10-04\n",
            "1   2022-10-05\n",
            "2   2022-10-06\n",
            "3   2022-10-09\n",
            "4   2022-10-10\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "26it [00:03,  9.14it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[179.79999 180.71    178.85    178.72    177.15   ]\n",
            "------------ 0   2022-10-11\n",
            "1   2022-10-12\n",
            "2   2022-10-13\n",
            "3   2022-10-16\n",
            "4   2022-10-17\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "27it [00:03,  9.05it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[175.84    175.45999 172.88    173.      173.43999]\n",
            "------------ 0   2022-10-18\n",
            "1   2022-10-19\n",
            "2   2022-10-20\n",
            "3   2022-10-23\n",
            "4   2022-10-24\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "28it [00:03,  8.98it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[171.1  166.89 168.22 170.29 170.77]\n",
            "------------ 0   2022-10-25\n",
            "1   2022-10-26\n",
            "2   2022-10-27\n",
            "3   2022-10-30\n",
            "4   2022-10-31\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "29it [00:03,  8.95it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[173.97    177.56999 176.65    179.23    181.81999]\n",
            "------------ 0   2022-11-01\n",
            "1   2022-11-02\n",
            "2   2022-11-03\n",
            "3   2022-11-06\n",
            "4   2022-11-07\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "30it [00:03,  8.96it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[182.89    182.41    186.4     184.79999 187.44   ]\n",
            "------------ 0   2022-11-08\n",
            "1   2022-11-09\n",
            "2   2022-11-10\n",
            "3   2022-11-13\n",
            "4   2022-11-14\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "31it [00:03,  8.96it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[188.01    189.70999 189.69    191.44998 190.63998]\n",
            "------------ 0   2022-11-15\n",
            "1   2022-11-16\n",
            "2   2022-11-17\n",
            "3   2022-11-20\n",
            "4   2022-11-21\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "32it [00:03,  8.95it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[191.31    189.97    189.79    190.39998 189.36998]\n",
            "------------ 0   2022-11-22\n",
            "1   2022-11-24\n",
            "2   2022-11-27\n",
            "3   2022-11-28\n",
            "4   2022-11-29\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "33it [00:04,  8.96it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[189.94998 191.23999 189.42998 193.42    192.32   ]\n",
            "------------ 0   2022-11-30\n",
            "1   2022-12-01\n",
            "2   2022-12-04\n",
            "3   2022-12-05\n",
            "4   2022-12-06\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "34it [00:04,  8.98it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[194.27    195.71    193.18    194.70999 197.95999]\n",
            "------------ 0   2022-12-07\n",
            "1   2022-12-08\n",
            "2   2022-12-11\n",
            "3   2022-12-12\n",
            "4   2022-12-13\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "35it [00:04,  8.97it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[198.10999 197.56999 195.89    196.93999 194.82999]\n",
            "------------ 0   2022-12-14\n",
            "1   2022-12-15\n",
            "2   2022-12-18\n",
            "3   2022-12-19\n",
            "4   2022-12-20\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "36it [00:04,  8.96it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[194.67998 193.59999 193.04999 193.15    193.58   ]\n",
            "------------ 0   2022-12-21\n",
            "1   2022-12-22\n",
            "2   2022-12-26\n",
            "3   2022-12-27\n",
            "4   2022-12-28\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "37it [00:04,  8.14it/s]\n",
            "Results saved to predictions_2023.csv\n"
          ]
        }
      ],
      "source": [
        "!python run_main.py \\\n",
        "    --task_name long_term_forecast \\\n",
        "    --is_training 1 \\\n",
        "    --model_id AAPL_TimeLLM \\\n",
        "    --model TimeLLM \\\n",
        "    --data_path 'data.csv' \\\n",
        "    --root_path '/home/fakoor/Desktop/chitsaz/emotion_detection/emotions' \\\n",
        "    --model_comment \"prediction\" \\\n",
        "    --target 'y' \\\n",
        "    --freq 'd' \\\n",
        "    --seq_len 64 \\\n",
        "    --label_len 64 \\\n",
        "    --pred_len 1 \\\n",
        "    --train_epochs 50 \\\n",
        "    --batch_size 5 \\\n",
        "    --learning_rate 0.0001 \\\n",
        "    --patience 5 \\\n",
        "    --checkpoints './checkpoints/emotion' \\\n",
        "    --loss MSE \\\n",
        "    --lradj type1 \\\n",
        "    --prompt_domain 1 \\\n",
        "    --data Traffic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "!export RDMAV_FORK_SAFE=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTZY0yHGDLlG",
        "outputId": "372e4790-d89f-4e5e-87a2-d555f9d608aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mpi4py\n",
            "  Using cached mpi4py-4.0.0.tar.gz (464 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for mpi4py: filename=mpi4py-4.0.0-cp310-cp310-linux_x86_64.whl size=4856817 sha256=7ef26bdd6d385be6ace823285d482964ac0c5f3b383ee25d74de02710f291b8a\n",
            "  Stored in directory: /home/fakoor/.cache/pip/wheels/96/17/12/83db63ee0ae5c4b040ee87f2e5c813aea4728b55ec6a37317c\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mpi4py\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
