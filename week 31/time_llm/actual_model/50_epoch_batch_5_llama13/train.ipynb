{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd /home/fakoor/Desktop/chitsaz/emotion_detection/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa5GTlUs2306",
        "outputId": "bd5b5e11-a69d-4b36-c910-9950053b9975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'stock_time_llm'...\n",
            "remote: Enumerating objects: 90, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 90 (delta 34), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (90/90), 1.07 MiB | 1.87 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "/home/fakoor/Desktop/chitsaz/emotion_detection/stock_time_llm\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/androidwoman/stock_time_llm.git\n",
        "%cd /home/fakoor/Desktop/chitsaz/emotion_detection/stock_time_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hH3daSu_B_S2",
        "outputId": "2d5f182a-48d6-4adb-f60d-5cf41819f671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==2.2.2 (from -r requirements.txt (line 1))\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting accelerate==0.28.0 (from -r requirements.txt (line 2))\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting einops==0.7.0 (from -r requirements.txt (line 3))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting matplotlib==3.7.0 (from -r requirements.txt (line 4))\n",
            "  Downloading matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 5))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting pandas==1.5.3 (from -r requirements.txt (line 6))\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scikit_learn==1.2.2 (from -r requirements.txt (line 7))\n",
            "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.12.0 (from -r requirements.txt (line 8))\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting tqdm==4.65.0 (from -r requirements.txt (line 9))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
            "Collecting peft==0.4.0 (from -r requirements.txt (line 10))\n",
            "  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting transformers==4.31.0 (from -r requirements.txt (line 11))\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "Collecting deepspeed==0.14.0 (from -r requirements.txt (line 12))\n",
            "  Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m963.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting sentencepiece==0.2.0 (from -r requirements.txt (line 13))\n",
            "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: filelock in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (12.1.105)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2->-r requirements.txt (line 1))\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: psutil in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (6.0.0)\n",
            "Requirement already satisfied: pyyaml in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (0.24.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (0.4.5)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Using cached fonttools-4.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pillow>=6.2.0 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Using cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib==3.7.0->-r requirements.txt (line 4))\n",
            "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from pandas==1.5.3->-r requirements.txt (line 6)) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from scikit_learn==1.2.2->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from scikit_learn==1.2.2->-r requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 11)) (2024.7.24)\n",
            "Requirement already satisfied: requests in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 11)) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0->-r requirements.txt (line 11))\n",
            "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting hjson (from deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ninja (from deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting py-cpuinfo (from deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
            "Collecting pydantic (from deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Downloading pydantic-2.9.1-py3-none-any.whl.metadata (146 kB)\n",
            "Collecting pynvml (from deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r requirements.txt (line 1)) (12.6.68)\n",
            "Requirement already satisfied: six>=1.5 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.7.0->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from jinja2->torch==2.2.2->-r requirements.txt (line 1)) (2.1.5)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.23.3 (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 12))\n",
            "  Downloading pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages (from sympy->torch==2.2.2->-r requirements.txt (line 1)) (1.3.0)\n",
            "Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0mm\n",
            "\u001b[?25hDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "Downloading matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
            "\u001b[?25hDownloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
            "Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading pydantic-2.9.1-py3-none-any.whl (434 kB)\n",
            "Downloading pydantic_core-2.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400345 sha256=8d72c049b30fef8b1d7207e4cd71e54103d745425e8dd431e4fa5fca6fdc7e13\n",
            "  Stored in directory: /home/fakoor/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tokenizers, sentencepiece, py-cpuinfo, ninja, hjson, triton, tqdm, pyparsing, pynvml, pydantic-core, pillow, nvidia-nccl-cu12, nvidia-cudnn-cu12, numpy, kiwisolver, fonttools, einops, cycler, annotated-types, scipy, pydantic, pandas, contourpy, transformers, torch, scikit_learn, matplotlib, deepspeed, accelerate, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.5\n",
            "    Uninstalling tqdm-4.66.5:\n",
            "      Successfully uninstalled tqdm-4.66.5\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.1\n",
            "    Uninstalling numpy-2.1.1:\n",
            "      Successfully uninstalled numpy-2.1.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1\n",
            "    Uninstalling torch-2.4.1:\n",
            "      Successfully uninstalled torch-2.4.1\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.5.1\n",
            "    Uninstalling scikit-learn-1.5.1:\n",
            "      Successfully uninstalled scikit-learn-1.5.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.34.2\n",
            "    Uninstalling accelerate-0.34.2:\n",
            "      Successfully uninstalled accelerate-0.34.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.21.0 requires tqdm>=4.66.3, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.28.0 annotated-types-0.7.0 contourpy-1.3.0 cycler-0.12.1 deepspeed-0.14.0 einops-0.7.0 fonttools-4.53.1 hjson-3.1.0 kiwisolver-1.4.7 matplotlib-3.7.0 ninja-1.11.1.1 numpy-1.23.5 nvidia-cudnn-cu12-8.9.2.26 nvidia-nccl-cu12-2.19.3 pandas-1.5.3 peft-0.4.0 pillow-10.4.0 py-cpuinfo-9.0.0 pydantic-2.9.1 pydantic-core-2.23.3 pynvml-11.5.3 pyparsing-3.1.4 scikit_learn-1.2.2 scipy-1.12.0 sentencepiece-0.2.0 tokenizers-0.13.3 torch-2.2.2 tqdm-4.65.0 transformers-4.31.0 triton-2.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.2.43-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pandas>=1.3.0 in ./.venv/lib/python3.10/site-packages (from yfinance) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in ./.venv/lib/python3.10/site-packages (from yfinance) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.31 in ./.venv/lib/python3.10/site-packages (from yfinance) (2.32.3)\n",
            "Collecting multitasking>=0.0.7 (from yfinance)\n",
            "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting lxml>=4.9.1 (from yfinance)\n",
            "  Using cached lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in ./.venv/lib/python3.10/site-packages (from yfinance) (4.3.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in ./.venv/lib/python3.10/site-packages (from yfinance) (2024.1)\n",
            "Collecting frozendict>=2.3.4 (from yfinance)\n",
            "  Downloading frozendict-2.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Collecting peewee>=3.16.2 (from yfinance)\n",
            "  Downloading peewee-3.17.6.tar.gz (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting beautifulsoup4>=4.11.1 (from yfinance)\n",
            "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting html5lib>=1.1 (from yfinance)\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4>=4.11.1->yfinance)\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: six>=1.9 in ./.venv/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Collecting webencodings (from html5lib>=1.1->yfinance)\n",
            "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2024.8.30)\n",
            "Downloading yfinance-0.2.43-py2.py3-none-any.whl (84 kB)\n",
            "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "Downloading frozendict-2.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
            "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "Using cached lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
            "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: peewee\n",
            "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for peewee: filename=peewee-3.17.6-cp310-cp310-linux_x86_64.whl size=848960 sha256=c460fade5c2d4b83291daefca13df1230ad21bc6273af0d59214fedfafe2482a\n",
            "  Stored in directory: /home/fakoor/.cache/pip/wheels/4b/b9/b0/83d6e258e8f963f5ff111a2cd8c483ca59372a86e6a2535212\n",
            "Successfully built peewee\n",
            "Installing collected packages: webencodings, peewee, multitasking, soupsieve, lxml, html5lib, frozendict, beautifulsoup4, yfinance\n",
            "Successfully installed beautifulsoup4-4.12.3 frozendict-2.4.4 html5lib-1.1 lxml-5.3.0 multitasking-0.0.11 peewee-3.17.6 soupsieve-2.6 webencodings-0.5.1 yfinance-0.2.43\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l102kT0SCAb1",
        "outputId": "93ebfd75-c7e3-4fd5-cd90-cf667d694c8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Download stock data for a specific ticker, e.g., Apple (AAPL)\n",
        "data = yf.download('AAPL', start='2000-01-01', end='2024-01-01', interval='1d')\n",
        "data.reset_index(inplace=True)\n",
        "\n",
        "# Prepare the data by selecting necessary columns and renaming them\n",
        "data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "data.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
        "\n",
        "# Save the data to CSV if needed\n",
        "data.to_csv('data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yq6sopLnDGNW"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR=\"./checkpoints/AAPL\"\n",
        "\n",
        "# Ensure the checkpoint directory exists\n",
        "!mkdir -p $CHECKPOINT_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiYf2-EFE2ES",
        "outputId": "d2ec644c-77f7-4e4f-8d18-4a7533452c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/             \u001b[01;34mlayers\u001b[0m/           run_main.py\n",
            "data.csv                 LEGAL.md          run_pretrain.py\n",
            "\u001b[01;34mdata_provider\u001b[0m/           LICENSE           \u001b[01;34mscripts\u001b[0m/\n",
            "\u001b[01;34mdata_provider_pretrain\u001b[0m/  \u001b[01;34mmodels\u001b[0m/           time_llm_apple_5_epoch_train.ipynb\n",
            "\u001b[01;34mdataset\u001b[0m/                 README.md         \u001b[01;34mutils\u001b[0m/\n",
            "ds_config_zero2.json     requirements.txt\n",
            "\u001b[01;34mfigures\u001b[0m/                 run_m4.py\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GzSJr5AFirw",
        "outputId": "a6a0380e-2e6c-45fb-9e38-9ad09890bc6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_596906/3370358579.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.rename(columns={'timestamp': 'date'}, inplace=True)\n",
            "/tmp/ipykernel_596906/3370358579.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.rename(columns={'close': 'y'}, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = data\n",
        "df.rename(columns={'timestamp': 'date'}, inplace=True)\n",
        "df.rename(columns={'close': 'y'}, inplace=True)\n",
        "df = df[['date', 'y']]\n",
        "df.to_csv('data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "SGhypmqiFqrO",
        "outputId": "17ac53dc-9b11-4bf3-9bdc-709e08670dbc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>0.999442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-01-04</td>\n",
              "      <td>0.915179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-01-05</td>\n",
              "      <td>0.928571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-01-06</td>\n",
              "      <td>0.848214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-01-07</td>\n",
              "      <td>0.888393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6032</th>\n",
              "      <td>2023-12-22</td>\n",
              "      <td>193.600006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6033</th>\n",
              "      <td>2023-12-26</td>\n",
              "      <td>193.050003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6034</th>\n",
              "      <td>2023-12-27</td>\n",
              "      <td>193.149994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6035</th>\n",
              "      <td>2023-12-28</td>\n",
              "      <td>193.580002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6036</th>\n",
              "      <td>2023-12-29</td>\n",
              "      <td>192.529999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6037 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           date           y\n",
              "0    2000-01-03    0.999442\n",
              "1    2000-01-04    0.915179\n",
              "2    2000-01-05    0.928571\n",
              "3    2000-01-06    0.848214\n",
              "4    2000-01-07    0.888393\n",
              "...         ...         ...\n",
              "6032 2023-12-22  193.600006\n",
              "6033 2023-12-26  193.050003\n",
              "6034 2023-12-27  193.149994\n",
              "6035 2023-12-28  193.580002\n",
              "6036 2023-12-29  192.529999\n",
              "\n",
              "[6037 rows x 2 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMi-Hn-def4t",
        "outputId": "650f0fc4-7f0b-4a04-9ad2-b31dfda94501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection/emotions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd /home/fakoor/Desktop/chitsaz/emotion_detection/emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!export RDMAV_FORK_SAFE=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['RDMAV_FORK_SAFE'] = '1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection/emotions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fakoor/Desktop/chitsaz/emotion_detection/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd /home/fakoor/Desktop/chitsaz/emotion_detection/emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acReWe6pDDE_",
        "outputId": "73ed6643-89ec-48ef-f44f-1a1e85ec6444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------\n",
            "0\n",
            "['2000-01-03T00:00:00.000000000' '2000-01-04T00:00:00.000000000'\n",
            " '2000-01-05T00:00:00.000000000' ... '2020-05-21T00:00:00.000000000'\n",
            " '2020-05-22T00:00:00.000000000' '2020-05-26T00:00:00.000000000']\n",
            "-------------------------------\n",
            "1\n",
            "['2020-02-25T00:00:00.000000000' '2020-02-26T00:00:00.000000000'\n",
            " '2020-02-27T00:00:00.000000000' '2020-02-28T00:00:00.000000000'\n",
            " '2020-03-02T00:00:00.000000000' '2020-03-03T00:00:00.000000000'\n",
            " '2020-03-04T00:00:00.000000000' '2020-03-05T00:00:00.000000000'\n",
            " '2020-03-06T00:00:00.000000000' '2020-03-09T00:00:00.000000000'\n",
            " '2020-03-10T00:00:00.000000000' '2020-03-11T00:00:00.000000000'\n",
            " '2020-03-12T00:00:00.000000000' '2020-03-13T00:00:00.000000000'\n",
            " '2020-03-16T00:00:00.000000000' '2020-03-17T00:00:00.000000000'\n",
            " '2020-03-18T00:00:00.000000000' '2020-03-19T00:00:00.000000000'\n",
            " '2020-03-20T00:00:00.000000000' '2020-03-23T00:00:00.000000000'\n",
            " '2020-03-24T00:00:00.000000000' '2020-03-25T00:00:00.000000000'\n",
            " '2020-03-26T00:00:00.000000000' '2020-03-27T00:00:00.000000000'\n",
            " '2020-03-30T00:00:00.000000000' '2020-03-31T00:00:00.000000000'\n",
            " '2020-04-01T00:00:00.000000000' '2020-04-02T00:00:00.000000000'\n",
            " '2020-04-03T00:00:00.000000000' '2020-04-06T00:00:00.000000000'\n",
            " '2020-04-07T00:00:00.000000000' '2020-04-08T00:00:00.000000000'\n",
            " '2020-04-09T00:00:00.000000000' '2020-04-13T00:00:00.000000000'\n",
            " '2020-04-14T00:00:00.000000000' '2020-04-15T00:00:00.000000000'\n",
            " '2020-04-16T00:00:00.000000000' '2020-04-17T00:00:00.000000000'\n",
            " '2020-04-20T00:00:00.000000000' '2020-04-21T00:00:00.000000000'\n",
            " '2020-04-22T00:00:00.000000000' '2020-04-23T00:00:00.000000000'\n",
            " '2020-04-24T00:00:00.000000000' '2020-04-27T00:00:00.000000000'\n",
            " '2020-04-28T00:00:00.000000000' '2020-04-29T00:00:00.000000000'\n",
            " '2020-04-30T00:00:00.000000000' '2020-05-01T00:00:00.000000000'\n",
            " '2020-05-04T00:00:00.000000000' '2020-05-05T00:00:00.000000000'\n",
            " '2020-05-06T00:00:00.000000000' '2020-05-07T00:00:00.000000000'\n",
            " '2020-05-08T00:00:00.000000000' '2020-05-11T00:00:00.000000000'\n",
            " '2020-05-12T00:00:00.000000000' '2020-05-13T00:00:00.000000000'\n",
            " '2020-05-14T00:00:00.000000000' '2020-05-15T00:00:00.000000000'\n",
            " '2020-05-18T00:00:00.000000000' '2020-05-19T00:00:00.000000000'\n",
            " '2020-05-20T00:00:00.000000000' '2020-05-21T00:00:00.000000000'\n",
            " '2020-05-22T00:00:00.000000000' '2020-05-26T00:00:00.000000000'\n",
            " '2020-05-27T00:00:00.000000000' '2020-05-28T00:00:00.000000000'\n",
            " '2020-05-29T00:00:00.000000000' '2020-06-01T00:00:00.000000000'\n",
            " '2020-06-02T00:00:00.000000000' '2020-06-03T00:00:00.000000000'\n",
            " '2020-06-04T00:00:00.000000000' '2020-06-05T00:00:00.000000000'\n",
            " '2020-06-08T00:00:00.000000000' '2020-06-09T00:00:00.000000000'\n",
            " '2020-06-10T00:00:00.000000000' '2020-06-11T00:00:00.000000000'\n",
            " '2020-06-12T00:00:00.000000000' '2020-06-15T00:00:00.000000000'\n",
            " '2020-06-16T00:00:00.000000000' '2020-06-17T00:00:00.000000000'\n",
            " '2020-06-18T00:00:00.000000000' '2020-06-19T00:00:00.000000000'\n",
            " '2020-06-22T00:00:00.000000000' '2020-06-23T00:00:00.000000000'\n",
            " '2020-06-24T00:00:00.000000000' '2020-06-25T00:00:00.000000000'\n",
            " '2020-06-26T00:00:00.000000000' '2020-06-29T00:00:00.000000000'\n",
            " '2020-06-30T00:00:00.000000000' '2020-07-01T00:00:00.000000000'\n",
            " '2020-07-02T00:00:00.000000000' '2020-07-06T00:00:00.000000000'\n",
            " '2020-07-07T00:00:00.000000000' '2020-07-08T00:00:00.000000000'\n",
            " '2020-07-09T00:00:00.000000000' '2020-07-10T00:00:00.000000000'\n",
            " '2020-07-13T00:00:00.000000000' '2020-07-14T00:00:00.000000000'\n",
            " '2020-07-15T00:00:00.000000000' '2020-07-16T00:00:00.000000000'\n",
            " '2020-07-17T00:00:00.000000000' '2020-07-20T00:00:00.000000000'\n",
            " '2020-07-21T00:00:00.000000000' '2020-07-22T00:00:00.000000000'\n",
            " '2020-07-23T00:00:00.000000000' '2020-07-24T00:00:00.000000000'\n",
            " '2020-07-27T00:00:00.000000000' '2020-07-28T00:00:00.000000000'\n",
            " '2020-07-29T00:00:00.000000000' '2020-07-30T00:00:00.000000000'\n",
            " '2020-07-31T00:00:00.000000000' '2020-08-03T00:00:00.000000000'\n",
            " '2020-08-04T00:00:00.000000000' '2020-08-05T00:00:00.000000000'\n",
            " '2020-08-06T00:00:00.000000000' '2020-08-07T00:00:00.000000000'\n",
            " '2020-08-10T00:00:00.000000000' '2020-08-11T00:00:00.000000000'\n",
            " '2020-08-12T00:00:00.000000000' '2020-08-13T00:00:00.000000000'\n",
            " '2020-08-14T00:00:00.000000000' '2020-08-17T00:00:00.000000000'\n",
            " '2020-08-18T00:00:00.000000000' '2020-08-19T00:00:00.000000000'\n",
            " '2020-08-20T00:00:00.000000000' '2020-08-21T00:00:00.000000000'\n",
            " '2020-08-24T00:00:00.000000000' '2020-08-25T00:00:00.000000000'\n",
            " '2020-08-26T00:00:00.000000000' '2020-08-27T00:00:00.000000000'\n",
            " '2020-08-28T00:00:00.000000000' '2020-08-31T00:00:00.000000000'\n",
            " '2020-09-01T00:00:00.000000000' '2020-09-02T00:00:00.000000000'\n",
            " '2020-09-03T00:00:00.000000000' '2020-09-04T00:00:00.000000000'\n",
            " '2020-09-08T00:00:00.000000000' '2020-09-09T00:00:00.000000000'\n",
            " '2020-09-10T00:00:00.000000000' '2020-09-11T00:00:00.000000000'\n",
            " '2020-09-14T00:00:00.000000000' '2020-09-15T00:00:00.000000000'\n",
            " '2020-09-16T00:00:00.000000000' '2020-09-17T00:00:00.000000000'\n",
            " '2020-09-18T00:00:00.000000000' '2020-09-21T00:00:00.000000000'\n",
            " '2020-09-22T00:00:00.000000000' '2020-09-23T00:00:00.000000000'\n",
            " '2020-09-24T00:00:00.000000000' '2020-09-25T00:00:00.000000000'\n",
            " '2020-09-28T00:00:00.000000000' '2020-09-29T00:00:00.000000000'\n",
            " '2020-09-30T00:00:00.000000000' '2020-10-01T00:00:00.000000000'\n",
            " '2020-10-02T00:00:00.000000000' '2020-10-05T00:00:00.000000000'\n",
            " '2020-10-06T00:00:00.000000000' '2020-10-07T00:00:00.000000000'\n",
            " '2020-10-08T00:00:00.000000000' '2020-10-09T00:00:00.000000000'\n",
            " '2020-10-12T00:00:00.000000000' '2020-10-13T00:00:00.000000000'\n",
            " '2020-10-14T00:00:00.000000000' '2020-10-15T00:00:00.000000000'\n",
            " '2020-10-16T00:00:00.000000000' '2020-10-19T00:00:00.000000000'\n",
            " '2020-10-20T00:00:00.000000000' '2020-10-21T00:00:00.000000000'\n",
            " '2020-10-22T00:00:00.000000000' '2020-10-23T00:00:00.000000000'\n",
            " '2020-10-26T00:00:00.000000000' '2020-10-27T00:00:00.000000000'\n",
            " '2020-10-28T00:00:00.000000000' '2020-10-29T00:00:00.000000000'\n",
            " '2020-10-30T00:00:00.000000000' '2020-11-02T00:00:00.000000000'\n",
            " '2020-11-03T00:00:00.000000000' '2020-11-04T00:00:00.000000000'\n",
            " '2020-11-05T00:00:00.000000000' '2020-11-06T00:00:00.000000000'\n",
            " '2020-11-09T00:00:00.000000000' '2020-11-10T00:00:00.000000000'\n",
            " '2020-11-11T00:00:00.000000000' '2020-11-12T00:00:00.000000000'\n",
            " '2020-11-13T00:00:00.000000000' '2020-11-16T00:00:00.000000000'\n",
            " '2020-11-17T00:00:00.000000000' '2020-11-18T00:00:00.000000000'\n",
            " '2020-11-19T00:00:00.000000000' '2020-11-20T00:00:00.000000000'\n",
            " '2020-11-23T00:00:00.000000000' '2020-11-24T00:00:00.000000000'\n",
            " '2020-11-25T00:00:00.000000000' '2020-11-27T00:00:00.000000000'\n",
            " '2020-11-30T00:00:00.000000000' '2020-12-01T00:00:00.000000000'\n",
            " '2020-12-02T00:00:00.000000000' '2020-12-03T00:00:00.000000000'\n",
            " '2020-12-04T00:00:00.000000000' '2020-12-07T00:00:00.000000000'\n",
            " '2020-12-08T00:00:00.000000000' '2020-12-09T00:00:00.000000000'\n",
            " '2020-12-10T00:00:00.000000000' '2020-12-11T00:00:00.000000000'\n",
            " '2020-12-14T00:00:00.000000000' '2020-12-15T00:00:00.000000000'\n",
            " '2020-12-16T00:00:00.000000000' '2020-12-17T00:00:00.000000000'\n",
            " '2020-12-18T00:00:00.000000000' '2020-12-21T00:00:00.000000000'\n",
            " '2020-12-22T00:00:00.000000000' '2020-12-23T00:00:00.000000000'\n",
            " '2020-12-24T00:00:00.000000000' '2020-12-28T00:00:00.000000000'\n",
            " '2020-12-29T00:00:00.000000000' '2020-12-30T00:00:00.000000000'\n",
            " '2020-12-31T00:00:00.000000000' '2021-01-04T00:00:00.000000000'\n",
            " '2021-01-05T00:00:00.000000000' '2021-01-06T00:00:00.000000000'\n",
            " '2021-01-07T00:00:00.000000000' '2021-01-08T00:00:00.000000000'\n",
            " '2021-01-11T00:00:00.000000000' '2021-01-12T00:00:00.000000000'\n",
            " '2021-01-13T00:00:00.000000000' '2021-01-14T00:00:00.000000000'\n",
            " '2021-01-15T00:00:00.000000000' '2021-01-19T00:00:00.000000000'\n",
            " '2021-01-20T00:00:00.000000000' '2021-01-21T00:00:00.000000000'\n",
            " '2021-01-22T00:00:00.000000000' '2021-01-25T00:00:00.000000000'\n",
            " '2021-01-26T00:00:00.000000000' '2021-01-27T00:00:00.000000000'\n",
            " '2021-01-28T00:00:00.000000000' '2021-01-29T00:00:00.000000000'\n",
            " '2021-02-01T00:00:00.000000000' '2021-02-02T00:00:00.000000000'\n",
            " '2021-02-03T00:00:00.000000000' '2021-02-04T00:00:00.000000000'\n",
            " '2021-02-05T00:00:00.000000000' '2021-02-08T00:00:00.000000000'\n",
            " '2021-02-09T00:00:00.000000000' '2021-02-10T00:00:00.000000000'\n",
            " '2021-02-11T00:00:00.000000000' '2021-02-12T00:00:00.000000000'\n",
            " '2021-02-16T00:00:00.000000000' '2021-02-17T00:00:00.000000000'\n",
            " '2021-02-18T00:00:00.000000000' '2021-02-19T00:00:00.000000000'\n",
            " '2021-02-22T00:00:00.000000000' '2021-02-23T00:00:00.000000000'\n",
            " '2021-02-24T00:00:00.000000000' '2021-02-25T00:00:00.000000000'\n",
            " '2021-02-26T00:00:00.000000000' '2021-03-01T00:00:00.000000000'\n",
            " '2021-03-02T00:00:00.000000000' '2021-03-03T00:00:00.000000000'\n",
            " '2021-03-04T00:00:00.000000000' '2021-03-05T00:00:00.000000000'\n",
            " '2021-03-08T00:00:00.000000000' '2021-03-09T00:00:00.000000000'\n",
            " '2021-03-10T00:00:00.000000000' '2021-03-11T00:00:00.000000000'\n",
            " '2021-03-12T00:00:00.000000000' '2021-03-15T00:00:00.000000000'\n",
            " '2021-03-16T00:00:00.000000000' '2021-03-17T00:00:00.000000000'\n",
            " '2021-03-18T00:00:00.000000000' '2021-03-19T00:00:00.000000000'\n",
            " '2021-03-22T00:00:00.000000000' '2021-03-23T00:00:00.000000000'\n",
            " '2021-03-24T00:00:00.000000000' '2021-03-25T00:00:00.000000000'\n",
            " '2021-03-26T00:00:00.000000000' '2021-03-29T00:00:00.000000000'\n",
            " '2021-03-30T00:00:00.000000000' '2021-03-31T00:00:00.000000000'\n",
            " '2021-04-01T00:00:00.000000000' '2021-04-05T00:00:00.000000000'\n",
            " '2021-04-06T00:00:00.000000000' '2021-04-07T00:00:00.000000000'\n",
            " '2021-04-08T00:00:00.000000000' '2021-04-09T00:00:00.000000000'\n",
            " '2021-04-12T00:00:00.000000000' '2021-04-13T00:00:00.000000000'\n",
            " '2021-04-14T00:00:00.000000000' '2021-04-15T00:00:00.000000000'\n",
            " '2021-04-16T00:00:00.000000000' '2021-04-19T00:00:00.000000000'\n",
            " '2021-04-20T00:00:00.000000000' '2021-04-21T00:00:00.000000000'\n",
            " '2021-04-22T00:00:00.000000000' '2021-04-23T00:00:00.000000000'\n",
            " '2021-04-26T00:00:00.000000000' '2021-04-27T00:00:00.000000000'\n",
            " '2021-04-28T00:00:00.000000000' '2021-04-29T00:00:00.000000000'\n",
            " '2021-04-30T00:00:00.000000000' '2021-05-03T00:00:00.000000000'\n",
            " '2021-05-04T00:00:00.000000000' '2021-05-05T00:00:00.000000000'\n",
            " '2021-05-06T00:00:00.000000000' '2021-05-07T00:00:00.000000000'\n",
            " '2021-05-10T00:00:00.000000000' '2021-05-11T00:00:00.000000000'\n",
            " '2021-05-12T00:00:00.000000000' '2021-05-13T00:00:00.000000000'\n",
            " '2021-05-14T00:00:00.000000000' '2021-05-17T00:00:00.000000000'\n",
            " '2021-05-18T00:00:00.000000000' '2021-05-19T00:00:00.000000000'\n",
            " '2021-05-20T00:00:00.000000000' '2021-05-21T00:00:00.000000000'\n",
            " '2021-05-24T00:00:00.000000000' '2021-05-25T00:00:00.000000000'\n",
            " '2021-05-26T00:00:00.000000000' '2021-05-27T00:00:00.000000000'\n",
            " '2021-05-28T00:00:00.000000000' '2021-06-01T00:00:00.000000000'\n",
            " '2021-06-02T00:00:00.000000000' '2021-06-03T00:00:00.000000000'\n",
            " '2021-06-04T00:00:00.000000000' '2021-06-07T00:00:00.000000000'\n",
            " '2021-06-08T00:00:00.000000000' '2021-06-09T00:00:00.000000000'\n",
            " '2021-06-10T00:00:00.000000000' '2021-06-11T00:00:00.000000000'\n",
            " '2021-06-14T00:00:00.000000000' '2021-06-15T00:00:00.000000000'\n",
            " '2021-06-16T00:00:00.000000000' '2021-06-17T00:00:00.000000000'\n",
            " '2021-06-18T00:00:00.000000000' '2021-06-21T00:00:00.000000000'\n",
            " '2021-06-22T00:00:00.000000000' '2021-06-23T00:00:00.000000000'\n",
            " '2021-06-24T00:00:00.000000000' '2021-06-25T00:00:00.000000000'\n",
            " '2021-06-28T00:00:00.000000000' '2021-06-29T00:00:00.000000000'\n",
            " '2021-06-30T00:00:00.000000000' '2021-07-01T00:00:00.000000000'\n",
            " '2021-07-02T00:00:00.000000000' '2021-07-06T00:00:00.000000000'\n",
            " '2021-07-07T00:00:00.000000000' '2021-07-08T00:00:00.000000000'\n",
            " '2021-07-09T00:00:00.000000000' '2021-07-12T00:00:00.000000000'\n",
            " '2021-07-13T00:00:00.000000000' '2021-07-14T00:00:00.000000000'\n",
            " '2021-07-15T00:00:00.000000000' '2021-07-16T00:00:00.000000000'\n",
            " '2021-07-19T00:00:00.000000000' '2021-07-20T00:00:00.000000000'\n",
            " '2021-07-21T00:00:00.000000000' '2021-07-22T00:00:00.000000000'\n",
            " '2021-07-23T00:00:00.000000000' '2021-07-26T00:00:00.000000000'\n",
            " '2021-07-27T00:00:00.000000000' '2021-07-28T00:00:00.000000000'\n",
            " '2021-07-29T00:00:00.000000000' '2021-07-30T00:00:00.000000000'\n",
            " '2021-08-02T00:00:00.000000000' '2021-08-03T00:00:00.000000000'\n",
            " '2021-08-04T00:00:00.000000000' '2021-08-05T00:00:00.000000000'\n",
            " '2021-08-06T00:00:00.000000000' '2021-08-09T00:00:00.000000000'\n",
            " '2021-08-10T00:00:00.000000000' '2021-08-11T00:00:00.000000000'\n",
            " '2021-08-12T00:00:00.000000000' '2021-08-13T00:00:00.000000000'\n",
            " '2021-08-16T00:00:00.000000000' '2021-08-17T00:00:00.000000000'\n",
            " '2021-08-18T00:00:00.000000000' '2021-08-19T00:00:00.000000000'\n",
            " '2021-08-20T00:00:00.000000000' '2021-08-23T00:00:00.000000000'\n",
            " '2021-08-24T00:00:00.000000000' '2021-08-25T00:00:00.000000000'\n",
            " '2021-08-26T00:00:00.000000000' '2021-08-27T00:00:00.000000000'\n",
            " '2021-08-30T00:00:00.000000000' '2021-08-31T00:00:00.000000000'\n",
            " '2021-09-01T00:00:00.000000000' '2021-09-02T00:00:00.000000000'\n",
            " '2021-09-03T00:00:00.000000000' '2021-09-07T00:00:00.000000000'\n",
            " '2021-09-08T00:00:00.000000000' '2021-09-09T00:00:00.000000000'\n",
            " '2021-09-10T00:00:00.000000000' '2021-09-13T00:00:00.000000000'\n",
            " '2021-09-14T00:00:00.000000000' '2021-09-15T00:00:00.000000000'\n",
            " '2021-09-16T00:00:00.000000000' '2021-09-17T00:00:00.000000000'\n",
            " '2021-09-20T00:00:00.000000000' '2021-09-21T00:00:00.000000000'\n",
            " '2021-09-22T00:00:00.000000000' '2021-09-23T00:00:00.000000000'\n",
            " '2021-09-24T00:00:00.000000000' '2021-09-27T00:00:00.000000000'\n",
            " '2021-09-28T00:00:00.000000000' '2021-09-29T00:00:00.000000000'\n",
            " '2021-09-30T00:00:00.000000000' '2021-10-01T00:00:00.000000000'\n",
            " '2021-10-04T00:00:00.000000000' '2021-10-05T00:00:00.000000000'\n",
            " '2021-10-06T00:00:00.000000000' '2021-10-07T00:00:00.000000000'\n",
            " '2021-10-08T00:00:00.000000000' '2021-10-11T00:00:00.000000000'\n",
            " '2021-10-12T00:00:00.000000000' '2021-10-13T00:00:00.000000000'\n",
            " '2021-10-14T00:00:00.000000000' '2021-10-15T00:00:00.000000000'\n",
            " '2021-10-18T00:00:00.000000000' '2021-10-19T00:00:00.000000000'\n",
            " '2021-10-20T00:00:00.000000000' '2021-10-21T00:00:00.000000000'\n",
            " '2021-10-22T00:00:00.000000000' '2021-10-25T00:00:00.000000000'\n",
            " '2021-10-26T00:00:00.000000000' '2021-10-27T00:00:00.000000000'\n",
            " '2021-10-28T00:00:00.000000000' '2021-10-29T00:00:00.000000000'\n",
            " '2021-11-01T00:00:00.000000000' '2021-11-02T00:00:00.000000000'\n",
            " '2021-11-03T00:00:00.000000000' '2021-11-04T00:00:00.000000000'\n",
            " '2021-11-05T00:00:00.000000000' '2021-11-08T00:00:00.000000000'\n",
            " '2021-11-09T00:00:00.000000000' '2021-11-10T00:00:00.000000000'\n",
            " '2021-11-11T00:00:00.000000000' '2021-11-12T00:00:00.000000000'\n",
            " '2021-11-15T00:00:00.000000000' '2021-11-16T00:00:00.000000000'\n",
            " '2021-11-17T00:00:00.000000000' '2021-11-18T00:00:00.000000000'\n",
            " '2021-11-19T00:00:00.000000000' '2021-11-22T00:00:00.000000000'\n",
            " '2021-11-23T00:00:00.000000000' '2021-11-24T00:00:00.000000000'\n",
            " '2021-11-26T00:00:00.000000000' '2021-11-29T00:00:00.000000000'\n",
            " '2021-11-30T00:00:00.000000000' '2021-12-01T00:00:00.000000000'\n",
            " '2021-12-02T00:00:00.000000000' '2021-12-03T00:00:00.000000000'\n",
            " '2021-12-06T00:00:00.000000000' '2021-12-07T00:00:00.000000000'\n",
            " '2021-12-08T00:00:00.000000000' '2021-12-09T00:00:00.000000000'\n",
            " '2021-12-10T00:00:00.000000000' '2021-12-13T00:00:00.000000000'\n",
            " '2021-12-14T00:00:00.000000000' '2021-12-15T00:00:00.000000000'\n",
            " '2021-12-16T00:00:00.000000000' '2021-12-17T00:00:00.000000000'\n",
            " '2021-12-20T00:00:00.000000000' '2021-12-21T00:00:00.000000000'\n",
            " '2021-12-22T00:00:00.000000000' '2021-12-23T00:00:00.000000000'\n",
            " '2021-12-27T00:00:00.000000000' '2021-12-28T00:00:00.000000000'\n",
            " '2021-12-29T00:00:00.000000000' '2021-12-30T00:00:00.000000000'\n",
            " '2021-12-31T00:00:00.000000000' '2022-01-03T00:00:00.000000000'\n",
            " '2022-01-04T00:00:00.000000000' '2022-01-05T00:00:00.000000000'\n",
            " '2022-01-06T00:00:00.000000000' '2022-01-07T00:00:00.000000000'\n",
            " '2022-01-10T00:00:00.000000000' '2022-01-11T00:00:00.000000000'\n",
            " '2022-01-12T00:00:00.000000000' '2022-01-13T00:00:00.000000000'\n",
            " '2022-01-14T00:00:00.000000000' '2022-01-18T00:00:00.000000000'\n",
            " '2022-01-19T00:00:00.000000000' '2022-01-20T00:00:00.000000000'\n",
            " '2022-01-21T00:00:00.000000000' '2022-01-24T00:00:00.000000000'\n",
            " '2022-01-25T00:00:00.000000000' '2022-01-26T00:00:00.000000000'\n",
            " '2022-01-27T00:00:00.000000000' '2022-01-28T00:00:00.000000000'\n",
            " '2022-01-31T00:00:00.000000000' '2022-02-01T00:00:00.000000000'\n",
            " '2022-02-02T00:00:00.000000000' '2022-02-03T00:00:00.000000000'\n",
            " '2022-02-04T00:00:00.000000000' '2022-02-07T00:00:00.000000000'\n",
            " '2022-02-08T00:00:00.000000000' '2022-02-09T00:00:00.000000000'\n",
            " '2022-02-10T00:00:00.000000000' '2022-02-11T00:00:00.000000000'\n",
            " '2022-02-14T00:00:00.000000000' '2022-02-15T00:00:00.000000000'\n",
            " '2022-02-16T00:00:00.000000000' '2022-02-17T00:00:00.000000000'\n",
            " '2022-02-18T00:00:00.000000000' '2022-02-22T00:00:00.000000000'\n",
            " '2022-02-23T00:00:00.000000000' '2022-02-24T00:00:00.000000000'\n",
            " '2022-02-25T00:00:00.000000000' '2022-02-28T00:00:00.000000000'\n",
            " '2022-03-01T00:00:00.000000000' '2022-03-02T00:00:00.000000000'\n",
            " '2022-03-03T00:00:00.000000000' '2022-03-04T00:00:00.000000000'\n",
            " '2022-03-07T00:00:00.000000000' '2022-03-08T00:00:00.000000000'\n",
            " '2022-03-09T00:00:00.000000000' '2022-03-10T00:00:00.000000000'\n",
            " '2022-03-11T00:00:00.000000000' '2022-03-14T00:00:00.000000000'\n",
            " '2022-03-15T00:00:00.000000000' '2022-03-16T00:00:00.000000000'\n",
            " '2022-03-17T00:00:00.000000000' '2022-03-18T00:00:00.000000000'\n",
            " '2022-03-21T00:00:00.000000000' '2022-03-22T00:00:00.000000000'\n",
            " '2022-03-23T00:00:00.000000000' '2022-03-24T00:00:00.000000000'\n",
            " '2022-03-25T00:00:00.000000000' '2022-03-28T00:00:00.000000000'\n",
            " '2022-03-29T00:00:00.000000000' '2022-03-30T00:00:00.000000000'\n",
            " '2022-03-31T00:00:00.000000000' '2022-04-01T00:00:00.000000000'\n",
            " '2022-04-04T00:00:00.000000000' '2022-04-05T00:00:00.000000000'\n",
            " '2022-04-06T00:00:00.000000000' '2022-04-07T00:00:00.000000000'\n",
            " '2022-04-08T00:00:00.000000000' '2022-04-11T00:00:00.000000000'\n",
            " '2022-04-12T00:00:00.000000000' '2022-04-13T00:00:00.000000000'\n",
            " '2022-04-14T00:00:00.000000000' '2022-04-18T00:00:00.000000000'\n",
            " '2022-04-19T00:00:00.000000000' '2022-04-20T00:00:00.000000000'\n",
            " '2022-04-21T00:00:00.000000000' '2022-04-22T00:00:00.000000000'\n",
            " '2022-04-25T00:00:00.000000000' '2022-04-26T00:00:00.000000000'\n",
            " '2022-04-27T00:00:00.000000000' '2022-04-28T00:00:00.000000000'\n",
            " '2022-04-29T00:00:00.000000000' '2022-05-02T00:00:00.000000000'\n",
            " '2022-05-03T00:00:00.000000000' '2022-05-04T00:00:00.000000000'\n",
            " '2022-05-05T00:00:00.000000000' '2022-05-06T00:00:00.000000000'\n",
            " '2022-05-09T00:00:00.000000000' '2022-05-10T00:00:00.000000000'\n",
            " '2022-05-11T00:00:00.000000000' '2022-05-12T00:00:00.000000000'\n",
            " '2022-05-13T00:00:00.000000000' '2022-05-16T00:00:00.000000000'\n",
            " '2022-05-17T00:00:00.000000000' '2022-05-18T00:00:00.000000000'\n",
            " '2022-05-19T00:00:00.000000000' '2022-05-20T00:00:00.000000000'\n",
            " '2022-05-23T00:00:00.000000000' '2022-05-24T00:00:00.000000000'\n",
            " '2022-05-25T00:00:00.000000000' '2022-05-26T00:00:00.000000000'\n",
            " '2022-05-27T00:00:00.000000000' '2022-05-31T00:00:00.000000000'\n",
            " '2022-06-01T00:00:00.000000000' '2022-06-02T00:00:00.000000000'\n",
            " '2022-06-03T00:00:00.000000000' '2022-06-06T00:00:00.000000000'\n",
            " '2022-06-07T00:00:00.000000000' '2022-06-08T00:00:00.000000000'\n",
            " '2022-06-09T00:00:00.000000000' '2022-06-10T00:00:00.000000000'\n",
            " '2022-06-13T00:00:00.000000000' '2022-06-14T00:00:00.000000000'\n",
            " '2022-06-15T00:00:00.000000000' '2022-06-16T00:00:00.000000000'\n",
            " '2022-06-17T00:00:00.000000000' '2022-06-21T00:00:00.000000000'\n",
            " '2022-06-22T00:00:00.000000000' '2022-06-23T00:00:00.000000000'\n",
            " '2022-06-24T00:00:00.000000000' '2022-06-27T00:00:00.000000000'\n",
            " '2022-06-28T00:00:00.000000000' '2022-06-29T00:00:00.000000000'\n",
            " '2022-06-30T00:00:00.000000000' '2022-07-01T00:00:00.000000000'\n",
            " '2022-07-05T00:00:00.000000000' '2022-07-06T00:00:00.000000000'\n",
            " '2022-07-07T00:00:00.000000000' '2022-07-08T00:00:00.000000000'\n",
            " '2022-07-11T00:00:00.000000000' '2022-07-12T00:00:00.000000000'\n",
            " '2022-07-13T00:00:00.000000000' '2022-07-14T00:00:00.000000000'\n",
            " '2022-07-15T00:00:00.000000000' '2022-07-18T00:00:00.000000000'\n",
            " '2022-07-19T00:00:00.000000000' '2022-07-20T00:00:00.000000000'\n",
            " '2022-07-21T00:00:00.000000000' '2022-07-22T00:00:00.000000000'\n",
            " '2022-07-25T00:00:00.000000000' '2022-07-26T00:00:00.000000000'\n",
            " '2022-07-27T00:00:00.000000000' '2022-07-28T00:00:00.000000000'\n",
            " '2022-07-29T00:00:00.000000000' '2022-08-01T00:00:00.000000000'\n",
            " '2022-08-02T00:00:00.000000000' '2022-08-03T00:00:00.000000000'\n",
            " '2022-08-04T00:00:00.000000000' '2022-08-05T00:00:00.000000000'\n",
            " '2022-08-08T00:00:00.000000000' '2022-08-09T00:00:00.000000000'\n",
            " '2022-08-10T00:00:00.000000000' '2022-08-11T00:00:00.000000000'\n",
            " '2022-08-12T00:00:00.000000000' '2022-08-15T00:00:00.000000000'\n",
            " '2022-08-16T00:00:00.000000000' '2022-08-17T00:00:00.000000000'\n",
            " '2022-08-18T00:00:00.000000000' '2022-08-19T00:00:00.000000000'\n",
            " '2022-08-22T00:00:00.000000000' '2022-08-23T00:00:00.000000000'\n",
            " '2022-08-24T00:00:00.000000000' '2022-08-25T00:00:00.000000000'\n",
            " '2022-08-26T00:00:00.000000000' '2022-08-29T00:00:00.000000000'\n",
            " '2022-08-30T00:00:00.000000000' '2022-08-31T00:00:00.000000000'\n",
            " '2022-09-01T00:00:00.000000000' '2022-09-02T00:00:00.000000000'\n",
            " '2022-09-06T00:00:00.000000000' '2022-09-07T00:00:00.000000000'\n",
            " '2022-09-08T00:00:00.000000000' '2022-09-09T00:00:00.000000000'\n",
            " '2022-09-12T00:00:00.000000000' '2022-09-13T00:00:00.000000000'\n",
            " '2022-09-14T00:00:00.000000000' '2022-09-15T00:00:00.000000000'\n",
            " '2022-09-16T00:00:00.000000000' '2022-09-19T00:00:00.000000000'\n",
            " '2022-09-20T00:00:00.000000000' '2022-09-21T00:00:00.000000000'\n",
            " '2022-09-22T00:00:00.000000000' '2022-09-23T00:00:00.000000000'\n",
            " '2022-09-26T00:00:00.000000000' '2022-09-27T00:00:00.000000000'\n",
            " '2022-09-28T00:00:00.000000000' '2022-09-29T00:00:00.000000000'\n",
            " '2022-09-30T00:00:00.000000000' '2022-10-03T00:00:00.000000000'\n",
            " '2022-10-04T00:00:00.000000000' '2022-10-05T00:00:00.000000000'\n",
            " '2022-10-06T00:00:00.000000000' '2022-10-07T00:00:00.000000000'\n",
            " '2022-10-10T00:00:00.000000000' '2022-10-11T00:00:00.000000000'\n",
            " '2022-10-12T00:00:00.000000000' '2022-10-13T00:00:00.000000000'\n",
            " '2022-10-14T00:00:00.000000000' '2022-10-17T00:00:00.000000000'\n",
            " '2022-10-18T00:00:00.000000000' '2022-10-19T00:00:00.000000000'\n",
            " '2022-10-20T00:00:00.000000000' '2022-10-21T00:00:00.000000000'\n",
            " '2022-10-24T00:00:00.000000000' '2022-10-25T00:00:00.000000000'\n",
            " '2022-10-26T00:00:00.000000000' '2022-10-27T00:00:00.000000000'\n",
            " '2022-10-28T00:00:00.000000000' '2022-10-31T00:00:00.000000000'\n",
            " '2022-11-01T00:00:00.000000000' '2022-11-02T00:00:00.000000000'\n",
            " '2022-11-03T00:00:00.000000000' '2022-11-04T00:00:00.000000000'\n",
            " '2022-11-07T00:00:00.000000000' '2022-11-08T00:00:00.000000000'\n",
            " '2022-11-09T00:00:00.000000000' '2022-11-10T00:00:00.000000000'\n",
            " '2022-11-11T00:00:00.000000000' '2022-11-14T00:00:00.000000000'\n",
            " '2022-11-15T00:00:00.000000000' '2022-11-16T00:00:00.000000000'\n",
            " '2022-11-17T00:00:00.000000000' '2022-11-18T00:00:00.000000000'\n",
            " '2022-11-21T00:00:00.000000000' '2022-11-22T00:00:00.000000000'\n",
            " '2022-11-23T00:00:00.000000000' '2022-11-25T00:00:00.000000000'\n",
            " '2022-11-28T00:00:00.000000000' '2022-11-29T00:00:00.000000000'\n",
            " '2022-11-30T00:00:00.000000000' '2022-12-01T00:00:00.000000000'\n",
            " '2022-12-02T00:00:00.000000000' '2022-12-05T00:00:00.000000000'\n",
            " '2022-12-06T00:00:00.000000000' '2022-12-07T00:00:00.000000000'\n",
            " '2022-12-08T00:00:00.000000000' '2022-12-09T00:00:00.000000000'\n",
            " '2022-12-12T00:00:00.000000000' '2022-12-13T00:00:00.000000000'\n",
            " '2022-12-14T00:00:00.000000000' '2022-12-15T00:00:00.000000000'\n",
            " '2022-12-16T00:00:00.000000000' '2022-12-19T00:00:00.000000000'\n",
            " '2022-12-20T00:00:00.000000000' '2022-12-21T00:00:00.000000000'\n",
            " '2022-12-22T00:00:00.000000000' '2022-12-23T00:00:00.000000000'\n",
            " '2022-12-27T00:00:00.000000000' '2022-12-28T00:00:00.000000000'\n",
            " '2022-12-29T00:00:00.000000000' '2022-12-30T00:00:00.000000000'\n",
            " '2023-01-03T00:00:00.000000000' '2023-01-04T00:00:00.000000000'\n",
            " '2023-01-05T00:00:00.000000000' '2023-01-06T00:00:00.000000000'\n",
            " '2023-01-09T00:00:00.000000000' '2023-01-10T00:00:00.000000000'\n",
            " '2023-01-11T00:00:00.000000000' '2023-01-12T00:00:00.000000000'\n",
            " '2023-01-13T00:00:00.000000000' '2023-01-17T00:00:00.000000000'\n",
            " '2023-01-18T00:00:00.000000000' '2023-01-19T00:00:00.000000000'\n",
            " '2023-01-20T00:00:00.000000000' '2023-01-23T00:00:00.000000000'\n",
            " '2023-01-24T00:00:00.000000000' '2023-01-25T00:00:00.000000000'\n",
            " '2023-01-26T00:00:00.000000000' '2023-01-27T00:00:00.000000000'\n",
            " '2023-01-30T00:00:00.000000000' '2023-01-31T00:00:00.000000000'\n",
            " '2023-02-01T00:00:00.000000000' '2023-02-02T00:00:00.000000000'\n",
            " '2023-02-03T00:00:00.000000000' '2023-02-06T00:00:00.000000000'\n",
            " '2023-02-07T00:00:00.000000000' '2023-02-08T00:00:00.000000000'\n",
            " '2023-02-09T00:00:00.000000000' '2023-02-10T00:00:00.000000000'\n",
            " '2023-02-13T00:00:00.000000000' '2023-02-14T00:00:00.000000000'\n",
            " '2023-02-15T00:00:00.000000000' '2023-02-16T00:00:00.000000000'\n",
            " '2023-02-17T00:00:00.000000000' '2023-02-21T00:00:00.000000000'\n",
            " '2023-02-22T00:00:00.000000000' '2023-02-23T00:00:00.000000000'\n",
            " '2023-02-24T00:00:00.000000000' '2023-02-27T00:00:00.000000000'\n",
            " '2023-02-28T00:00:00.000000000' '2023-03-01T00:00:00.000000000'\n",
            " '2023-03-02T00:00:00.000000000' '2023-03-03T00:00:00.000000000'\n",
            " '2023-03-06T00:00:00.000000000' '2023-03-07T00:00:00.000000000'\n",
            " '2023-03-08T00:00:00.000000000' '2023-03-09T00:00:00.000000000'\n",
            " '2023-03-10T00:00:00.000000000' '2023-03-13T00:00:00.000000000'\n",
            " '2023-03-14T00:00:00.000000000' '2023-03-15T00:00:00.000000000'\n",
            " '2023-03-16T00:00:00.000000000' '2023-03-17T00:00:00.000000000'\n",
            " '2023-03-20T00:00:00.000000000' '2023-03-21T00:00:00.000000000'\n",
            " '2023-03-22T00:00:00.000000000' '2023-03-23T00:00:00.000000000'\n",
            " '2023-03-24T00:00:00.000000000' '2023-03-27T00:00:00.000000000'\n",
            " '2023-03-28T00:00:00.000000000' '2023-03-29T00:00:00.000000000'\n",
            " '2023-03-30T00:00:00.000000000' '2023-03-31T00:00:00.000000000'\n",
            " '2023-04-03T00:00:00.000000000' '2023-04-04T00:00:00.000000000']\n",
            "-------------------------------\n",
            "2\n",
            "['2023-01-03T00:00:00.000000000' '2023-01-04T00:00:00.000000000'\n",
            " '2023-01-05T00:00:00.000000000' '2023-01-06T00:00:00.000000000'\n",
            " '2023-01-09T00:00:00.000000000' '2023-01-10T00:00:00.000000000'\n",
            " '2023-01-11T00:00:00.000000000' '2023-01-12T00:00:00.000000000'\n",
            " '2023-01-13T00:00:00.000000000' '2023-01-17T00:00:00.000000000'\n",
            " '2023-01-18T00:00:00.000000000' '2023-01-19T00:00:00.000000000'\n",
            " '2023-01-20T00:00:00.000000000' '2023-01-23T00:00:00.000000000'\n",
            " '2023-01-24T00:00:00.000000000' '2023-01-25T00:00:00.000000000'\n",
            " '2023-01-26T00:00:00.000000000' '2023-01-27T00:00:00.000000000'\n",
            " '2023-01-30T00:00:00.000000000' '2023-01-31T00:00:00.000000000'\n",
            " '2023-02-01T00:00:00.000000000' '2023-02-02T00:00:00.000000000'\n",
            " '2023-02-03T00:00:00.000000000' '2023-02-06T00:00:00.000000000'\n",
            " '2023-02-07T00:00:00.000000000' '2023-02-08T00:00:00.000000000'\n",
            " '2023-02-09T00:00:00.000000000' '2023-02-10T00:00:00.000000000'\n",
            " '2023-02-13T00:00:00.000000000' '2023-02-14T00:00:00.000000000'\n",
            " '2023-02-15T00:00:00.000000000' '2023-02-16T00:00:00.000000000'\n",
            " '2023-02-17T00:00:00.000000000' '2023-02-21T00:00:00.000000000'\n",
            " '2023-02-22T00:00:00.000000000' '2023-02-23T00:00:00.000000000'\n",
            " '2023-02-24T00:00:00.000000000' '2023-02-27T00:00:00.000000000'\n",
            " '2023-02-28T00:00:00.000000000' '2023-03-01T00:00:00.000000000'\n",
            " '2023-03-02T00:00:00.000000000' '2023-03-03T00:00:00.000000000'\n",
            " '2023-03-06T00:00:00.000000000' '2023-03-07T00:00:00.000000000'\n",
            " '2023-03-08T00:00:00.000000000' '2023-03-09T00:00:00.000000000'\n",
            " '2023-03-10T00:00:00.000000000' '2023-03-13T00:00:00.000000000'\n",
            " '2023-03-14T00:00:00.000000000' '2023-03-15T00:00:00.000000000'\n",
            " '2023-03-16T00:00:00.000000000' '2023-03-17T00:00:00.000000000'\n",
            " '2023-03-20T00:00:00.000000000' '2023-03-21T00:00:00.000000000'\n",
            " '2023-03-22T00:00:00.000000000' '2023-03-23T00:00:00.000000000'\n",
            " '2023-03-24T00:00:00.000000000' '2023-03-27T00:00:00.000000000'\n",
            " '2023-03-28T00:00:00.000000000' '2023-03-29T00:00:00.000000000'\n",
            " '2023-03-30T00:00:00.000000000' '2023-03-31T00:00:00.000000000'\n",
            " '2023-04-03T00:00:00.000000000' '2023-04-04T00:00:00.000000000'\n",
            " '2023-04-05T00:00:00.000000000' '2023-04-06T00:00:00.000000000'\n",
            " '2023-04-10T00:00:00.000000000' '2023-04-11T00:00:00.000000000'\n",
            " '2023-04-12T00:00:00.000000000' '2023-04-13T00:00:00.000000000'\n",
            " '2023-04-14T00:00:00.000000000' '2023-04-17T00:00:00.000000000'\n",
            " '2023-04-18T00:00:00.000000000' '2023-04-19T00:00:00.000000000'\n",
            " '2023-04-20T00:00:00.000000000' '2023-04-21T00:00:00.000000000'\n",
            " '2023-04-24T00:00:00.000000000' '2023-04-25T00:00:00.000000000'\n",
            " '2023-04-26T00:00:00.000000000' '2023-04-27T00:00:00.000000000'\n",
            " '2023-04-28T00:00:00.000000000' '2023-05-01T00:00:00.000000000'\n",
            " '2023-05-02T00:00:00.000000000' '2023-05-03T00:00:00.000000000'\n",
            " '2023-05-04T00:00:00.000000000' '2023-05-05T00:00:00.000000000'\n",
            " '2023-05-08T00:00:00.000000000' '2023-05-09T00:00:00.000000000'\n",
            " '2023-05-10T00:00:00.000000000' '2023-05-11T00:00:00.000000000'\n",
            " '2023-05-12T00:00:00.000000000' '2023-05-15T00:00:00.000000000'\n",
            " '2023-05-16T00:00:00.000000000' '2023-05-17T00:00:00.000000000'\n",
            " '2023-05-18T00:00:00.000000000' '2023-05-19T00:00:00.000000000'\n",
            " '2023-05-22T00:00:00.000000000' '2023-05-23T00:00:00.000000000'\n",
            " '2023-05-24T00:00:00.000000000' '2023-05-25T00:00:00.000000000'\n",
            " '2023-05-26T00:00:00.000000000' '2023-05-30T00:00:00.000000000'\n",
            " '2023-05-31T00:00:00.000000000' '2023-06-01T00:00:00.000000000'\n",
            " '2023-06-02T00:00:00.000000000' '2023-06-05T00:00:00.000000000'\n",
            " '2023-06-06T00:00:00.000000000' '2023-06-07T00:00:00.000000000'\n",
            " '2023-06-08T00:00:00.000000000' '2023-06-09T00:00:00.000000000'\n",
            " '2023-06-12T00:00:00.000000000' '2023-06-13T00:00:00.000000000'\n",
            " '2023-06-14T00:00:00.000000000' '2023-06-15T00:00:00.000000000'\n",
            " '2023-06-16T00:00:00.000000000' '2023-06-20T00:00:00.000000000'\n",
            " '2023-06-21T00:00:00.000000000' '2023-06-22T00:00:00.000000000'\n",
            " '2023-06-23T00:00:00.000000000' '2023-06-26T00:00:00.000000000'\n",
            " '2023-06-27T00:00:00.000000000' '2023-06-28T00:00:00.000000000'\n",
            " '2023-06-29T00:00:00.000000000' '2023-06-30T00:00:00.000000000'\n",
            " '2023-07-03T00:00:00.000000000' '2023-07-05T00:00:00.000000000'\n",
            " '2023-07-06T00:00:00.000000000' '2023-07-07T00:00:00.000000000'\n",
            " '2023-07-10T00:00:00.000000000' '2023-07-11T00:00:00.000000000'\n",
            " '2023-07-12T00:00:00.000000000' '2023-07-13T00:00:00.000000000'\n",
            " '2023-07-14T00:00:00.000000000' '2023-07-17T00:00:00.000000000'\n",
            " '2023-07-18T00:00:00.000000000' '2023-07-19T00:00:00.000000000'\n",
            " '2023-07-20T00:00:00.000000000' '2023-07-21T00:00:00.000000000'\n",
            " '2023-07-24T00:00:00.000000000' '2023-07-25T00:00:00.000000000'\n",
            " '2023-07-26T00:00:00.000000000' '2023-07-27T00:00:00.000000000'\n",
            " '2023-07-28T00:00:00.000000000' '2023-07-31T00:00:00.000000000'\n",
            " '2023-08-01T00:00:00.000000000' '2023-08-02T00:00:00.000000000'\n",
            " '2023-08-03T00:00:00.000000000' '2023-08-04T00:00:00.000000000'\n",
            " '2023-08-07T00:00:00.000000000' '2023-08-08T00:00:00.000000000'\n",
            " '2023-08-09T00:00:00.000000000' '2023-08-10T00:00:00.000000000'\n",
            " '2023-08-11T00:00:00.000000000' '2023-08-14T00:00:00.000000000'\n",
            " '2023-08-15T00:00:00.000000000' '2023-08-16T00:00:00.000000000'\n",
            " '2023-08-17T00:00:00.000000000' '2023-08-18T00:00:00.000000000'\n",
            " '2023-08-21T00:00:00.000000000' '2023-08-22T00:00:00.000000000'\n",
            " '2023-08-23T00:00:00.000000000' '2023-08-24T00:00:00.000000000'\n",
            " '2023-08-25T00:00:00.000000000' '2023-08-28T00:00:00.000000000'\n",
            " '2023-08-29T00:00:00.000000000' '2023-08-30T00:00:00.000000000'\n",
            " '2023-08-31T00:00:00.000000000' '2023-09-01T00:00:00.000000000'\n",
            " '2023-09-05T00:00:00.000000000' '2023-09-06T00:00:00.000000000'\n",
            " '2023-09-07T00:00:00.000000000' '2023-09-08T00:00:00.000000000'\n",
            " '2023-09-11T00:00:00.000000000' '2023-09-12T00:00:00.000000000'\n",
            " '2023-09-13T00:00:00.000000000' '2023-09-14T00:00:00.000000000'\n",
            " '2023-09-15T00:00:00.000000000' '2023-09-18T00:00:00.000000000'\n",
            " '2023-09-19T00:00:00.000000000' '2023-09-20T00:00:00.000000000'\n",
            " '2023-09-21T00:00:00.000000000' '2023-09-22T00:00:00.000000000'\n",
            " '2023-09-25T00:00:00.000000000' '2023-09-26T00:00:00.000000000'\n",
            " '2023-09-27T00:00:00.000000000' '2023-09-28T00:00:00.000000000'\n",
            " '2023-09-29T00:00:00.000000000' '2023-10-02T00:00:00.000000000'\n",
            " '2023-10-03T00:00:00.000000000' '2023-10-04T00:00:00.000000000'\n",
            " '2023-10-05T00:00:00.000000000' '2023-10-06T00:00:00.000000000'\n",
            " '2023-10-09T00:00:00.000000000' '2023-10-10T00:00:00.000000000'\n",
            " '2023-10-11T00:00:00.000000000' '2023-10-12T00:00:00.000000000'\n",
            " '2023-10-13T00:00:00.000000000' '2023-10-16T00:00:00.000000000'\n",
            " '2023-10-17T00:00:00.000000000' '2023-10-18T00:00:00.000000000'\n",
            " '2023-10-19T00:00:00.000000000' '2023-10-20T00:00:00.000000000'\n",
            " '2023-10-23T00:00:00.000000000' '2023-10-24T00:00:00.000000000'\n",
            " '2023-10-25T00:00:00.000000000' '2023-10-26T00:00:00.000000000'\n",
            " '2023-10-27T00:00:00.000000000' '2023-10-30T00:00:00.000000000'\n",
            " '2023-10-31T00:00:00.000000000' '2023-11-01T00:00:00.000000000'\n",
            " '2023-11-02T00:00:00.000000000' '2023-11-03T00:00:00.000000000'\n",
            " '2023-11-06T00:00:00.000000000' '2023-11-07T00:00:00.000000000'\n",
            " '2023-11-08T00:00:00.000000000' '2023-11-09T00:00:00.000000000'\n",
            " '2023-11-10T00:00:00.000000000' '2023-11-13T00:00:00.000000000'\n",
            " '2023-11-14T00:00:00.000000000' '2023-11-15T00:00:00.000000000'\n",
            " '2023-11-16T00:00:00.000000000' '2023-11-17T00:00:00.000000000'\n",
            " '2023-11-20T00:00:00.000000000' '2023-11-21T00:00:00.000000000'\n",
            " '2023-11-22T00:00:00.000000000' '2023-11-24T00:00:00.000000000'\n",
            " '2023-11-27T00:00:00.000000000' '2023-11-28T00:00:00.000000000'\n",
            " '2023-11-29T00:00:00.000000000' '2023-11-30T00:00:00.000000000'\n",
            " '2023-12-01T00:00:00.000000000' '2023-12-04T00:00:00.000000000'\n",
            " '2023-12-05T00:00:00.000000000' '2023-12-06T00:00:00.000000000'\n",
            " '2023-12-07T00:00:00.000000000' '2023-12-08T00:00:00.000000000'\n",
            " '2023-12-11T00:00:00.000000000' '2023-12-12T00:00:00.000000000'\n",
            " '2023-12-13T00:00:00.000000000' '2023-12-14T00:00:00.000000000'\n",
            " '2023-12-15T00:00:00.000000000' '2023-12-18T00:00:00.000000000'\n",
            " '2023-12-19T00:00:00.000000000' '2023-12-20T00:00:00.000000000'\n",
            " '2023-12-21T00:00:00.000000000' '2023-12-22T00:00:00.000000000'\n",
            " '2023-12-26T00:00:00.000000000' '2023-12-27T00:00:00.000000000'\n",
            " '2023-12-28T00:00:00.000000000' '2023-12-29T00:00:00.000000000']\n",
            "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:01<00:00,  2.90it/s]\n",
            "[2024-09-16 09:53:07,101] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-09-16 09:53:07,302] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
            "[2024-09-16 09:53:07,302] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-09-16 09:53:07,302] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
            "[2024-09-16 09:53:07,552] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=192.168.59.30, master_port=29500\n",
            "[2024-09-16 09:53:07,552] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2024-09-16 09:53:10,632] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2024-09-16 09:53:10,633] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
            "[2024-09-16 09:53:10,633] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2024-09-16 09:53:10,633] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
            "[2024-09-16 09:53:10,633] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
            "[2024-09-16 09:53:10,634] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
            "[2024-09-16 09:53:10,634] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
            "[2024-09-16 09:53:10,634] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
            "[2024-09-16 09:53:10,634] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
            "[2024-09-16 09:53:10,634] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
            "[2024-09-16 09:53:10,936] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
            "[2024-09-16 09:53:10,937] [INFO] [utils.py:801:see_memory_usage] MA 4.08 GB         Max_MA 4.15 GB         CA 4.15 GB         Max_CA 4 GB \n",
            "[2024-09-16 09:53:10,937] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.02 GB, percent = 20.7%\n",
            "[2024-09-16 09:53:11,097] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
            "[2024-09-16 09:53:11,098] [INFO] [utils.py:801:see_memory_usage] MA 4.08 GB         Max_MA 4.21 GB         CA 4.28 GB         Max_CA 4 GB \n",
            "[2024-09-16 09:53:11,098] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.02 GB, percent = 20.8%\n",
            "[2024-09-16 09:53:11,098] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
            "[2024-09-16 09:53:11,260] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2024-09-16 09:53:11,260] [INFO] [utils.py:801:see_memory_usage] MA 4.08 GB         Max_MA 4.08 GB         CA 4.28 GB         Max_CA 4 GB \n",
            "[2024-09-16 09:53:11,260] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.02 GB, percent = 20.8%\n",
            "[2024-09-16 09:53:11,261] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
            "[2024-09-16 09:53:11,261] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
            "[2024-09-16 09:53:11,261] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
            "[2024-09-16 09:53:11,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4.000000000000002e-06], mom=[(0.95, 0.999)]\n",
            "[2024-09-16 09:53:11,262] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
            "[2024-09-16 09:53:11,262] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2024-09-16 09:53:11,262] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2024-09-16 09:53:11,262] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
            "[2024-09-16 09:53:11,262] [INFO] [config.py:1000:print]   amp_params ................... False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7252895d2740>\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   dump_state ................... False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
            "[2024-09-16 09:53:11,263] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   pld_params ................... False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   train_batch_size ............. 5\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  5\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
            "[2024-09-16 09:53:11,264] [INFO] [config.py:1000:print]   world_size ................... 1\n",
            "[2024-09-16 09:53:11,265] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
            "[2024-09-16 09:53:11,265] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2024-09-16 09:53:11,265] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
            "[2024-09-16 09:53:11,265] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2024-09-16 09:53:11,265] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
            "[2024-09-16 09:53:11,265] [INFO] [config.py:986:print_user_config]   json = {\n",
            "    \"bf16\": {\n",
            "        \"enabled\": true, \n",
            "        \"auto_cast\": true\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 2, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 2.000000e+08, \n",
            "        \"overlap_comm\": true, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 2.000000e+08, \n",
            "        \"contiguous_gradients\": true, \n",
            "        \"sub_group_size\": 1.000000e+09\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"train_batch_size\": 5, \n",
            "    \"train_micro_batch_size_per_gpu\": 5, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"wall_clock_breakdown\": false, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": false\n",
            "    }, \n",
            "    \"zero_allow_untested_optimizer\": true\n",
            "}\n",
            "99it [00:12,  9.23it/s]\titers: 100, epoch: 1 | loss: 0.0048906\n",
            "\tspeed: 0.1734s/iter; left time: 8763.2588s\n",
            "199it [00:23,  9.23it/s]\titers: 200, epoch: 1 | loss: 0.0020118\n",
            "\tspeed: 0.1089s/iter; left time: 5493.3878s\n",
            "299it [00:33,  9.14it/s]\titers: 300, epoch: 1 | loss: 0.0347027\n",
            "\tspeed: 0.1099s/iter; left time: 5532.3891s\n",
            "399it [00:44,  8.96it/s]\titers: 400, epoch: 1 | loss: 0.0396048\n",
            "\tspeed: 0.1098s/iter; left time: 5519.8690s\n",
            "499it [00:56,  9.13it/s]\titers: 500, epoch: 1 | loss: 0.0018384\n",
            "\tspeed: 0.1102s/iter; left time: 5524.7502s\n",
            "599it [01:07,  8.98it/s]\titers: 600, epoch: 1 | loss: 0.0108324\n",
            "\tspeed: 0.1114s/iter; left time: 5574.2284s\n",
            "699it [01:18,  8.88it/s]\titers: 700, epoch: 1 | loss: 0.0070887\n",
            "\tspeed: 0.1116s/iter; left time: 5573.0133s\n",
            "799it [01:29,  8.94it/s]\titers: 800, epoch: 1 | loss: 0.0027419\n",
            "\tspeed: 0.1119s/iter; left time: 5575.8482s\n",
            "899it [01:40,  8.92it/s]\titers: 900, epoch: 1 | loss: 0.0050218\n",
            "\tspeed: 0.1114s/iter; left time: 5542.3413s\n",
            "999it [01:51,  8.98it/s]\titers: 1000, epoch: 1 | loss: 0.0008086\n",
            "\tspeed: 0.1113s/iter; left time: 5523.6858s\n",
            "1013it [01:53,  8.94it/s]\n",
            "Epoch: 1 cost time: 113.31917238235474\n",
            "144it [00:07, 18.58it/s]\n",
            "37it [00:02, 16.42it/s]\n",
            "Epoch: 1 | Train Loss: 0.0159270 Vali Loss: 0.4094956 Test Loss: 0.3671114 MAE Loss: 0.5610538\n",
            "lr = 0.0000040000\n",
            "Updating learning rate to 4.000000000000002e-06\n",
            "99it [00:11,  8.96it/s]\titers: 100, epoch: 2 | loss: 0.0002084\n",
            "\tspeed: 0.3162s/iter; left time: 15664.8378s\n",
            "199it [00:22,  8.79it/s]\titers: 200, epoch: 2 | loss: 0.0018531\n",
            "\tspeed: 0.1123s/iter; left time: 5550.5404s\n",
            "299it [00:33,  8.92it/s]\titers: 300, epoch: 2 | loss: 0.0097658\n",
            "\tspeed: 0.1122s/iter; left time: 5535.2015s\n",
            "399it [00:45,  8.81it/s]\titers: 400, epoch: 2 | loss: 0.0212499\n",
            "\tspeed: 0.1122s/iter; left time: 5523.8119s\n",
            "499it [00:56,  8.89it/s]\titers: 500, epoch: 2 | loss: 0.0031181\n",
            "\tspeed: 0.1126s/iter; left time: 5532.3708s\n",
            "599it [01:07,  8.90it/s]\titers: 600, epoch: 2 | loss: 0.0042625\n",
            "\tspeed: 0.1122s/iter; left time: 5502.3322s\n",
            "699it [01:18,  8.94it/s]\titers: 700, epoch: 2 | loss: 0.0068524\n",
            "\tspeed: 0.1130s/iter; left time: 5527.8599s\n",
            "799it [01:30,  8.97it/s]\titers: 800, epoch: 2 | loss: 0.0116835\n",
            "\tspeed: 0.1128s/iter; left time: 5510.6177s\n",
            "899it [01:41,  8.93it/s]\titers: 900, epoch: 2 | loss: 0.0037790\n",
            "\tspeed: 0.1126s/iter; left time: 5486.6878s\n",
            "999it [01:52,  8.97it/s]\titers: 1000, epoch: 2 | loss: 0.0014413\n",
            "\tspeed: 0.1132s/iter; left time: 5505.4391s\n",
            "1013it [01:54,  8.87it/s]\n",
            "Epoch: 2 cost time: 114.24968338012695\n",
            "144it [00:07, 18.80it/s]\n",
            "37it [00:02, 16.60it/s]\n",
            "Epoch: 2 | Train Loss: 0.0141008 Vali Loss: 0.4151472 Test Loss: 0.3249654 MAE Loss: 0.5215734\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 2.000000000000001e-06\n",
            "99it [00:11,  8.67it/s]\titers: 100, epoch: 3 | loss: 0.0037572\n",
            "\tspeed: 0.2316s/iter; left time: 11237.7762s\n",
            "199it [00:23,  8.78it/s]\titers: 200, epoch: 3 | loss: 0.0033460\n",
            "\tspeed: 0.1140s/iter; left time: 5520.4407s\n",
            "299it [00:34,  8.74it/s]\titers: 300, epoch: 3 | loss: 0.0062388\n",
            "\tspeed: 0.1137s/iter; left time: 5495.0764s\n",
            "399it [00:45,  8.71it/s]\titers: 400, epoch: 3 | loss: 0.0006939\n",
            "\tspeed: 0.1141s/iter; left time: 5501.4632s\n",
            "499it [00:57,  8.71it/s]\titers: 500, epoch: 3 | loss: 0.0111403\n",
            "\tspeed: 0.1144s/iter; left time: 5506.3876s\n",
            "599it [01:08,  8.79it/s]\titers: 600, epoch: 3 | loss: 0.0003108\n",
            "\tspeed: 0.1135s/iter; left time: 5450.4061s\n",
            "699it [01:20,  8.77it/s]\titers: 700, epoch: 3 | loss: 0.0049458\n",
            "\tspeed: 0.1140s/iter; left time: 5461.9020s\n",
            "799it [01:31,  8.66it/s]\titers: 800, epoch: 3 | loss: 0.0034512\n",
            "\tspeed: 0.1136s/iter; left time: 5434.8683s\n",
            "899it [01:42,  8.73it/s]\titers: 900, epoch: 3 | loss: 0.0041686\n",
            "\tspeed: 0.1134s/iter; left time: 5414.3279s\n",
            "999it [01:54,  8.73it/s]\titers: 1000, epoch: 3 | loss: 0.0289952\n",
            "\tspeed: 0.1137s/iter; left time: 5414.9303s\n",
            "1013it [01:55,  8.75it/s]\n",
            "Epoch: 3 cost time: 115.71910881996155\n",
            "144it [00:07, 18.62it/s]\n",
            "37it [00:02, 15.90it/s]\n",
            "Epoch: 3 | Train Loss: 0.0117406 Vali Loss: 0.2816522 Test Loss: 0.2596362 MAE Loss: 0.4635374\n",
            "Updating learning rate to 1.0000000000000006e-06\n",
            "99it [00:11,  8.90it/s]\titers: 100, epoch: 4 | loss: 0.0012490\n",
            "\tspeed: 0.3836s/iter; left time: 18227.6114s\n",
            "199it [00:22,  8.75it/s]\titers: 200, epoch: 4 | loss: 0.0053393\n",
            "\tspeed: 0.1129s/iter; left time: 5350.4583s\n",
            "299it [00:34,  8.92it/s]\titers: 300, epoch: 4 | loss: 0.0001023\n",
            "\tspeed: 0.1126s/iter; left time: 5329.1729s\n",
            "399it [00:45,  8.87it/s]\titers: 400, epoch: 4 | loss: 0.0085566\n",
            "\tspeed: 0.1124s/iter; left time: 5305.8668s\n",
            "499it [00:56,  8.89it/s]\titers: 500, epoch: 4 | loss: 0.0024959\n",
            "\tspeed: 0.1137s/iter; left time: 5357.9005s\n",
            "599it [01:08,  8.83it/s]\titers: 600, epoch: 4 | loss: 0.0023622\n",
            "\tspeed: 0.1135s/iter; left time: 5336.1078s\n",
            "699it [01:19,  8.92it/s]\titers: 700, epoch: 4 | loss: 0.0006212\n",
            "\tspeed: 0.1124s/iter; left time: 5271.3274s\n",
            "799it [01:30,  8.65it/s]\titers: 800, epoch: 4 | loss: 0.0245463\n",
            "\tspeed: 0.1127s/iter; left time: 5276.9331s\n",
            "899it [01:41,  8.91it/s]\titers: 900, epoch: 4 | loss: 0.0178242\n",
            "\tspeed: 0.1127s/iter; left time: 5265.6023s\n",
            "999it [01:53,  8.95it/s]\titers: 1000, epoch: 4 | loss: 0.0088680\n",
            "\tspeed: 0.1129s/iter; left time: 5261.9457s\n",
            "1013it [01:54,  8.83it/s]\n",
            "Epoch: 4 cost time: 114.70614457130432\n",
            "144it [00:07, 18.62it/s]\n",
            "37it [00:02, 16.33it/s]\n",
            "Epoch: 4 | Train Loss: 0.0104966 Vali Loss: 0.2677236 Test Loss: 0.2366510 MAE Loss: 0.4407426\n",
            "Updating learning rate to 5.000000000000003e-07\n",
            "99it [00:11,  8.90it/s]\titers: 100, epoch: 5 | loss: 0.0020538\n",
            "\tspeed: 0.3792s/iter; left time: 17634.7029s\n",
            "199it [00:22,  8.96it/s]\titers: 200, epoch: 5 | loss: 0.0072459\n",
            "\tspeed: 0.1124s/iter; left time: 5215.6954s\n",
            "299it [00:33,  8.74it/s]\titers: 300, epoch: 5 | loss: 0.0060089\n",
            "\tspeed: 0.1119s/iter; left time: 5181.8635s\n",
            "399it [00:45,  8.89it/s]\titers: 400, epoch: 5 | loss: 0.0060288\n",
            "\tspeed: 0.1130s/iter; left time: 5220.8895s\n",
            "499it [00:56,  8.91it/s]\titers: 500, epoch: 5 | loss: 0.0112305\n",
            "\tspeed: 0.1123s/iter; left time: 5179.0083s\n",
            "599it [01:07,  8.92it/s]\titers: 600, epoch: 5 | loss: 0.0009158\n",
            "\tspeed: 0.1127s/iter; left time: 5182.9970s\n",
            "699it [01:19,  8.89it/s]\titers: 700, epoch: 5 | loss: 0.0124311\n",
            "\tspeed: 0.1129s/iter; left time: 5180.7209s\n",
            "799it [01:30,  8.87it/s]\titers: 800, epoch: 5 | loss: 0.0003990\n",
            "\tspeed: 0.1127s/iter; left time: 5163.5712s\n",
            "899it [01:41,  8.85it/s]\titers: 900, epoch: 5 | loss: 0.0007784\n",
            "\tspeed: 0.1127s/iter; left time: 5149.4836s\n",
            "999it [01:52,  8.86it/s]\titers: 1000, epoch: 5 | loss: 0.0171074\n",
            "\tspeed: 0.1135s/iter; left time: 5174.4323s\n",
            "1013it [01:54,  8.84it/s]\n",
            "Epoch: 5 cost time: 114.54769563674927\n",
            "144it [00:07, 18.50it/s]\n",
            "37it [00:02, 16.26it/s]\n",
            "Epoch: 5 | Train Loss: 0.0099987 Vali Loss: 0.2525771 Test Loss: 0.2229746 MAE Loss: 0.4274273\n",
            "Updating learning rate to 2.5000000000000015e-07\n",
            "99it [00:11,  8.90it/s]\titers: 100, epoch: 6 | loss: 0.0023075\n",
            "\tspeed: 0.3815s/iter; left time: 17353.8376s\n",
            "199it [00:22,  8.74it/s]\titers: 200, epoch: 6 | loss: 0.0125873\n",
            "\tspeed: 0.1120s/iter; left time: 5084.1343s\n",
            "299it [00:33,  8.94it/s]\titers: 300, epoch: 6 | loss: 0.0107175\n",
            "\tspeed: 0.1127s/iter; left time: 5102.7390s\n",
            "399it [00:45,  8.78it/s]\titers: 400, epoch: 6 | loss: 0.0131782\n",
            "\tspeed: 0.1139s/iter; left time: 5145.3693s\n",
            "499it [00:56,  8.94it/s]\titers: 500, epoch: 6 | loss: 0.0011766\n",
            "\tspeed: 0.1132s/iter; left time: 5101.6933s\n",
            "599it [01:07,  8.86it/s]\titers: 600, epoch: 6 | loss: 0.0019885\n",
            "\tspeed: 0.1132s/iter; left time: 5090.8586s\n",
            "699it [01:19,  8.96it/s]\titers: 700, epoch: 6 | loss: 0.0066196\n",
            "\tspeed: 0.1132s/iter; left time: 5080.9553s\n",
            "799it [01:30,  8.92it/s]\titers: 800, epoch: 6 | loss: 0.0025351\n",
            "\tspeed: 0.1132s/iter; left time: 5068.0913s\n",
            "899it [01:41,  8.89it/s]\titers: 900, epoch: 6 | loss: 0.0018343\n",
            "\tspeed: 0.1126s/iter; left time: 5033.6171s\n",
            "999it [01:53,  8.83it/s]\titers: 1000, epoch: 6 | loss: 0.0001990\n",
            "\tspeed: 0.1127s/iter; left time: 5023.7429s\n",
            "1013it [01:54,  8.83it/s]\n",
            "Epoch: 6 cost time: 114.76552724838257\n",
            "144it [00:07, 18.66it/s]\n",
            "37it [00:02, 16.11it/s]\n",
            "Epoch: 6 | Train Loss: 0.0095096 Vali Loss: 0.2464094 Test Loss: 0.2177191 MAE Loss: 0.4240543\n",
            "Updating learning rate to 1.2500000000000007e-07\n",
            "99it [00:11,  8.75it/s]\titers: 100, epoch: 7 | loss: 0.0077687\n",
            "\tspeed: 0.3786s/iter; left time: 16837.2287s\n",
            "199it [00:22,  8.98it/s]\titers: 200, epoch: 7 | loss: 0.0044323\n",
            "\tspeed: 0.1129s/iter; left time: 5011.0538s\n",
            "299it [00:34,  8.95it/s]\titers: 300, epoch: 7 | loss: 0.0001912\n",
            "\tspeed: 0.1133s/iter; left time: 5015.1213s\n",
            "399it [00:45,  8.62it/s]\titers: 400, epoch: 7 | loss: 0.0006975\n",
            "\tspeed: 0.1137s/iter; left time: 5021.9113s\n",
            "499it [00:56,  8.72it/s]\titers: 500, epoch: 7 | loss: 0.0007903\n",
            "\tspeed: 0.1140s/iter; left time: 5025.8911s\n",
            "599it [01:08,  8.83it/s]\titers: 600, epoch: 7 | loss: 0.0012985\n",
            "\tspeed: 0.1138s/iter; left time: 5002.6490s\n",
            "699it [01:19,  8.57it/s]\titers: 700, epoch: 7 | loss: 0.0067721\n",
            "\tspeed: 0.1135s/iter; left time: 4981.4719s\n",
            "799it [01:30,  8.61it/s]\titers: 800, epoch: 7 | loss: 0.0051789\n",
            "\tspeed: 0.1137s/iter; left time: 4976.7797s\n",
            "899it [01:42,  8.86it/s]\titers: 900, epoch: 7 | loss: 0.0221428\n",
            "\tspeed: 0.1134s/iter; left time: 4954.4948s\n",
            "999it [01:53,  8.64it/s]\titers: 1000, epoch: 7 | loss: 0.0356223\n",
            "\tspeed: 0.1141s/iter; left time: 4971.1857s\n",
            "1013it [01:55,  8.79it/s]\n",
            "Epoch: 7 cost time: 115.30796456336975\n",
            "144it [00:07, 18.43it/s]\n",
            "37it [00:02, 15.86it/s]\n",
            "Epoch: 7 | Train Loss: 0.0096204 Vali Loss: 0.2411648 Test Loss: 0.2137825 MAE Loss: 0.4187114\n",
            "Updating learning rate to 6.250000000000004e-08\n",
            "99it [00:11,  8.83it/s]\titers: 100, epoch: 8 | loss: 0.0014365\n",
            "\tspeed: 0.3862s/iter; left time: 16785.7963s\n",
            "199it [00:22,  8.96it/s]\titers: 200, epoch: 8 | loss: 0.0006035\n",
            "\tspeed: 0.1128s/iter; left time: 4892.0939s\n",
            "299it [00:34,  8.92it/s]\titers: 300, epoch: 8 | loss: 0.0402506\n",
            "\tspeed: 0.1125s/iter; left time: 4866.2369s\n",
            "399it [00:45,  8.80it/s]\titers: 400, epoch: 8 | loss: 0.0008965\n",
            "\tspeed: 0.1131s/iter; left time: 4882.0523s\n",
            "499it [00:56,  8.86it/s]\titers: 500, epoch: 8 | loss: 0.0008529\n",
            "\tspeed: 0.1139s/iter; left time: 4905.8917s\n",
            "599it [01:08,  8.75it/s]\titers: 600, epoch: 8 | loss: 0.0153129\n",
            "\tspeed: 0.1136s/iter; left time: 4880.6349s\n",
            "699it [01:19,  8.85it/s]\titers: 700, epoch: 8 | loss: 0.0003923\n",
            "\tspeed: 0.1140s/iter; left time: 4885.4975s\n",
            "799it [01:30,  8.63it/s]\titers: 800, epoch: 8 | loss: 0.0031095\n",
            "\tspeed: 0.1135s/iter; left time: 4853.5004s\n",
            "899it [01:42,  8.79it/s]\titers: 900, epoch: 8 | loss: 0.0067931\n",
            "\tspeed: 0.1131s/iter; left time: 4826.0030s\n",
            "999it [01:53,  8.75it/s]\titers: 1000, epoch: 8 | loss: 0.0043410\n",
            "\tspeed: 0.1139s/iter; left time: 4846.0407s\n",
            "1013it [01:55,  8.80it/s]\n",
            "Epoch: 8 cost time: 115.16206502914429\n",
            "144it [00:07, 18.55it/s]\n",
            "37it [00:02, 15.72it/s]\n",
            "Epoch: 8 | Train Loss: 0.0091461 Vali Loss: 0.2408635 Test Loss: 0.2123751 MAE Loss: 0.4175232\n",
            "Updating learning rate to 3.125000000000002e-08\n",
            "99it [00:11,  8.64it/s]\titers: 100, epoch: 9 | loss: 0.0009238\n",
            "\tspeed: 0.3737s/iter; left time: 15861.8133s\n",
            "199it [00:22,  8.98it/s]\titers: 200, epoch: 9 | loss: 0.0032605\n",
            "\tspeed: 0.1124s/iter; left time: 4760.5116s\n",
            "299it [00:34,  8.93it/s]\titers: 300, epoch: 9 | loss: 0.0027521\n",
            "\tspeed: 0.1127s/iter; left time: 4763.1121s\n",
            "399it [00:45,  8.83it/s]\titers: 400, epoch: 9 | loss: 0.0517619\n",
            "\tspeed: 0.1133s/iter; left time: 4773.7930s\n",
            "499it [00:56,  8.88it/s]\titers: 500, epoch: 9 | loss: 0.0603357\n",
            "\tspeed: 0.1136s/iter; left time: 4775.9871s\n",
            "599it [01:08,  8.90it/s]\titers: 600, epoch: 9 | loss: 0.0024584\n",
            "\tspeed: 0.1136s/iter; left time: 4766.2820s\n",
            "699it [01:19,  8.90it/s]\titers: 700, epoch: 9 | loss: 0.0081787\n",
            "\tspeed: 0.1133s/iter; left time: 4742.4895s\n",
            "799it [01:30,  8.83it/s]\titers: 800, epoch: 9 | loss: 0.0015339\n",
            "\tspeed: 0.1134s/iter; left time: 4732.0783s\n",
            "899it [01:42,  8.76it/s]\titers: 900, epoch: 9 | loss: 0.0011770\n",
            "\tspeed: 0.1136s/iter; left time: 4731.5754s\n",
            "999it [01:53,  8.90it/s]\titers: 1000, epoch: 9 | loss: 0.0017832\n",
            "\tspeed: 0.1133s/iter; left time: 4706.7516s\n",
            "1013it [01:55,  8.80it/s]\n",
            "Epoch: 9 cost time: 115.0591950416565\n",
            "144it [00:07, 18.39it/s]\n",
            "37it [00:02, 16.27it/s]\n",
            "Epoch: 9 | Train Loss: 0.0091930 Vali Loss: 0.2440465 Test Loss: 0.2116430 MAE Loss: 0.4165266\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 1.562500000000001e-08\n",
            "99it [00:11,  8.93it/s]\titers: 100, epoch: 10 | loss: 0.0046984\n",
            "\tspeed: 0.2331s/iter; left time: 9659.6763s\n",
            "199it [00:22,  8.91it/s]\titers: 200, epoch: 10 | loss: 0.0040323\n",
            "\tspeed: 0.1137s/iter; left time: 4699.4511s\n",
            "299it [00:34,  8.73it/s]\titers: 300, epoch: 10 | loss: 0.0046672\n",
            "\tspeed: 0.1140s/iter; left time: 4700.6092s\n",
            "399it [00:45,  8.82it/s]\titers: 400, epoch: 10 | loss: 0.0012804\n",
            "\tspeed: 0.1131s/iter; left time: 4651.9897s\n",
            "499it [00:56,  8.71it/s]\titers: 500, epoch: 10 | loss: 0.1103031\n",
            "\tspeed: 0.1133s/iter; left time: 4649.8566s\n",
            "599it [01:08,  8.83it/s]\titers: 600, epoch: 10 | loss: 0.0024857\n",
            "\tspeed: 0.1136s/iter; left time: 4649.6336s\n",
            "699it [01:19,  8.58it/s]\titers: 700, epoch: 10 | loss: 0.0004580\n",
            "\tspeed: 0.1136s/iter; left time: 4639.5755s\n",
            "799it [01:31,  8.90it/s]\titers: 800, epoch: 10 | loss: 0.0123426\n",
            "\tspeed: 0.1139s/iter; left time: 4638.1286s\n",
            "899it [01:42,  8.73it/s]\titers: 900, epoch: 10 | loss: 0.0008359\n",
            "\tspeed: 0.1130s/iter; left time: 4590.7792s\n",
            "999it [01:53,  8.96it/s]\titers: 1000, epoch: 10 | loss: 0.0044377\n",
            "\tspeed: 0.1127s/iter; left time: 4570.1044s\n",
            "1013it [01:55,  8.79it/s]\n",
            "Epoch: 10 cost time: 115.28114438056946\n",
            "144it [00:07, 18.61it/s]\n",
            "37it [00:02, 16.12it/s]\n",
            "Epoch: 10 | Train Loss: 0.0092884 Vali Loss: 0.2389183 Test Loss: 0.2110105 MAE Loss: 0.4155884\n",
            "Updating learning rate to 7.812500000000005e-09\n",
            "99it [00:11,  8.90it/s]\titers: 100, epoch: 11 | loss: 0.0081360\n",
            "\tspeed: 0.3790s/iter; left time: 15318.1199s\n",
            "199it [00:22,  8.94it/s]\titers: 200, epoch: 11 | loss: 0.0050670\n",
            "\tspeed: 0.1121s/iter; left time: 4518.7752s\n",
            "299it [00:33,  8.97it/s]\titers: 300, epoch: 11 | loss: 0.0272985\n",
            "\tspeed: 0.1125s/iter; left time: 4522.9752s\n",
            "399it [00:45,  8.79it/s]\titers: 400, epoch: 11 | loss: 0.0010841\n",
            "\tspeed: 0.1130s/iter; left time: 4533.2023s\n",
            "499it [00:56,  8.82it/s]\titers: 500, epoch: 11 | loss: 0.0009129\n",
            "\tspeed: 0.1129s/iter; left time: 4517.7515s\n",
            "599it [01:07,  8.73it/s]\titers: 600, epoch: 11 | loss: 0.0716148\n",
            "\tspeed: 0.1140s/iter; left time: 4550.3901s\n",
            "699it [01:19,  8.59it/s]\titers: 700, epoch: 11 | loss: 0.0100028\n",
            "\tspeed: 0.1141s/iter; left time: 4544.2492s\n",
            "799it [01:30,  8.81it/s]\titers: 800, epoch: 11 | loss: 0.0037555\n",
            "\tspeed: 0.1140s/iter; left time: 4526.7020s\n",
            "899it [01:42,  8.60it/s]\titers: 900, epoch: 11 | loss: 0.0019484\n",
            "\tspeed: 0.1138s/iter; left time: 4509.6063s\n",
            "999it [01:53,  8.76it/s]\titers: 1000, epoch: 11 | loss: 0.0051996\n",
            "\tspeed: 0.1139s/iter; left time: 4500.4958s\n",
            "1013it [01:55,  8.80it/s]\n",
            "Epoch: 11 cost time: 115.07632184028625\n",
            "144it [00:07, 18.50it/s]\n",
            "37it [00:02, 15.96it/s]\n",
            "Epoch: 11 | Train Loss: 0.0092908 Vali Loss: 0.2403506 Test Loss: 0.2111003 MAE Loss: 0.4152564\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 3.906250000000002e-09\n",
            "99it [00:11,  8.59it/s]\titers: 100, epoch: 12 | loss: 0.0734922\n",
            "\tspeed: 0.2342s/iter; left time: 9227.4388s\n",
            "199it [00:23,  8.75it/s]\titers: 200, epoch: 12 | loss: 0.0016287\n",
            "\tspeed: 0.1137s/iter; left time: 4470.2750s\n",
            "299it [00:34,  8.86it/s]\titers: 300, epoch: 12 | loss: 0.0071906\n",
            "\tspeed: 0.1142s/iter; left time: 4477.5382s\n",
            "399it [00:45,  8.76it/s]\titers: 400, epoch: 12 | loss: 0.0023055\n",
            "\tspeed: 0.1140s/iter; left time: 4457.2759s\n",
            "499it [00:57,  8.80it/s]\titers: 500, epoch: 12 | loss: 0.0014862\n",
            "\tspeed: 0.1139s/iter; left time: 4441.5147s\n",
            "599it [01:08,  8.62it/s]\titers: 600, epoch: 12 | loss: 0.0042661\n",
            "\tspeed: 0.1148s/iter; left time: 4465.4484s\n",
            "699it [01:20,  8.69it/s]\titers: 700, epoch: 12 | loss: 0.0039326\n",
            "\tspeed: 0.1146s/iter; left time: 4447.4525s\n",
            "799it [01:31,  8.67it/s]\titers: 800, epoch: 12 | loss: 0.0017546\n",
            "\tspeed: 0.1149s/iter; left time: 4445.8592s\n",
            "899it [01:43,  8.86it/s]\titers: 900, epoch: 12 | loss: 0.0011173\n",
            "\tspeed: 0.1146s/iter; left time: 4422.8191s\n",
            "999it [01:54,  8.67it/s]\titers: 1000, epoch: 12 | loss: 0.0055513\n",
            "\tspeed: 0.1145s/iter; left time: 4410.3449s\n",
            "1013it [01:56,  8.72it/s]\n",
            "Epoch: 12 cost time: 116.23352146148682\n",
            "144it [00:07, 18.28it/s]\n",
            "37it [00:02, 15.83it/s]\n",
            "Epoch: 12 | Train Loss: 0.0093146 Vali Loss: 0.2405822 Test Loss: 0.2109115 MAE Loss: 0.4152605\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Updating learning rate to 1.953125000000001e-09\n",
            "99it [00:11,  8.76it/s]\titers: 100, epoch: 13 | loss: 0.0179504\n",
            "\tspeed: 0.2344s/iter; left time: 9001.0057s\n",
            "199it [00:22,  8.82it/s]\titers: 200, epoch: 13 | loss: 0.0008308\n",
            "\tspeed: 0.1135s/iter; left time: 4347.2486s\n",
            "299it [00:34,  8.83it/s]\titers: 300, epoch: 13 | loss: 0.0520658\n",
            "\tspeed: 0.1139s/iter; left time: 4349.6796s\n",
            "399it [00:45,  8.92it/s]\titers: 400, epoch: 13 | loss: 0.0100657\n",
            "\tspeed: 0.1139s/iter; left time: 4338.6667s\n",
            "499it [00:57,  8.77it/s]\titers: 500, epoch: 13 | loss: 0.0018184\n",
            "\tspeed: 0.1136s/iter; left time: 4316.6920s\n",
            "599it [01:08,  8.77it/s]\titers: 600, epoch: 13 | loss: 0.0008186\n",
            "\tspeed: 0.1132s/iter; left time: 4290.5105s\n",
            "699it [01:19,  8.78it/s]\titers: 700, epoch: 13 | loss: 0.0009583\n",
            "\tspeed: 0.1131s/iter; left time: 4273.4939s\n",
            "799it [01:31,  8.93it/s]\titers: 800, epoch: 13 | loss: 0.0053069\n",
            "\tspeed: 0.1135s/iter; left time: 4280.0535s\n",
            "899it [01:42,  8.91it/s]\titers: 900, epoch: 13 | loss: 0.0025085\n",
            "\tspeed: 0.1137s/iter; left time: 4273.2962s\n",
            "999it [01:53,  8.88it/s]\titers: 1000, epoch: 13 | loss: 0.0032725\n",
            "\tspeed: 0.1133s/iter; left time: 4248.2257s\n",
            "1013it [01:55,  8.78it/s]\n",
            "Epoch: 13 cost time: 115.38762950897217\n",
            "144it [00:07, 18.49it/s]\n",
            "37it [00:02, 16.38it/s]\n",
            "Epoch: 13 | Train Loss: 0.0092050 Vali Loss: 0.2359233 Test Loss: 0.2110119 MAE Loss: 0.4154169\n",
            "Updating learning rate to 9.765625000000006e-10\n",
            "99it [00:11,  8.87it/s]\titers: 100, epoch: 14 | loss: 0.0002552\n",
            "\tspeed: 0.3553s/iter; left time: 13281.3078s\n",
            "199it [00:22,  8.97it/s]\titers: 200, epoch: 14 | loss: 0.0080274\n",
            "\tspeed: 0.1120s/iter; left time: 4176.5540s\n",
            "299it [00:34,  8.84it/s]\titers: 300, epoch: 14 | loss: 0.0041766\n",
            "\tspeed: 0.1129s/iter; left time: 4199.5986s\n",
            "399it [00:45,  8.82it/s]\titers: 400, epoch: 14 | loss: 0.0038855\n",
            "\tspeed: 0.1129s/iter; left time: 4184.8687s\n",
            "499it [00:56,  8.91it/s]\titers: 500, epoch: 14 | loss: 0.0007891\n",
            "\tspeed: 0.1128s/iter; left time: 4170.8516s\n",
            "599it [01:07,  8.70it/s]\titers: 600, epoch: 14 | loss: 0.0013901\n",
            "\tspeed: 0.1136s/iter; left time: 4189.7197s\n",
            "699it [01:19,  8.83it/s]\titers: 700, epoch: 14 | loss: 0.0059297\n",
            "\tspeed: 0.1141s/iter; left time: 4195.7771s\n",
            "799it [01:30,  8.75it/s]\titers: 800, epoch: 14 | loss: 0.0031175\n",
            "\tspeed: 0.1135s/iter; left time: 4164.1594s\n",
            "899it [01:42,  8.81it/s]\titers: 900, epoch: 14 | loss: 0.0042742\n",
            "\tspeed: 0.1139s/iter; left time: 4167.8358s\n",
            "999it [01:53,  8.82it/s]\titers: 1000, epoch: 14 | loss: 0.0014217\n",
            "\tspeed: 0.1137s/iter; left time: 4149.7103s\n",
            "1013it [01:55,  8.80it/s]\n",
            "Epoch: 14 cost time: 115.10464787483215\n",
            "144it [00:07, 18.45it/s]\n",
            "37it [00:02, 16.07it/s]\n",
            "Epoch: 14 | Train Loss: 0.0092346 Vali Loss: 0.2438475 Test Loss: 0.2111382 MAE Loss: 0.4155077\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 4.882812500000003e-10\n",
            "99it [00:11,  8.86it/s]\titers: 100, epoch: 15 | loss: 0.0005678\n",
            "\tspeed: 0.2336s/iter; left time: 8497.0252s\n",
            "199it [00:22,  8.70it/s]\titers: 200, epoch: 15 | loss: 0.0164499\n",
            "\tspeed: 0.1133s/iter; left time: 4111.0817s\n",
            "299it [00:34,  8.86it/s]\titers: 300, epoch: 15 | loss: 0.0694464\n",
            "\tspeed: 0.1138s/iter; left time: 4115.4526s\n",
            "399it [00:45,  8.86it/s]\titers: 400, epoch: 15 | loss: 0.0033552\n",
            "\tspeed: 0.1140s/iter; left time: 4111.0324s\n",
            "499it [00:57,  8.88it/s]\titers: 500, epoch: 15 | loss: 0.0058280\n",
            "\tspeed: 0.1130s/iter; left time: 4064.8627s\n",
            "599it [01:08,  8.94it/s]\titers: 600, epoch: 15 | loss: 0.0031988\n",
            "\tspeed: 0.1132s/iter; left time: 4058.6030s\n",
            "699it [01:19,  8.91it/s]\titers: 700, epoch: 15 | loss: 0.0004913\n",
            "\tspeed: 0.1132s/iter; left time: 4049.4222s\n",
            "799it [01:31,  8.87it/s]\titers: 800, epoch: 15 | loss: 0.0049739\n",
            "\tspeed: 0.1136s/iter; left time: 4053.3466s\n",
            "899it [01:42,  8.90it/s]\titers: 900, epoch: 15 | loss: 0.0149885\n",
            "\tspeed: 0.1133s/iter; left time: 4028.6027s\n",
            "999it [01:53,  8.93it/s]\titers: 1000, epoch: 15 | loss: 0.0053595\n",
            "\tspeed: 0.1135s/iter; left time: 4025.5519s\n",
            "1013it [01:55,  8.78it/s]\n",
            "Epoch: 15 cost time: 115.34230184555054\n",
            "144it [00:07, 18.70it/s]\n",
            "37it [00:02, 16.38it/s]\n",
            "Epoch: 15 | Train Loss: 0.0092857 Vali Loss: 0.2388508 Test Loss: 0.2106287 MAE Loss: 0.4150416\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Updating learning rate to 2.4414062500000014e-10\n",
            "99it [00:11,  8.87it/s]\titers: 100, epoch: 16 | loss: 0.0355693\n",
            "\tspeed: 0.2321s/iter; left time: 8204.9836s\n",
            "199it [00:22,  8.79it/s]\titers: 200, epoch: 16 | loss: 0.0133023\n",
            "\tspeed: 0.1135s/iter; left time: 4000.7089s\n",
            "299it [00:34,  8.87it/s]\titers: 300, epoch: 16 | loss: 0.0024822\n",
            "\tspeed: 0.1133s/iter; left time: 3984.2497s\n",
            "399it [00:45,  8.67it/s]\titers: 400, epoch: 16 | loss: 0.0394468\n",
            "\tspeed: 0.1132s/iter; left time: 3967.6511s\n",
            "499it [00:56,  8.85it/s]\titers: 500, epoch: 16 | loss: 0.1161615\n",
            "\tspeed: 0.1136s/iter; left time: 3970.0775s\n",
            "599it [01:08,  8.84it/s]\titers: 600, epoch: 16 | loss: 0.0151598\n",
            "\tspeed: 0.1133s/iter; left time: 3948.5222s\n",
            "699it [01:19,  8.65it/s]\titers: 700, epoch: 16 | loss: 0.0001633\n",
            "\tspeed: 0.1142s/iter; left time: 3968.2881s\n",
            "799it [01:31,  8.84it/s]\titers: 800, epoch: 16 | loss: 0.0014675\n",
            "\tspeed: 0.1141s/iter; left time: 3954.8071s\n",
            "899it [01:42,  8.94it/s]\titers: 900, epoch: 16 | loss: 0.0009028\n",
            "\tspeed: 0.1141s/iter; left time: 3941.2212s\n",
            "999it [01:53,  8.92it/s]\titers: 1000, epoch: 16 | loss: 0.0023630\n",
            "\tspeed: 0.1135s/iter; left time: 3910.1667s\n",
            "1013it [01:55,  8.77it/s]\n",
            "Epoch: 16 cost time: 115.49690580368042\n",
            "144it [00:07, 18.25it/s]\n",
            "37it [00:02, 15.91it/s]\n",
            "Epoch: 16 | Train Loss: 0.0091350 Vali Loss: 0.2428007 Test Loss: 0.2106450 MAE Loss: 0.4151360\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Updating learning rate to 1.2207031250000007e-10\n",
            "99it [00:11,  8.81it/s]\titers: 100, epoch: 17 | loss: 0.0554859\n",
            "\tspeed: 0.2344s/iter; left time: 8051.3445s\n",
            "199it [00:23,  8.88it/s]\titers: 200, epoch: 17 | loss: 0.0219937\n",
            "\tspeed: 0.1147s/iter; left time: 3927.3145s\n",
            "299it [00:34,  8.71it/s]\titers: 300, epoch: 17 | loss: 0.0031653\n",
            "\tspeed: 0.1136s/iter; left time: 3879.4415s\n",
            "399it [00:45,  8.57it/s]\titers: 400, epoch: 17 | loss: 0.0042418\n",
            "\tspeed: 0.1138s/iter; left time: 3873.6352s\n",
            "499it [00:57,  8.66it/s]\titers: 500, epoch: 17 | loss: 0.0002698\n",
            "\tspeed: 0.1132s/iter; left time: 3843.3335s\n",
            "599it [01:08,  8.88it/s]\titers: 600, epoch: 17 | loss: 0.0014289\n",
            "\tspeed: 0.1139s/iter; left time: 3855.3023s\n",
            "699it [01:19,  8.89it/s]\titers: 700, epoch: 17 | loss: 0.0028154\n",
            "\tspeed: 0.1135s/iter; left time: 3828.4364s\n",
            "799it [01:31,  8.71it/s]\titers: 800, epoch: 17 | loss: 0.0269854\n",
            "\tspeed: 0.1132s/iter; left time: 3806.9903s\n",
            "899it [01:42,  8.69it/s]\titers: 900, epoch: 17 | loss: 0.0002722\n",
            "\tspeed: 0.1134s/iter; left time: 3805.1495s\n",
            "999it [01:53,  8.90it/s]\titers: 1000, epoch: 17 | loss: 0.0025963\n",
            "\tspeed: 0.1139s/iter; left time: 3807.5411s\n",
            "1013it [01:55,  8.77it/s]\n",
            "Epoch: 17 cost time: 115.55969643592834\n",
            "144it [00:07, 18.10it/s]\n",
            "37it [00:02, 15.95it/s]\n",
            "Epoch: 17 | Train Loss: 0.0094845 Vali Loss: 0.2391626 Test Loss: 0.2117188 MAE Loss: 0.4159484\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Updating learning rate to 6.103515625000004e-11\n",
            "99it [00:11,  8.55it/s]\titers: 100, epoch: 18 | loss: 0.0031041\n",
            "\tspeed: 0.2344s/iter; left time: 7813.8668s\n",
            "199it [00:22,  8.91it/s]\titers: 200, epoch: 18 | loss: 0.0011272\n",
            "\tspeed: 0.1132s/iter; left time: 3762.0648s\n",
            "299it [00:34,  8.86it/s]\titers: 300, epoch: 18 | loss: 0.0015786\n",
            "\tspeed: 0.1133s/iter; left time: 3752.5085s\n",
            "399it [00:45,  8.87it/s]\titers: 400, epoch: 18 | loss: 0.0060830\n",
            "\tspeed: 0.1135s/iter; left time: 3750.0141s\n",
            "499it [00:56,  8.73it/s]\titers: 500, epoch: 18 | loss: 0.0256131\n",
            "\tspeed: 0.1138s/iter; left time: 3747.6714s\n",
            "599it [01:08,  8.82it/s]\titers: 600, epoch: 18 | loss: 0.0111290\n",
            "\tspeed: 0.1146s/iter; left time: 3763.3344s\n",
            "699it [01:19,  8.84it/s]\titers: 700, epoch: 18 | loss: 0.0014202\n",
            "\tspeed: 0.1141s/iter; left time: 3734.6874s\n",
            "799it [01:31,  8.74it/s]\titers: 800, epoch: 18 | loss: 0.0008490\n",
            "\tspeed: 0.1141s/iter; left time: 3723.5353s\n",
            "899it [01:42,  8.89it/s]\titers: 900, epoch: 18 | loss: 0.0080284\n",
            "\tspeed: 0.1136s/iter; left time: 3696.0717s\n",
            "999it [01:53,  8.73it/s]\titers: 1000, epoch: 18 | loss: 0.0004694\n",
            "\tspeed: 0.1142s/iter; left time: 3704.0992s\n",
            "1013it [01:55,  8.76it/s]\n",
            "Epoch: 18 cost time: 115.62858200073242\n",
            "144it [00:07, 18.41it/s]\n",
            "37it [00:02, 15.92it/s]\n",
            "Epoch: 18 | Train Loss: 0.0095098 Vali Loss: 0.2339698 Test Loss: 0.2105480 MAE Loss: 0.4148963\n",
            "Updating learning rate to 3.051757812500002e-11\n",
            "99it [00:11,  8.68it/s]\titers: 100, epoch: 19 | loss: 0.0005702\n",
            "\tspeed: 0.3882s/iter; left time: 12545.0483s\n",
            "199it [00:22,  8.85it/s]\titers: 200, epoch: 19 | loss: 0.0055558\n",
            "\tspeed: 0.1130s/iter; left time: 3641.3780s\n",
            "299it [00:34,  8.68it/s]\titers: 300, epoch: 19 | loss: 0.0738670\n",
            "\tspeed: 0.1134s/iter; left time: 3642.7755s\n",
            "399it [00:45,  8.91it/s]\titers: 400, epoch: 19 | loss: 0.0026709\n",
            "\tspeed: 0.1127s/iter; left time: 3607.8938s\n",
            "499it [00:56,  8.82it/s]\titers: 500, epoch: 19 | loss: 0.0003746\n",
            "\tspeed: 0.1136s/iter; left time: 3624.2924s\n",
            "599it [01:08,  8.68it/s]\titers: 600, epoch: 19 | loss: 0.0018589\n",
            "\tspeed: 0.1144s/iter; left time: 3638.9227s\n",
            "699it [01:19,  8.73it/s]\titers: 700, epoch: 19 | loss: 0.0019391\n",
            "\tspeed: 0.1138s/iter; left time: 3610.6646s\n",
            "799it [01:31,  8.84it/s]\titers: 800, epoch: 19 | loss: 0.0042715\n",
            "\tspeed: 0.1143s/iter; left time: 3612.6206s\n",
            "899it [01:42,  8.71it/s]\titers: 900, epoch: 19 | loss: 0.0000496\n",
            "\tspeed: 0.1145s/iter; left time: 3610.1715s\n",
            "999it [01:53,  8.83it/s]\titers: 1000, epoch: 19 | loss: 0.0435303\n",
            "\tspeed: 0.1140s/iter; left time: 3582.1818s\n",
            "1013it [01:55,  8.77it/s]\n",
            "Epoch: 19 cost time: 115.50861644744873\n",
            "144it [00:07, 18.36it/s]\n",
            "37it [00:02, 16.12it/s]\n",
            "Epoch: 19 | Train Loss: 0.0092242 Vali Loss: 0.2439367 Test Loss: 0.2112443 MAE Loss: 0.4155884\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 1.525878906250001e-11\n",
            "99it [00:11,  8.84it/s]\titers: 100, epoch: 20 | loss: 0.0010453\n",
            "\tspeed: 0.2342s/iter; left time: 7330.3424s\n",
            "199it [00:22,  8.94it/s]\titers: 200, epoch: 20 | loss: 0.0026120\n",
            "\tspeed: 0.1134s/iter; left time: 3539.7528s\n",
            "299it [00:34,  8.51it/s]\titers: 300, epoch: 20 | loss: 0.0478457\n",
            "\tspeed: 0.1143s/iter; left time: 3555.7496s\n",
            "399it [00:45,  8.91it/s]\titers: 400, epoch: 20 | loss: 0.0014302\n",
            "\tspeed: 0.1137s/iter; left time: 3524.0605s\n",
            "499it [00:57,  8.68it/s]\titers: 500, epoch: 20 | loss: 0.0131608\n",
            "\tspeed: 0.1140s/iter; left time: 3521.9306s\n",
            "599it [01:08,  8.92it/s]\titers: 600, epoch: 20 | loss: 0.0002044\n",
            "\tspeed: 0.1135s/iter; left time: 3495.5061s\n",
            "699it [01:19,  8.69it/s]\titers: 700, epoch: 20 | loss: 0.0010642\n",
            "\tspeed: 0.1139s/iter; left time: 3498.4146s\n",
            "799it [01:31,  8.71it/s]\titers: 800, epoch: 20 | loss: 0.0374430\n",
            "\tspeed: 0.1139s/iter; left time: 3486.3234s\n",
            "899it [01:42,  8.86it/s]\titers: 900, epoch: 20 | loss: 0.0040495\n",
            "\tspeed: 0.1132s/iter; left time: 3452.4859s\n",
            "999it [01:53,  8.87it/s]\titers: 1000, epoch: 20 | loss: 0.0354119\n",
            "\tspeed: 0.1136s/iter; left time: 3454.5449s\n",
            "1013it [01:55,  8.76it/s]\n",
            "Epoch: 20 cost time: 115.62941384315491\n",
            "144it [00:07, 18.42it/s]\n",
            "37it [00:02, 16.16it/s]\n",
            "Epoch: 20 | Train Loss: 0.0096377 Vali Loss: 0.2379295 Test Loss: 0.2116307 MAE Loss: 0.4161708\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Updating learning rate to 7.629394531250005e-12\n",
            "99it [00:11,  8.91it/s]\titers: 100, epoch: 21 | loss: 0.0030630\n",
            "\tspeed: 0.2335s/iter; left time: 7072.6007s\n",
            "199it [00:22,  8.84it/s]\titers: 200, epoch: 21 | loss: 0.0023073\n",
            "\tspeed: 0.1140s/iter; left time: 3440.2955s\n",
            "299it [00:34,  8.88it/s]\titers: 300, epoch: 21 | loss: 0.0001445\n",
            "\tspeed: 0.1136s/iter; left time: 3419.6406s\n",
            "399it [00:45,  8.84it/s]\titers: 400, epoch: 21 | loss: 0.0276686\n",
            "\tspeed: 0.1138s/iter; left time: 3413.5160s\n",
            "499it [00:57,  8.77it/s]\titers: 500, epoch: 21 | loss: 0.0182382\n",
            "\tspeed: 0.1135s/iter; left time: 3391.3885s\n",
            "599it [01:08,  8.66it/s]\titers: 600, epoch: 21 | loss: 0.0178246\n",
            "\tspeed: 0.1144s/iter; left time: 3408.5186s\n",
            "699it [01:19,  8.84it/s]\titers: 700, epoch: 21 | loss: 0.0066523\n",
            "\tspeed: 0.1140s/iter; left time: 3385.5295s\n",
            "799it [01:31,  8.87it/s]\titers: 800, epoch: 21 | loss: 0.0082934\n",
            "\tspeed: 0.1141s/iter; left time: 3377.0939s\n",
            "899it [01:42,  8.80it/s]\titers: 900, epoch: 21 | loss: 0.0015661\n",
            "\tspeed: 0.1140s/iter; left time: 3363.3919s\n",
            "999it [01:54,  8.94it/s]\titers: 1000, epoch: 21 | loss: 0.0037591\n",
            "\tspeed: 0.1147s/iter; left time: 3371.3984s\n",
            "1013it [01:55,  8.74it/s]\n",
            "Epoch: 21 cost time: 115.85317635536194\n",
            "144it [00:07, 18.35it/s]\n",
            "37it [00:02, 16.22it/s]\n",
            "Epoch: 21 | Train Loss: 0.0091979 Vali Loss: 0.2418621 Test Loss: 0.2113180 MAE Loss: 0.4156468\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Updating learning rate to 3.814697265625002e-12\n",
            "99it [00:11,  8.53it/s]\titers: 100, epoch: 22 | loss: 0.0037742\n",
            "\tspeed: 0.2342s/iter; left time: 6858.1458s\n",
            "199it [00:23,  8.82it/s]\titers: 200, epoch: 22 | loss: 0.0014337\n",
            "\tspeed: 0.1139s/iter; left time: 3322.0491s\n",
            "299it [00:34,  8.75it/s]\titers: 300, epoch: 22 | loss: 0.0049153\n",
            "\tspeed: 0.1140s/iter; left time: 3314.3143s\n",
            "399it [00:45,  8.64it/s]\titers: 400, epoch: 22 | loss: 0.0024975\n",
            "\tspeed: 0.1133s/iter; left time: 3283.7687s\n",
            "499it [00:57,  8.84it/s]\titers: 500, epoch: 22 | loss: 0.0042938\n",
            "\tspeed: 0.1139s/iter; left time: 3290.3280s\n",
            "599it [01:08,  8.88it/s]\titers: 600, epoch: 22 | loss: 0.0024959\n",
            "\tspeed: 0.1137s/iter; left time: 3273.4029s\n",
            "699it [01:19,  8.83it/s]\titers: 700, epoch: 22 | loss: 0.0005395\n",
            "\tspeed: 0.1142s/iter; left time: 3273.7239s\n",
            "799it [01:31,  8.74it/s]\titers: 800, epoch: 22 | loss: 0.0000461\n",
            "\tspeed: 0.1139s/iter; left time: 3256.3720s\n",
            "899it [01:42,  8.88it/s]\titers: 900, epoch: 22 | loss: 0.0001716\n",
            "\tspeed: 0.1137s/iter; left time: 3238.1736s\n",
            "999it [01:54,  8.86it/s]\titers: 1000, epoch: 22 | loss: 0.0016361\n",
            "\tspeed: 0.1129s/iter; left time: 3203.7366s\n",
            "1013it [01:55,  8.76it/s]\n",
            "Epoch: 22 cost time: 115.65153336524963\n",
            "144it [00:07, 18.56it/s]\n",
            "37it [00:02, 16.31it/s]\n",
            "Epoch: 22 | Train Loss: 0.0091805 Vali Loss: 0.2380018 Test Loss: 0.2113312 MAE Loss: 0.4156492\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Updating learning rate to 1.907348632812501e-12\n",
            "99it [00:11,  8.81it/s]\titers: 100, epoch: 23 | loss: 0.0066710\n",
            "\tspeed: 0.2323s/iter; left time: 6566.1438s\n",
            "199it [00:22,  8.70it/s]\titers: 200, epoch: 23 | loss: 0.0029993\n",
            "\tspeed: 0.1142s/iter; left time: 3216.9347s\n",
            "299it [00:34,  8.69it/s]\titers: 300, epoch: 23 | loss: 0.0063600\n",
            "\tspeed: 0.1143s/iter; left time: 3208.5315s\n",
            "399it [00:45,  8.85it/s]\titers: 400, epoch: 23 | loss: 0.0015548\n",
            "\tspeed: 0.1143s/iter; left time: 3196.4507s\n",
            "499it [00:57,  8.60it/s]\titers: 500, epoch: 23 | loss: 0.0033153\n",
            "\tspeed: 0.1146s/iter; left time: 3193.0102s\n",
            "599it [01:08,  8.67it/s]\titers: 600, epoch: 23 | loss: 0.0083791\n",
            "\tspeed: 0.1138s/iter; left time: 3158.7907s\n",
            "699it [01:20,  8.86it/s]\titers: 700, epoch: 23 | loss: 0.0005187\n",
            "\tspeed: 0.1142s/iter; left time: 3158.5110s\n",
            "799it [01:31,  8.79it/s]\titers: 800, epoch: 23 | loss: 0.0036643\n",
            "\tspeed: 0.1139s/iter; left time: 3138.3377s\n",
            "899it [01:42,  8.81it/s]\titers: 900, epoch: 23 | loss: 0.0100269\n",
            "\tspeed: 0.1136s/iter; left time: 3119.3023s\n",
            "999it [01:54,  8.87it/s]\titers: 1000, epoch: 23 | loss: 0.0020183\n",
            "\tspeed: 0.1133s/iter; left time: 3101.7098s\n",
            "1013it [01:55,  8.75it/s]\n",
            "Epoch: 23 cost time: 115.80776834487915\n",
            "144it [00:07, 18.56it/s]\n",
            "37it [00:02, 16.37it/s]\n",
            "Epoch: 23 | Train Loss: 0.0092007 Vali Loss: 0.2490957 Test Loss: 0.2112310 MAE Loss: 0.4155452\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n",
            "0it [00:00, ?it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[163.76 164.66 162.03 160.8  160.1 ]\n",
            "------------ 0   2022-04-05\n",
            "1   2022-04-06\n",
            "2   2022-04-10\n",
            "3   2022-04-11\n",
            "4   2022-04-12\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "1it [00:00,  2.39it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[165.56 165.21 165.23 166.47 167.63]\n",
            "------------ 0   2022-04-13\n",
            "1   2022-04-14\n",
            "2   2022-04-17\n",
            "3   2022-04-18\n",
            "4   2022-04-19\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[166.65    165.02    165.32999 163.76999 163.76   ]\n",
            "------------ 0   2022-04-20\n",
            "1   2022-04-21\n",
            "2   2022-04-24\n",
            "3   2022-04-25\n",
            "4   2022-04-26\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "3it [00:00,  6.86it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[168.40999 169.68    169.59    168.54    167.45   ]\n",
            "------------ 0   2022-04-27\n",
            "1   2022-04-28\n",
            "2   2022-05-01\n",
            "3   2022-05-02\n",
            "4   2022-05-03\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[165.79    173.57    173.49998 171.77    173.55998]\n",
            "------------ 0   2022-05-04\n",
            "1   2022-05-05\n",
            "2   2022-05-08\n",
            "3   2022-05-09\n",
            "4   2022-05-10\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "5it [00:00, 10.27it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[173.75    172.56999 172.07    172.07    172.68999]\n",
            "------------ 0   2022-05-11\n",
            "1   2022-05-12\n",
            "2   2022-05-15\n",
            "3   2022-05-16\n",
            "4   2022-05-17\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[175.05    175.15999 174.2     171.56    171.83998]\n",
            "------------ 0   2022-05-18\n",
            "1   2022-05-19\n",
            "2   2022-05-22\n",
            "3   2022-05-23\n",
            "4   2022-05-24\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "7it [00:00, 12.32it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[172.98999 175.42998 177.29999 177.25    180.09   ]\n",
            "------------ 0   2022-05-25\n",
            "1   2022-05-26\n",
            "2   2022-05-30\n",
            "3   2022-05-31\n",
            "4   2022-06-01\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[180.95    179.57999 179.21    177.82    180.57   ]\n",
            "------------ 0   2022-06-02\n",
            "1   2022-06-05\n",
            "2   2022-06-06\n",
            "3   2022-06-07\n",
            "4   2022-06-08\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "9it [00:00, 13.55it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[180.96    183.79    183.31    183.94998 186.01   ]\n",
            "------------ 0   2022-06-09\n",
            "1   2022-06-12\n",
            "2   2022-06-13\n",
            "3   2022-06-14\n",
            "4   2022-06-15\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[184.91998 185.00998 183.96    186.99998 186.68   ]\n",
            "------------ 0   2022-06-16\n",
            "1   2022-06-20\n",
            "2   2022-06-21\n",
            "3   2022-06-22\n",
            "4   2022-06-23\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "11it [00:00, 15.22it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[185.27    188.06    189.25    189.58998 193.97   ]\n",
            "------------ 0   2022-06-26\n",
            "1   2022-06-27\n",
            "2   2022-06-28\n",
            "3   2022-06-29\n",
            "4   2022-06-30\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[192.46    191.32999 191.81    190.68    188.61   ]\n",
            "------------ 0   2022-07-03\n",
            "1   2022-07-05\n",
            "2   2022-07-06\n",
            "3   2022-07-07\n",
            "4   2022-07-10\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "13it [00:01, 16.24it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[188.07999 189.76999 190.54    190.69    193.98999]\n",
            "------------ 0   2022-07-11\n",
            "1   2022-07-12\n",
            "2   2022-07-13\n",
            "3   2022-07-14\n",
            "4   2022-07-17\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[193.73    195.09999 193.13    191.93999 192.74998]\n",
            "------------ 0   2022-07-18\n",
            "1   2022-07-19\n",
            "2   2022-07-20\n",
            "3   2022-07-21\n",
            "4   2022-07-24\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "15it [00:01, 17.23it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[193.61998 194.49998 193.22    195.83    196.45   ]\n",
            "------------ 0   2022-07-25\n",
            "1   2022-07-26\n",
            "2   2022-07-27\n",
            "3   2022-07-28\n",
            "4   2022-07-31\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[195.60999 192.58    191.17    181.99    178.85   ]\n",
            "------------ 0   2022-08-01\n",
            "1   2022-08-02\n",
            "2   2022-08-03\n",
            "3   2022-08-04\n",
            "4   2022-08-07\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "17it [00:01, 17.96it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[179.79999 178.19    177.97    177.79    179.46   ]\n",
            "------------ 0   2022-08-08\n",
            "1   2022-08-09\n",
            "2   2022-08-10\n",
            "3   2022-08-11\n",
            "4   2022-08-14\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[177.45    176.56999 174.      174.48999 175.84   ]\n",
            "------------ 0   2022-08-15\n",
            "1   2022-08-16\n",
            "2   2022-08-17\n",
            "3   2022-08-18\n",
            "4   2022-08-21\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "19it [00:01, 18.38it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[177.22998 181.11998 176.38    178.61    180.18999]\n",
            "------------ 0   2022-08-22\n",
            "1   2022-08-23\n",
            "2   2022-08-24\n",
            "3   2022-08-25\n",
            "4   2022-08-28\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[184.12    187.65    187.86998 189.46    189.7    ]\n",
            "------------ 0   2022-08-29\n",
            "1   2022-08-30\n",
            "2   2022-08-31\n",
            "3   2022-09-01\n",
            "4   2022-09-05\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "21it [00:01, 18.05it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[182.91    177.56    178.18    179.36    176.29999]\n",
            "------------ 0   2022-09-06\n",
            "1   2022-09-07\n",
            "2   2022-09-08\n",
            "3   2022-09-11\n",
            "4   2022-09-12\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[174.21    175.74    175.01    177.97    179.06999]\n",
            "------------ 0   2022-09-13\n",
            "1   2022-09-14\n",
            "2   2022-09-15\n",
            "3   2022-09-18\n",
            "4   2022-09-19\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "23it [00:01, 18.21it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[175.48999 173.93    174.78998 176.08    171.95999]\n",
            "------------ 0   2022-09-20\n",
            "1   2022-09-21\n",
            "2   2022-09-22\n",
            "3   2022-09-25\n",
            "4   2022-09-26\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[170.43    170.69    171.20999 173.75    172.4    ]\n",
            "------------ 0   2022-09-27\n",
            "1   2022-09-28\n",
            "2   2022-09-29\n",
            "3   2022-10-02\n",
            "4   2022-10-03\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "25it [00:01, 18.37it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[173.66 174.91 177.49 178.99 178.39]\n",
            "------------ 0   2022-10-04\n",
            "1   2022-10-05\n",
            "2   2022-10-06\n",
            "3   2022-10-09\n",
            "4   2022-10-10\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[179.79999 180.71    178.85    178.72    177.15   ]\n",
            "------------ 0   2022-10-11\n",
            "1   2022-10-12\n",
            "2   2022-10-13\n",
            "3   2022-10-16\n",
            "4   2022-10-17\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "27it [00:01, 18.17it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[175.84    175.45999 172.88    173.      173.43999]\n",
            "------------ 0   2022-10-18\n",
            "1   2022-10-19\n",
            "2   2022-10-20\n",
            "3   2022-10-23\n",
            "4   2022-10-24\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[171.1  166.89 168.22 170.29 170.77]\n",
            "------------ 0   2022-10-25\n",
            "1   2022-10-26\n",
            "2   2022-10-27\n",
            "3   2022-10-30\n",
            "4   2022-10-31\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "29it [00:01, 18.47it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[173.97    177.56999 176.65    179.23    181.81999]\n",
            "------------ 0   2022-11-01\n",
            "1   2022-11-02\n",
            "2   2022-11-03\n",
            "3   2022-11-06\n",
            "4   2022-11-07\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[182.89    182.41    186.4     184.79999 187.44   ]\n",
            "------------ 0   2022-11-08\n",
            "1   2022-11-09\n",
            "2   2022-11-10\n",
            "3   2022-11-13\n",
            "4   2022-11-14\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "31it [00:02, 18.58it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[188.01    189.70999 189.69    191.44998 190.63998]\n",
            "------------ 0   2022-11-15\n",
            "1   2022-11-16\n",
            "2   2022-11-17\n",
            "3   2022-11-20\n",
            "4   2022-11-21\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[191.31    189.97    189.79    190.39998 189.36998]\n",
            "------------ 0   2022-11-22\n",
            "1   2022-11-24\n",
            "2   2022-11-27\n",
            "3   2022-11-28\n",
            "4   2022-11-29\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "33it [00:02, 18.40it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[189.94998 191.23999 189.42998 193.42    192.32   ]\n",
            "------------ 0   2022-11-30\n",
            "1   2022-12-01\n",
            "2   2022-12-04\n",
            "3   2022-12-05\n",
            "4   2022-12-06\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[194.27    195.71    193.18    194.70999 197.95999]\n",
            "------------ 0   2022-12-07\n",
            "1   2022-12-08\n",
            "2   2022-12-11\n",
            "3   2022-12-12\n",
            "4   2022-12-13\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "35it [00:02, 18.53it/s]batch_y_mark torch.Size([5, 65, 3])\n",
            "[198.10999 197.56999 195.89    196.93999 194.82999]\n",
            "------------ 0   2022-12-14\n",
            "1   2022-12-15\n",
            "2   2022-12-18\n",
            "3   2022-12-19\n",
            "4   2022-12-20\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "batch_y_mark torch.Size([5, 65, 3])\n",
            "[194.67998 193.59999 193.04999 193.15    193.58   ]\n",
            "------------ 0   2022-12-21\n",
            "1   2022-12-22\n",
            "2   2022-12-26\n",
            "3   2022-12-27\n",
            "4   2022-12-28\n",
            "dtype: datetime64[ns] \n",
            "\n",
            "37it [00:02, 15.37it/s]\n",
            "Results saved to predictions_2023.csv\n"
          ]
        }
      ],
      "source": [
        "!python run_main.py \\\n",
        "    --task_name long_term_forecast \\\n",
        "    --is_training 1 \\\n",
        "    --model_id AAPL_TimeLLM \\\n",
        "    --model TimeLLM \\\n",
        "    --data_path 'data.csv' \\\n",
        "    --root_path '/home/fakoor/Desktop/chitsaz/emotion_detection/emotions' \\\n",
        "    --model_comment \"prediction\" \\\n",
        "    --target 'y' \\\n",
        "    --freq 'd' \\\n",
        "    --seq_len 64 \\\n",
        "    --label_len 64 \\\n",
        "    --pred_len 1 \\\n",
        "    --train_epochs 50 \\\n",
        "    --batch_size 5 \\\n",
        "    --learning_rate 0.0001 \\\n",
        "    --patience 5 \\\n",
        "    --checkpoints './checkpoints/emotion' \\\n",
        "    --llm_dim 5120 \\\n",
        "    --loss MSE \\\n",
        "    --lradj type1 \\\n",
        "    --prompt_domain 1 \\\n",
        "    --data Traffic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "!export RDMAV_FORK_SAFE=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTZY0yHGDLlG",
        "outputId": "372e4790-d89f-4e5e-87a2-d555f9d608aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mpi4py\n",
            "  Using cached mpi4py-4.0.0.tar.gz (464 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for mpi4py: filename=mpi4py-4.0.0-cp310-cp310-linux_x86_64.whl size=4856817 sha256=7ef26bdd6d385be6ace823285d482964ac0c5f3b383ee25d74de02710f291b8a\n",
            "  Stored in directory: /home/fakoor/.cache/pip/wheels/96/17/12/83db63ee0ae5c4b040ee87f2e5c813aea4728b55ec6a37317c\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mpi4py\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
