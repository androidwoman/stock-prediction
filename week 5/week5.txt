mailto:superalignmentfastgrants@openai.com
https://airtable.com/appnIXmOlWAJBzrJp/paghnoKL6EHiKmKbf/form
february 18 ---> 29 bahamn

چندتا نکته جالب بود ادما گفته بودن یک اینکه درک از محیطی که ما داریم زندگی میکنیم داشته باشند درک از قوانین فیزیک داشته باشند
مصرف منابع زیاد رو درک نمیکنند و این ممکنه مشکل ساز بشه 

---------------------------------------------------------------------

WEAK-TO-STRONG GENERALIZATION: ELICITING
STRONG CAPABILITIES WITH WEAK SUPERVISION

using auxiliary confidence loss


generates a million lines of extremely complicated code, humans will not be able to provide reliable supervision for key alignmentrelevant tasks, including -->whether the code is safe or dangerous to execute

(RM) or safety classification task

We can empirically
test this by finetuning large (strong) pretrained models on labels generated by small (weak) models and observing how they generalize

****learn to imitate the weak supervisor, including its error
alignment-relevant tasks we care abou

DB:
a large set of popular natural language processing
(NLP) benchmarks, chess puzzles, and our internal ChatGPT reward modeling dataset.

1.Strong pretrained models naturally generalize beyond their weak supervisors.
we typically recover about half of the
performance gap between the two models.

2.Naively finetuning on weak supervison is not enough

 that naive RLHF will likely scale poorly to superhuman


3.Improving weak-to-strong generalization is tractable
when supervising GPT-4 with a GPT-2-
level model on NLP tasks using the auxiliary confidence loss, we typically recover nearly
80% of the performance gap


گرفتاری:
None of our methods work consistently in all settings,
we are still far from recovering the full performance gap between weak
and strong model



چیزای مرتبط:

setting in which models are trained using unreliable labels
, noise-robust losses
 bootstrapping

semi-supervised setting =====>
 “easy” subset of examples that weak supervisors provide reliable labels
“hard” examples that the weak supervisor can’t reliably label, 



***Student-teacher training*** --->of first training a teacher and then training a student
in contrast to most prior work, we focus on the setting where the student is much more capable than
the teacher.

semi-supervised learning and domain adaptation
 ------confidence auxiliary loss-----


Debiasing:
specific form of bias,
which results from the weak models’ lack of capability
 difficult debiasing problem---> bias is
unknown


Knowledge elicitation and honesty:
Eliciting Latent Knowledge (ELK)


method:
we generate weak labels by taking the
weak model’s predictions on a held-out set of examples.
We finetune a strong model with
the generated weak labels and weak-to-strong performance , the strong ceiling performance.
 متر اندازه گیری:

performance gap recovered (PGR)
---> perfect weakto-strong generalization, PGR is 1
گرفتاری دوباره:
easier failure mode to avoid in our setting than it will be in the future
we do not yet know how superhuman models will be built,


کارایی که باید بکنیم:
We produce soft labels from the weak model

nlp --> binery
we sample from the weak model
with temperature 0.---> chess
For each query, the
humans compare multiple possible responses 
reward model is trained to predict the results of pairwise comparisons
gpt ---> maximize reward model accuracy  کاری که باید بکنیم برای تسک سوم 


ساده قضیه رو ببینیم :
naively finetuned on labels generated by weak supervisors
We find that PGRs are almost universally positive
 for nlp -->The PGR
increases both with weak supervisor size and with strong student size;


for chess --> PGR is close to zero and the test accuracy curves appear flat
ncreases substantially; for small supervisor-student gaps, PGR
can be above 40%


****PGR decreases with the strong student size for a given weak supervisor on chess puzzles***


BOOTSTRAPPING--->instead of directly aligning very superhuman

we could first align an only slightly superhuman model,
align an even smarter
model, and so on
like :M1 → M2 → . . . → Mn
what we do ?   weak supervision using two intermediate model sizes before finally finetuning the largest model
in the sequence

but :::::  small improvements with bootstrapping on NLP tasks and no improvements in the RM setting.

 AUXILIARY CONFIDENCE LOSS:
 it may also learn to imitate the errors of the supervisor 
not to imitate its mistakes    هدف اینه 
adding an auxiliary confidence loss term to the standard cross
entropy objective. This method is closely related to conditional entropy minimization

  بدبختی :
when the gap between the weak supervisor
and strong student is small or when the dataset features inverse scaling even with ground truth


چیز جالب :
strong model instead learns to imitate the weak
supervisor—predicting how the weak supervisor would have classified each example


گرفتاری:
OVERFITTING TO WEAK SUPERVISION
strong student indeed appears to overfit to the weak supervisor’s errors
راه حل:
. Ground truth early stopping, which “cheats”
We see overfitting to weak labels for large weak-strong
gaps, even within one epoch


داده :
early stopping on ground truth gives a 15 percentage point boost in PGR over the model at the end
of training, and a 10 percentage point boost in PGR compared to “non-cheating” early stopping with
respect to weak labels


بحث AGREEMENT
fraction of test inputs where the strong student makes the same prediction as the
weak supervisor. Note that if agreement were 100%, then weak-to-strong accuracy would be equal
to supervisor accuracy, and PGR would be 0.


the confidence loss reduces student-supervisor agreement----> جالب


بحث INVERSE SCALING
 Surprisingly, we find inverse scaling (McKenzie et al., 2023): larger student models
consistently agree less with the errors of the supervisor than smaller student models

برسی کنیم که:
t how different types of weak supervision errors
impact what the strong student learns.


بحث SALIENCY
 it is possible that strong pretrained models can solve
many relevant tasks zero-shot with a simple prompt


بحث prompting 
be elicited fairly easily with prompting. However, our current setup may be more disanalogous for
prompting than for finetuning; many of our NLP tasks may have been implicitly observed during
pretraining, which we conjecture benefits prompting more than finetuning.

مثال :
finetuning a language model in an unsupervised way on online reviews, sentiment
becomes saliently represented to models internally (Radford et al., 2017).


بحث : Generative finetuning
generative finetuning on the RM data leads to better weak-to-strong
performance---> 40 %

بحث ft(weak) + lp(gt): 

----------------------------------------------
data set 
https://huggingface.co/datasets/sciq
Models:

gpt2
gpt2-medium
gpt2-large
gpt2-xl
Qwen/Qwen-1_8B
Qwen/Qwen-7B
Qwen/Qwen-14B
Qwen/Qwen-72B
Datasets:
"amazon_polarity," "sciq," "anthropic_hh," "cosmos_qa," and "boolq."
---------------------------------------------
