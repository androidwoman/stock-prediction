{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHT84ILyzpxL",
        "outputId": "8fd03794-4cf7-4b52-f383-d9c8917ccb4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-27 09:43:03--  https://drive.usercontent.google.com/download?id=1CUS371Eo1VQKlCbnymt6dHxRe9-EyCRE&export=download&authuser=0&confirm=t&uuid=1fdaaab9-e021-4f0e-9fa9-8e15700004f5&at=APZUnTVaT-rQ9cTXhI5sUn7lC332:1711524803062\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.120.132, 2607:f8b0:4001:c2e::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.120.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38608462 (37M) [application/octet-stream]\n",
            "Saving to: ‘train.text’\n",
            "\n",
            "train.text          100%[===================>]  36.82M   138MB/s    in 0.3s    \n",
            "\n",
            "2024-03-27 09:43:05 (138 MB/s) - ‘train.text’ saved [38608462/38608462]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \"https://drive.usercontent.google.com/download?id=1CUS371Eo1VQKlCbnymt6dHxRe9-EyCRE&export=download&authuser=0&confirm=t&uuid=1fdaaab9-e021-4f0e-9fa9-8e15700004f5&at=APZUnTVaT-rQ9cTXhI5sUn7lC332:1711524803062\" -O train.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOImfs431Gyn",
        "outputId": "42a81019-5b26-44ce-82e0-281c0c6c019e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-27 09:43:30--  https://drive.usercontent.google.com/download?id=13HWHgoyp-RwyzpKg7BS1p3qs4iO_43AZ&export=download&authuser=0&confirm=t&uuid=65d985cd-f56d-4fd0-9561-95df94fa2edf&at=APZUnTWqEp66qIftgpKQF7NrlGhn:1711524915881\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.120.132, 2607:f8b0:4001:c2e::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.120.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3515119 (3.4M) [application/octet-stream]\n",
            "Saving to: ‘dev.text’\n",
            "\n",
            "dev.text            100%[===================>]   3.35M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-03-27 09:43:32 (244 MB/s) - ‘dev.text’ saved [3515119/3515119]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \"https://drive.usercontent.google.com/download?id=13HWHgoyp-RwyzpKg7BS1p3qs4iO_43AZ&export=download&authuser=0&confirm=t&uuid=65d985cd-f56d-4fd0-9561-95df94fa2edf&at=APZUnTWqEp66qIftgpKQF7NrlGhn:1711524915881\" -O dev.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lujcY4OP2ZUN"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    DataCollatorForPermutationLanguageModeling,\n",
        "    HfArgumentParser,\n",
        "    LineByLineTextDataset,\n",
        "    PreTrainedTokenizer,\n",
        "    TextDataset,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ahnvhuH22iaa"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    train_data_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
        "    )\n",
        "    eval_data_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
        "    )\n",
        "    line_by_line: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
        "    )\n",
        "\n",
        "    mlm: bool = field(\n",
        "        default=False, metadata={\"help\": \"Train with masked-language modeling loss instead of language modeling.\"}\n",
        "    )\n",
        "    mlm_probability: float = field(\n",
        "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
        "    )\n",
        "    plm_probability: float = field(\n",
        "        default=1 / 6,\n",
        "        metadata={\n",
        "            \"help\": \"Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling.\"\n",
        "        },\n",
        "    )\n",
        "    max_span_length: int = field(\n",
        "        default=5, metadata={\"help\": \"Maximum length of a span of masked tokens for permutation language modeling.\"}\n",
        "    )\n",
        "\n",
        "    block_size: int = field(\n",
        "        default=-1,\n",
        "        metadata={\n",
        "            \"help\": \"Optional input sequence length after tokenization.\"\n",
        "            \"The training dataset will be truncated in block of this size for training.\"\n",
        "            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yEFRfRys2k50"
      },
      "outputs": [],
      "source": [
        "def get_dataset(\n",
        "    args: DataTrainingArguments,\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    evaluate: bool = False,\n",
        "    cache_dir: Optional[str] = None,\n",
        "):\n",
        "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "    if args.line_by_line:\n",
        "        return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)\n",
        "    else:\n",
        "        return TextDataset(\n",
        "            tokenizer=tokenizer,\n",
        "            file_path=file_path,\n",
        "            block_size=args.block_size,\n",
        "            overwrite_cache=args.overwrite_cache,\n",
        "            cache_dir=cache_dir,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osXZNidG5PD5",
        "outputId": "0b8826a9-7111-470b-e097-a0f65a25fd9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (2.1.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (1.3.0)\n",
            "Collecting transformers==3.1.0\n",
            "  Using cached transformers-3.1.0-py3-none-any.whl.metadata (49 kB)\n",
            "Collecting nltk\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (2.8.2)\n",
            "Collecting yahoo-ticker-downloader\n",
            "  Using cached Yahoo_ticker_downloader-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers==3.1.0) (1.26.0)\n",
            "Collecting tokenizers==0.8.1.rc2 (from transformers==3.1.0)\n",
            "  Using cached tokenizers-0.8.1rc2.tar.gz (97 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: packaging in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers==3.1.0) (23.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers==3.1.0) (3.13.1)\n",
            "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers==3.1.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers==3.1.0) (4.66.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers==3.1.0) (2023.12.25)\n",
            "Collecting sentencepiece!=0.1.92 (from transformers==3.1.0)\n",
            "  Using cached sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
            "Collecting sacremoses (from transformers==3.1.0)\n",
            "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
            "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil) (1.16.0)\n",
            "Collecting tablib>=0.9.11 (from yahoo-ticker-downloader)\n",
            "  Using cached tablib-3.6.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting backports.csv>=1.0.4 (from yahoo-ticker-downloader)\n",
            "  Using cached backports.csv-1.0.7-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers==3.1.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers==3.1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers==3.1.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers==3.1.0) (2023.7.22)\n",
            "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers==3.1.0) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asus\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Using cached transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Using cached backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Using cached sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
            "Using cached tablib-3.6.0-py3-none-any.whl (47 kB)\n",
            "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  Building wheel for tokenizers (pyproject.toml): started\n",
            "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
            "Failed to build tokenizers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [50 lines of output]\n",
            "      C:\\Users\\ASUS\\AppData\\Local\\Temp\\pip-build-env-cjjwn7pg\\overlay\\Lib\\site-packages\\setuptools\\dist.py:318: InformationOnly: Normalizing '0.8.1.rc2' to '0.8.1rc2'\n",
            "        self.metadata.version = self._normalize_version(self.metadata.version)\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-310\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\n",
            "      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
            "      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
            "      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
            "      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
            "      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
            "      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
            "      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
            "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
            "      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\n",
            "      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
            "      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
            "      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
            "      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
            "      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
            "      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
            "      running build_ext\n",
            "      running build_rust\n",
            "      info: syncing channel updates for 'nightly-2020-05-14-x86_64-pc-windows-msvc'\n",
            "      error: could not download file from 'https://static.rust-lang.org/dist/2020-05-14/channel-rust-nightly.toml.sha256' to 'C:\\Users\\ASUS\\.rustup\\tmp\\plvow2txw_ekayre_file': failed to make network request: error sending request for url (https://static.rust-lang.org/dist/2020-05-14/channel-rust-nightly.toml.sha256): error trying to connect: tcp connect error: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. (os error 10060): error trying to connect: tcp connect error: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. (os error 10060): tcp connect error: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. (os error 10060): A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. (os error 10060)\n",
            "      error: can't find Rust compiler\n",
            "      \n",
            "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
            "      \n",
            "      To update pip, run:\n",
            "      \n",
            "          pip install --upgrade pip\n",
            "      \n",
            "      and then retry package installation.\n",
            "      \n",
            "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for tokenizers\n",
            "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "!pip install torch scikit-learn   transformers==3.1.0 nltk python-dateutil yahoo-ticker-downloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "H93QjLua1iAm",
        "outputId": "04e6625c-d846-41d5-a316-501104717e31"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "    TRAIN_FILE = \"/content/EDT/Domain_adaptation/train.text\"  # Replace with actual file path\n",
        "    TEST_FILE = \"/content/EDT/Domain_adaptation/dev.text\"   # Replace with actual file path\n",
        "    OUTPUT_DIR = \"/content/model\"  # This directory will store training results\n",
        "\n",
        "    # Initialize the arguments\n",
        "    model_args = ModelArguments()\n",
        "    data_args = DataTrainingArguments()\n",
        "    training_args = TrainingArguments(output_dir=OUTPUT_DIR)  # Provide output_dir argument\n",
        "\n",
        "    # Set the rest of the arguments\n",
        "    training_args.model_type = \"bert\"\n",
        "    training_args.model_name_or_path = \"bert-base-cased\"\n",
        "    training_args.do_train = True\n",
        "    training_args.train_data_file = TRAIN_FILE\n",
        "    training_args.do_eval = True\n",
        "    training_args.eval_data_file = TEST_FILE\n",
        "    training_args.mlm = True\n",
        "    training_args.per_device_train_batch_size = 4\n",
        "    training_args.gradient_accumulation_steps = 2\n",
        "    training_args.warmup_steps = 500\n",
        "    training_args.learning_rate = 3e-5\n",
        "    training_args.evaluate_during_training = True\n",
        "    training_args.eval_steps = 500\n",
        "    training_args.num_train_epochs = 20\n",
        "    training_args.max_steps = 10000\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    if data_args.eval_data_file is None and training_args.do_eval:\n",
        "        raise ValueError(\n",
        "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "            \"or remove the --do_eval argument.\"\n",
        "        )\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        training_args.local_rank,\n",
        "        training_args.device,\n",
        "        training_args.n_gpu,\n",
        "        bool(training_args.local_rank != -1),\n",
        "        training_args.fp16,\n",
        "    )\n",
        "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    #\n",
        "    # Distributed training:\n",
        "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "\n",
        "    if model_args.config_name:\n",
        "        config = AutoConfig.from_pretrained(model_args.config_name, cache_dir=model_args.cache_dir)\n",
        "    elif model_args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
        "    else:\n",
        "        config = CONFIG_MAPPING[model_args.model_type]()\n",
        "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
        "    config.max_seq_length = 512\n",
        "\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
        "            \"and load it from here, using --tokenizer_name\"\n",
        "        )\n",
        "\n",
        "    if model_args.model_name_or_path:\n",
        "        model = AutoModelWithLMHead.from_pretrained(\n",
        "            model_args.model_name_or_path,\n",
        "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "            config=config,\n",
        "            cache_dir=model_args.cache_dir,\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Training new model from scratch\")\n",
        "        model = AutoModelWithLMHead.from_config(config)\n",
        "\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if config.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"] and not data_args.mlm:\n",
        "        raise ValueError(\n",
        "            \"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the\"\n",
        "            \"--mlm flag (masked language modeling).\"\n",
        "        )\n",
        "\n",
        "    if data_args.block_size <= 0:\n",
        "        data_args.block_size = tokenizer.max_len\n",
        "        # Our input block size will be the max possible for the model\n",
        "    else:\n",
        "        data_args.block_size = min(data_args.block_size, tokenizer.max_len)\n",
        "\n",
        "    # Get datasets\n",
        "\n",
        "    train_dataset = (\n",
        "        get_dataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir) if training_args.do_train else None\n",
        "    )\n",
        "    eval_dataset = (\n",
        "        get_dataset(data_args, tokenizer=tokenizer, evaluate=True, cache_dir=model_args.cache_dir)\n",
        "        if training_args.do_eval\n",
        "        else None\n",
        "    )\n",
        "    if config.model_type == \"xlnet\":\n",
        "        data_collator = DataCollatorForPermutationLanguageModeling(\n",
        "            tokenizer=tokenizer,\n",
        "            plm_probability=data_args.plm_probability,\n",
        "            max_span_length=data_args.max_span_length,\n",
        "        )\n",
        "    else:\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability\n",
        "        )\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        prediction_loss_only=True,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    if training_args.do_train:\n",
        "        model_path = (\n",
        "            model_args.model_name_or_path\n",
        "            if model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path)\n",
        "            else None\n",
        "        )\n",
        "        trainer.train(model_path=model_path)\n",
        "        trainer.save_model()\n",
        "        # For convenience, we also re-save the tokenizer to the same directory,\n",
        "        # so that you can share your model easily on huggingface.co/models =)\n",
        "        if trainer.is_world_master():\n",
        "            tokenizer.save_pretrained(training_args.output_dir)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        eval_output = trainer.evaluate()\n",
        "\n",
        "        perplexity = math.exp(eval_output[\"eval_loss\"])\n",
        "        result = {\"perplexity\": perplexity}\n",
        "\n",
        "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
        "        if trainer.is_world_master():\n",
        "            with open(output_eval_file, \"w\") as writer:\n",
        "                logger.info(\"***** Eval results *****\")\n",
        "                for key in sorted(result.keys()):\n",
        "                    logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "        results.update(result)\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GfLASb6nJgKC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1564: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d202f90ca1834e459ea15dd96b1b5596",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead, TrainingArguments, Trainer\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "TRAIN_FILE = \"EDT/Domain_adaptation/train.text\"\n",
        "TEST_FILE = \"EDT/Domain_adaptation/dev.text\"\n",
        "OUTPUT_DIR = \"model\"\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelWithLMHead.from_pretrained(model_name)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=500,\n",
        "    learning_rate=3e-5,\n",
        "    max_steps=10000,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Get datasets\n",
        "train_dataset = get_dataset(tokenizer=tokenizer, file_path=TRAIN_FILE)\n",
        "eval_dataset = get_dataset(tokenizer=tokenizer, file_path=TEST_FILE, evaluate=True)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "# Save the model\n",
        "if trainer.is_world_process_zero():\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    model.save_pretrained(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
